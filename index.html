<!DOCTYPE html>
<html>

<head>
    <meta charset="UTF-8">
    <!-- 引入样式 -->
    <link rel="stylesheet" href="style/style.css">

    <style type="text/css" media="print">
        .noprint {
            display: none
        }
        
        .print {
            display: block !important;
        }
    </style>
</head>

<body>
    <div id="app">
        <header class="el-header noprint">
            <div class="icon-btns">
                <i class="icon-list" @click="changeLeftMenu"></i>
                <i class="icon-skip_previous" v-bind:class="{'disabled': currentPage == 1}" @click="changeCurrentPage('first')"></i>
                <i class="icon-play_arrow prev-icon" v-bind:class="{'disabled': currentPage == 1}" @click="changeCurrentPage('prev')"></i>
                <i class="icon-play_arrow" v-bind:class="{'disabled': currentPage == pageNum}" @click="changeCurrentPage('next')"></i>
                <i class="icon-skip_next" v-bind:class="{'disabled': currentPage == pageNum}" @click="changeCurrentPage('last')"></i>
                <select v-model="currentPage">
                    <option v-for="page in pageNum" v-bind:value="page">page {{ page }}</option>
                </select>
                <i class="icon-zoom_in" v-bind:class="{'disabled': zoomNum == 2}" @click="modifyZoom('in')"></i>
                <select v-model="zoomNum">
                    <option value="0.5">50%</option>
                    <option value="0.6">60%</option>
                    <option value="0.7">70%</option>
                    <option value="0.8">80%</option>
                    <option value="0.9">90%</option>
                    <option value="1.0" selected>100%</option>
                    <option value="1.1">110%</option>
                    <option value="1.2">120%</option>
                    <option value="1.3">130%</option>
                    <option value="1.4">140%</option>
                    <option value="1.5">150%</option>
                    <option value="1.6">160%</option>
                    <option value="1.7">170%</option>
                    <option value="1.8">180%</option>
                    <option value="1.9">190%</option>
                    <option value="2.0">200%</option>
                </select>
                <i class="icon-zoom_out" v-bind:class="{'disabled': zoomNum == 0.5}" @click="modifyZoom('out')"></i>
                <i class="icon-format_align_left" @click="textAlign = 'left'"></i>
                <i class="icon-format_align_center" @click="textAlign = 'center'"></i>
                <i class="icon-format_align_right" @click="textAlign = 'right'"></i>
                <i class="icon-print" @click="window.print()"></i>
            </div>
        </header>

        <aside class="noprint" width="240px" v-show="ifMenuShow">
            <nav class="tabNav">
                <ul>
                    <li v-bind:class="{ 'curr': currentNav == 0 }" @click="currentNav = 0">Page</li>
                    <li v-bind:class="{ 'curr': currentNav == 1 }" @click="currentNav = 1">Bookmark</li>
                </ul>

                <div class="clear"></div>
            </nav>

            <div class="tab-conent scrollbar" v-bind:style="{ height: asideHeight + 'px' }">

            <section v-show="currentNav == 0">
                <ul class="page-menu">
                    <li v-for="page in pageNum" v-bind:class="{ 'curr': currentPage == page }" @click="changePage(page)"><i class="icon-file-text2"></i> page {{ page }}</li>
                </ul>
            </section>

            <section v-show="currentNav == 1">
                <ul class="page-menu">
                    <li v-for="page in pageNum" v-bind:class="{ 'curr': currentPage == page }" @click="changePage(page)"><i class="icon-turned_in_not"></i> Bookmark {{ page }}</li>
                </ul>
            </section>
        </div>

        </aside>
        <div class="main scrollbar noprint"  v-bind:style="{ height: mainHeight + 'px' }" v-bind:class="{ 'mainLeftM': ifMenuShow, 'aleft': textAlign === 'left','acenter': textAlign === 'center','aright': textAlign === 'right'}">
            <div class="conent" v-html="pageContent" v-bind:style="zoomStyle"></div>

            <div class="clear"></div>
        </div>

        <!--专门只为打印的内容-->
        <div class="conent print" style="display:none" v-html="pageContent"></div>
    </div>
</body>
<!-- 先引入 Vue -->
<script src="js/vue.min.js"></script>
<script>

var app = new Vue({
        el: '#app',
        data: function() {
            return {
                // visible: false,
                isCollapse: false,
                currentNav: 0,
                activeName2: 'first',
                pageNum: 1, 
                currentPage: 1,
                pageContent: '',
                asideHeight: 300,
                mainHeight: 300,
                ifMenuShow: true,
                zoomNum: '1.0',
                textAlign: 'left',
                zoomStyle: {},
                pageDatas: ['<div style="position:absolute;top:0.000000px;left:0.000000px"><nobr><img height="1056.000000" width="816.000000" src ="bgimg/bg00001.jpg"/></nobr></div><p><span style="font-family:Arial;font-size:7.970100px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:53.028076px;left:127.201332px"><nobr>Speech and Language Processing. Daniel Jurafsky &amp; James H. Martin. Copyright © 2023. All </nobr></span><span style="position:absolute;top:67.125244px;left:127.201332px"><nobr>rights reserved. Draft of January 7, 2023. </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:11.955200px;font-style:normal;font-weight:normal;color:#808080;"><span style="position:absolute;top:159.175125px;left:131.186661px"><nobr>CHAPTER </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:59.775803px;font-style:normal;font-weight:normal;color:#0000FC;"><span style="position:absolute;top:195.646652px;left:131.186661px"><nobr>5 </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:24.787100px;font-weight:bold;color:#000000;"><span style="position:absolute;top:191.999832px;left:223.108017px"><nobr>Logistic Regression </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:italic;color:#0000FF;"><span style="position:absolute;top:292.070648px;left:239.837341px"><nobr>“And how do you know that these ﬁne begonias are not of equal importance?” </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:8.966399px;font-style:normal;font-weight:normal;color:#0000FF;"><span style="position:absolute;top:308.792969px;left:335.102661px"><nobr>Hercule Poirot, in Agatha Christie’s The Mysterious Affair at Styles </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:331.021729px;left:233.525314px"><nobr>Detective stories are as littered with clues as texts are with words. Yet for the </nobr></span><span style="position:absolute;top:346.961761px;left:213.599976px"><nobr>poor reader it can be challenging to know how to weigh the author’s clues in order </nobr></span><span style="position:absolute;top:362.901764px;left:213.599976px"><nobr>to make the crucial classiﬁcation task: deciding whodunnit. </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.222899px;font-weight:bold;color:#0000FF;"><span style="position:absolute;top:391.782867px;left:165.509323px"><nobr>logistic </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.222899px;font-weight:bold;color:#0000FF;"><span style="position:absolute;top:400.417572px;left:152.489319px"><nobr>regression </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:378.841736px;left:233.525314px"><nobr>In this chapter we introduce an algorithm that is admirably suited for discovering </nobr></span><span style="position:absolute;top:394.623657px;left:213.599976px"><nobr>the link between features or cues and some particular outcome: logistic regression. </nobr></span><span style="position:absolute;top:410.723083px;left:213.599976px"><nobr>Indeed, logistic regression is one of the most important analytic tools in the social </nobr></span><span style="position:absolute;top:426.663086px;left:213.599976px"><nobr>and natural sciences. In natural language processing, logistic regression is the base-</nobr></span><span style="position:absolute;top:442.603088px;left:213.599976px"><nobr>line supervised machine learning algorithm for classiﬁcation, and also has a very </nobr></span><span style="position:absolute;top:458.543091px;left:213.599976px"><nobr>close relationship with neural networks. As we will see in Chapter 7, a neural net-</nobr></span><span style="position:absolute;top:474.483032px;left:213.599976px"><nobr>work can be viewed as a series of logistic regression classiﬁers stacked on top of </nobr></span><span style="position:absolute;top:490.424377px;left:213.599976px"><nobr>each other. Thus the classiﬁcation and machine learning techniques introduced here </nobr></span><span style="position:absolute;top:506.364380px;left:213.599976px"><nobr>will play an important role throughout the book. </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:522.304321px;left:233.525314px"><nobr>Logistic regression can be used to classify an observation into one of two classes </nobr></span><span style="position:absolute;top:538.244324px;left:213.599976px"><nobr>(like ‘positive sentiment’ and ‘negative sentiment’), or into one of many classes. </nobr></span><span style="position:absolute;top:554.184326px;left:213.599976px"><nobr>Because the mathematics for the two-class case is simpler, we’ll describe this special </nobr></span><span style="position:absolute;top:570.125610px;left:213.599976px"><nobr>case of logistic regression ﬁrst in the next few sections, and then brieﬂy summarize </nobr></span><span style="position:absolute;top:585.906128px;left:213.599976px"><nobr>the use of multinomial logistic regression for more than two classes in Section <a href="#" onclick="gotoPage(7)">5.3</a>. </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:602.005615px;left:233.525330px"><nobr>We’ll introduce the mathematics of logistic regression in the next few sections. </nobr></span><span style="position:absolute;top:617.945557px;left:213.599991px"><nobr>But let’s begin with some high-level issues. </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-weight:bold;color:#0000FF;"><span style="position:absolute;top:643.510132px;left:213.599991px"><nobr>Generative and Discriminative Classiﬁers: The most important difference be-</nobr></span><span style="position:absolute;top:659.451416px;left:213.599991px"><nobr>tween naive Bayes and logistic regression is that logistic regression is a discrimina-</nobr></span><span style="position:absolute;top:675.391418px;left:213.600021px"><nobr>tive classiﬁer while naive Bayes is a generative classiﬁer. </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:691.491211px;left:233.525330px"><nobr>These are two very different frameworks for how </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:707.431152px;left:213.599991px"><nobr>to build a machine learning model. Consider a visual </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:723.371216px;left:213.599991px"><nobr>metaphor: imagine we’re trying to distinguish dog </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:739.311218px;left:213.599991px"><nobr>images from cat images. A generative model would </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:755.252502px;left:213.599991px"><nobr>have the goal of understanding what dogs look like </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:771.192566px;left:213.599991px"><nobr>and what cats look like. You might literally ask such </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:787.132507px;left:213.599991px"><nobr>a model to ‘generate’, i.e., draw, a dog. Given a test </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:803.072510px;left:213.599991px"><nobr>image, the system then asks whether it’s the cat model or the dog model that better </nobr></span><span style="position:absolute;top:819.012512px;left:213.599991px"><nobr>ﬁts (is less surprised by) the image, and chooses that as its label. </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:834.952454px;left:397.990631px"><nobr>A discriminative model, by contrast, is only try-</nobr></span><span style="position:absolute;top:850.893799px;left:378.065308px"><nobr>ing to learn to distinguish the classes (perhaps with-</nobr></span><span style="position:absolute;top:866.833801px;left:378.065308px"><nobr>out learning much about them). So maybe all the </nobr></span><span style="position:absolute;top:882.773743px;left:378.065308px"><nobr>dogs in the training data are wearing collars and the </nobr></span><span style="position:absolute;top:898.713745px;left:378.065308px"><nobr>cats aren’t. If that one feature neatly separates the </nobr></span><span style="position:absolute;top:914.653870px;left:378.065308px"><nobr>classes, the model is satisﬁed. If you ask such a </nobr></span><span style="position:absolute;top:930.593872px;left:378.065308px"><nobr>model what it knows about cats all it can say is that </nobr></span><span style="position:absolute;top:946.535156px;left:378.065308px"><nobr>they don’t wear collars. </nobr></span></span></p>','<p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-weight:bold;color:#000000;"><span style="position:absolute;top:112.375732px;left:103.201332px"><nobr>2 C HAPTER 5 </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#808080;"><span style="position:absolute;top:112.535156px;left:204.473267px"><nobr>• </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:112.535156px;left:222.738037px"><nobr>L OGISTIC R EGRESSION </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:149.817795px;left:209.525330px"><nobr>More formally, recall that the naive Bayes assigns a class c to a document d not </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:164.801437px;left:189.600021px"><nobr>by directly computing P(c|d ) but by computing a likelihood and a prior </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:217.170578px;left:337.266663px"><nobr>c ˆ = argmax </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:230.711670px;left:370.592010px"><nobr>c∈ C </nobr></span></span></p><div style="position:absolute;top:208.004959px;left:409.351471px"><nobr><img height="8.000000" width="19.000000" src ="bgimg/bg00002.jpg"/></nobr></div><div style="position:absolute;top:208.004959px;left:433.313324px"><nobr><img height="6.000000" width="47.000000" src ="bgimg/bg00003.jpg"/></nobr></div><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:195.199142px;left:400.462646px"><nobr>likelihood </nobr></span><span style="position:absolute;top:216.426834px;left:409.669342px"><nobr>P(d |c) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:193.504562px;left:460.112030px"><nobr>prior </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:217.170654px;left:460.862671px"><nobr>P(c) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:217.383224px;left:608.149292px"><nobr>(5.1) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.222899px;font-weight:bold;color:#0000FF;"><span style="position:absolute;top:251.635010px;left:127.805298px"><nobr>generative </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.222899px;font-weight:bold;color:#0000FF;"><span style="position:absolute;top:260.269684px;left:145.255966px"><nobr>model </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:253.819824px;left:189.599976px"><nobr>A generative model like naive Bayes makes use of this likelihood term, which </nobr></span><span style="position:absolute;top:269.919281px;left:189.599960px"><nobr>expresses how to generate the features of a document if we knew it was of class c. </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.222899px;font-weight:bold;color:#0000FF;"><span style="position:absolute;top:283.775055px;left:111.741310px"><nobr>discriminative </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.222899px;font-weight:bold;color:#0000FF;"><span style="position:absolute;top:292.409760px;left:145.255981px"><nobr>model </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:285.699860px;left:209.525314px"><nobr>By contrast a discriminative model in this text categorization scenario attempts </nobr></span><span style="position:absolute;top:300.844330px;left:189.599976px"><nobr>to directly compute P(c|d ). Perhaps it will learn to assign a high weight to document </nobr></span><span style="position:absolute;top:317.740723px;left:189.599991px"><nobr>features that directly improve its ability to discriminate between possible classes, </nobr></span><span style="position:absolute;top:333.680756px;left:189.599991px"><nobr>even if it couldn’t generate an example of one of the classes. </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-weight:bold;color:#0000FF;"><span style="position:absolute;top:358.323975px;left:189.599991px"><nobr>Components of a probabilistic machine learning classiﬁer: Like naive Bayes, </nobr></span><span style="position:absolute;top:374.424713px;left:189.599976px"><nobr>logistic regression is a probabilistic classiﬁer that makes use of supervised machine </nobr></span><span style="position:absolute;top:390.364685px;left:189.599976px"><nobr>learning. Machine learning classiﬁers require a training corpus of m input/output </nobr></span><span style="position:absolute;top:406.092163px;left:189.600021px"><nobr>pairs (x , y ). (We’ll use superscripts in parentheses to refer to individual instances </nobr></span><span style="position:absolute;top:422.244690px;left:189.600021px"><nobr>in the training set—for sentiment classiﬁcation each instance might be an individual </nobr></span><span style="position:absolute;top:438.184662px;left:189.600021px"><nobr>document to be classiﬁed.) A machine learning system for classiﬁcation then has </nobr></span><span style="position:absolute;top:454.124634px;left:189.600021px"><nobr>four components: </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:475.653198px;left:206.205353px"><nobr>1. A feature representation of the input. For each input observation x , this </nobr></span><span style="position:absolute;top:491.541443px;left:222.809280px"><nobr>will be a vector of features [x1 , x2 , ..., xn ]. We will generally refer to feature </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:510.801971px;left:222.809280px"><nobr>i for input x as x </nobr></span></span></p><p><span style="font-family:Arial;font-size:7.372300px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:505.959686px;left:328.565308px"><nobr>( j ) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:517.166748px;left:328.525269px"><nobr>i </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:510.801971px;left:341.309296px"><nobr>, sometimes simpliﬁed as xi </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:510.801971px;left:494.174652px"><nobr>, but we will also see the </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:526.741943px;left:222.809296px"><nobr>notation f </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:531.165405px;left:275.690643px"><nobr>i </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:526.741943px;left:279.086639px"><nobr>, f </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:531.165405px;left:291.746643px"><nobr>i </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:526.529480px;left:295.142639px"><nobr>(x), or, for multiclass classiﬁcation, f </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:531.165405px;left:493.237305px"><nobr>i </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:526.529480px;left:496.633301px"><nobr>(c, x). </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:544.268250px;left:206.205292px"><nobr>2. A classiﬁcation function that computes y ˆ , the estimated class, via p(y|x). In </nobr></span><span style="position:absolute;top:561.005188px;left:222.809372px"><nobr>the next section we will introduce the sigmoid and softmax tools for classiﬁ-</nobr></span><span style="position:absolute;top:577.104614px;left:222.809387px"><nobr>cation. </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:595.587280px;left:206.205383px"><nobr>3. An objective function for learning, usually involving minimizing error on </nobr></span><span style="position:absolute;top:611.367859px;left:222.809387px"><nobr>training examples. We will introduce the cross-entropy loss function. </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:629.849182px;left:206.205368px"><nobr>4. An algorithm for optimizing the objective function. We introduce the stochas-</nobr></span><span style="position:absolute;top:645.789185px;left:222.809402px"><nobr>tic gradient descent algorithm. </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:667.637939px;left:189.600067px"><nobr>Logistic regression has two phases: </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-weight:bold;color:#000000;"><span style="position:absolute;top:689.166504px;left:209.525421px"><nobr>training: We train the system (speciﬁcally the weights w and b) using stochastic </nobr></span><span style="position:absolute;top:705.265869px;left:222.809372px"><nobr>gradient descent and the cross-entropy loss. </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-weight:bold;color:#000000;"><span style="position:absolute;top:722.790771px;left:209.525360px"><nobr>test: Given a test example x we compute p(y|x) and return the higher probability </nobr></span><span style="position:absolute;top:739.474731px;left:222.809357px"><nobr>label y = 1 or y = 0. </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:17.215401px;font-style:normal;font-weight:normal;color:#0000FF;"><span style="position:absolute;top:783.173340px;left:103.201332px"><nobr>5.1 The sigmoid function </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:834.953857px;left:189.599991px"><nobr>The goal of binary logistic regression is to train a classiﬁer that can make a binary </nobr></span><span style="position:absolute;top:850.734497px;left:189.599991px"><nobr>decision about the class of a new input observation. Here we introduce the sigmoid </nobr></span><span style="position:absolute;top:866.833862px;left:189.599976px"><nobr>classiﬁer that will help us make this decision. </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:882.773926px;left:209.525314px"><nobr>Consider a single input observation x, which we will represent by a vector of fea-</nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:898.501282px;left:189.599976px"><nobr>tures [x1 </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:898.501282px;left:233.187988px"><nobr>, x2 </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:898.501282px;left:249.820007px"><nobr>, ..., xn </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:898.501282px;left:282.646698px"><nobr>] (we’ll show sample features in the next subsection). The classiﬁer </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:914.653870px;left:189.600021px"><nobr>output y can be 1 (meaning the observation is a member of the class) or 0 (the ob-</nobr></span><span style="position:absolute;top:929.638916px;left:189.600021px"><nobr>servation is not a member of the class). We want to know the probability P(y = 1|x) </nobr></span><span style="position:absolute;top:946.535278px;left:189.600021px"><nobr>that this observation is a member of the class. So perhaps the decision is “positive </nobr></span></span></p><div style="position:absolute;top:208.004959px;left:427.017090px"><nobr><img height="5.000000" width="7.000000" src ="bgimg/bg00004.jpg"/></nobr></div><div style="position:absolute;top:210.845871px;left:438.891022px"><nobr><img height="5.000000" width="7.000000" src ="bgimg/bg00005.jpg"/></nobr></div><div style="position:absolute;top:210.845871px;left:460.544800px"><nobr><img height="5.000000" width="7.000000" src ="bgimg/bg00006.jpg"/></nobr></div><div style="position:absolute;top:210.845871px;left:479.073669px"><nobr><img height="5.000000" width="7.000000" src ="bgimg/bg00007.jpg"/></nobr></div>','<div style="position:absolute;top:0.000000px;left:0.000000px"><nobr><img height="1056.000000" width="816.000000" src ="bgimg/bg00008.jpg"/></nobr></div><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:113.696449px;left:445.624023px"><nobr>5.1 • </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:113.696449px;left:495.437073px"><nobr>T HE SIGMOID FUNCTION </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-weight:bold;color:#000000;"><span style="position:absolute;top:113.537025px;left:650.958740px"><nobr>3 </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:149.817795px;left:213.600021px"><nobr>sentiment” versus “negative sentiment”, the features represent counts of words in a </nobr></span><span style="position:absolute;top:164.801437px;left:213.600021px"><nobr>document, P(y = 1|x) is the probability that the document has positive sentiment, </nobr></span><span style="position:absolute;top:180.742752px;left:213.600037px"><nobr>and P(y = 0|x) is the probability that the document has negative sentiment. </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.222899px;font-weight:bold;color:#0000FF;"><span style="position:absolute;top:311.453705px;left:155.618729px"><nobr>bias term </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.222899px;font-weight:bold;color:#0000FF;"><span style="position:absolute;top:326.223053px;left:157.669403px"><nobr>intercept </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:197.639160px;left:233.525360px"><nobr>Logistic regression solves this task by learning, from a training set, a vector of </nobr></span><span style="position:absolute;top:213.419754px;left:213.600037px"><nobr>weights and a bias term. Each weight wi is a real number, and is associated with one </nobr></span><span style="position:absolute;top:229.519211px;left:213.600021px"><nobr>of the input features xi . The weight wi represents how important that input feature </nobr></span><span style="position:absolute;top:245.459229px;left:213.600052px"><nobr>is to the classiﬁcation decision, and can be positive (providing evidence that the in-</nobr></span><span style="position:absolute;top:261.399261px;left:213.600052px"><nobr>stance being classiﬁed belongs in the positive class) or negative (providing evidence </nobr></span><span style="position:absolute;top:277.340576px;left:213.600052px"><nobr>that the instance being classiﬁed belongs in the negative class). Thus we might </nobr></span><span style="position:absolute;top:293.280609px;left:213.600052px"><nobr>expect in a sentiment task the word awesome to have a high positive weight, and </nobr></span><span style="position:absolute;top:309.061188px;left:213.600052px"><nobr>abysmal to have a very negative weight. The bias term, also called the intercept, is </nobr></span><span style="position:absolute;top:325.160645px;left:213.600067px"><nobr>another real number that’s added to the weighted inputs. </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:341.100677px;left:233.525421px"><nobr>To make a decision on a test instance—after we’ve learned the weights in training— </nobr></span><span style="position:absolute;top:357.040680px;left:213.600067px"><nobr>the classiﬁer ﬁrst multiplies each xi by its weight wi , sums up the weighted features, </nobr></span><span style="position:absolute;top:372.982025px;left:213.600067px"><nobr>and adds the bias term b. The resulting single number z expresses the weighted sum </nobr></span><span style="position:absolute;top:388.921997px;left:213.600067px"><nobr>of the evidence for the class. </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:423.534790px;left:377.936096px"><nobr>z = </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:409.573395px;left:427.017426px"><nobr>n </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:442.027710px;left:421.832123px"><nobr>i=1 </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:423.866913px;left:440.544098px"><nobr>wi </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:423.866913px;left:452.801422px"><nobr>xi </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:423.534821px;left:474.457458px"><nobr>+ b </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:8.966399px;font-style:normal;font-weight:normal;color:#0000FF;"><span style="position:absolute;top:424.647888px;left:634.694763px"><nobr>(5.2) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.222899px;font-weight:bold;color:#0000FF;"><span style="position:absolute;top:463.056488px;left:146.152100px"><nobr>dot product </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:461.574585px;left:213.600098px"><nobr>In the rest of the book we’ll represent such sums using the dot product notation </nobr></span><span style="position:absolute;top:476.717621px;left:213.600098px"><nobr>from linear algebra. The dot product of two vectors a and b, written as a · b, is the </nobr></span><span style="position:absolute;top:493.614044px;left:213.600052px"><nobr>sum of the products of the corresponding elements of each vector. (Notice that we </nobr></span><span style="position:absolute;top:509.394592px;left:213.600052px"><nobr>represent vectors using the boldface notation b). Thus the following is an equivalent </nobr></span><span style="position:absolute;top:525.493958px;left:213.600037px"><nobr>formation to Eq. <a href="#" onclick="gotoPage(2)">5.2</a>: </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:550.544189px;left:397.758698px"><nobr>z = w · x + b </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:8.966399px;font-style:normal;font-weight:normal;color:#0000FF;"><span style="position:absolute;top:552.401184px;left:634.694641px"><nobr>(5.3) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:577.505981px;left:213.600021px"><nobr>But note that nothing in Eq. <a href="#" onclick="gotoPage(2)">5.3 </a>forces z to be a legal probability, that is, to lie </nobr></span><span style="position:absolute;top:593.445984px;left:213.600037px"><nobr>between 0 and 1. In fact, since weights are real-valued, the output might even be </nobr></span><span style="position:absolute;top:608.430908px;left:213.600037px"><nobr>negative; z ranges from −∞ to ∞. </nobr></span></span></p><p><span style="background:#000000;font-family:Nimbus Roman;font-size:8.966399px;font-weight:bold;color:#FFFFFF;"><span style="position:absolute;top:773.896362px;left:214.929337px"><nobr>Figure 5.1 The sigmoid function σ (z) = −z takes a real value and maps it to the range </nobr></span><span style="position:absolute;top:788.508362px;left:213.599976px"><nobr>(0, 1). It is nearly linear around 0 but outlier values get squashed toward 0 or 1. </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.222899px;font-weight:bold;color:#0000FF;"><span style="position:absolute;top:819.180237px;left:162.291946px"><nobr>sigmoid </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.222899px;font-weight:bold;color:#0000FF;"><span style="position:absolute;top:847.657532px;left:165.509323px"><nobr>logistic </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.222899px;font-weight:bold;color:#0000FF;"><span style="position:absolute;top:856.292236px;left:160.154648px"><nobr>function </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:817.649231px;left:233.525314px"><nobr>To create a probability, we’ll pass z through the sigmoid function, σ (z). The </nobr></span><span style="position:absolute;top:833.642395px;left:213.599960px"><nobr>sigmoid function (named because it looks like an s) is also called the logistic func-</nobr></span><span style="position:absolute;top:849.582336px;left:213.599976px"><nobr>tion, and gives logistic regression its name. The sigmoid has the following equation, </nobr></span><span style="position:absolute;top:865.683105px;left:213.599991px"><nobr>shown graphically in Fig. <a href="#" onclick="gotoPage(2)">5.1</a>: </nobr></span></span></p><p><span style="font-family:Standard Symbols PS;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:900.618591px;left:350.937347px"><nobr>σ (z) = </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:891.837769px;left:409.692017px"><nobr>1 </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:909.730652px;left:393.674683px"><nobr>1 + e </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:900.618591px;left:436.895996px"><nobr>= </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:891.837769px;left:481.895996px"><nobr>1 </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:908.986755px;left:451.763947px"><nobr>1 + exp (−z) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:900.831177px;left:632.149231px"><nobr>(5.4) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:930.381287px;left:233.525284px"><nobr>(For the rest of the book, we’ll use the notation exp(x) to mean e .) The sigmoid </nobr></span><span style="position:absolute;top:946.535156px;left:213.599930px"><nobr>has a number of advantages; it takes a real-valued number and maps it into the range </nobr></span></span></p>','<p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-weight:bold;color:#000000;"><span style="position:absolute;top:112.375732px;left:103.201332px"><nobr>4 C HAPTER 5 </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#808080;"><span style="position:absolute;top:112.535156px;left:204.473267px"><nobr>• </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:112.535156px;left:222.738037px"><nobr>L OGISTIC R EGRESSION </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:149.605225px;left:189.599976px"><nobr>(0, 1), which is just what we want for a probability. Because it is nearly linear around </nobr></span><span style="position:absolute;top:165.757812px;left:189.599991px"><nobr>0 but ﬂattens toward the ends, it tends to squash outlier values toward 0 or 1. And </nobr></span><span style="position:absolute;top:181.699142px;left:189.599991px"><nobr>it’s differentiable, which as we’ll see in Section <a href="#" onclick="gotoPage(21)">5.10 </a>will be handy for learning. </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:197.639160px;left:209.525330px"><nobr>We’re almost there. If we apply the sigmoid to the sum of the weighted features, </nobr></span><span style="position:absolute;top:213.579178px;left:189.599991px"><nobr>we get a number between 0 and 1. To make it a probability, we just need to make </nobr></span><span style="position:absolute;top:229.306641px;left:189.599991px"><nobr>sure that the two cases, p(y = 1) and p(y = 0), sum to 1. We can do this as follows: </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:270.202789px;left:304.803925px"><nobr>P(y = 1) = σ (w · x + b) </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:297.459961px;left:359.962555px"><nobr>= </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:288.680511px;left:434.678558px"><nobr>1 </nobr></span></span></p><div style="position:absolute;top:302.161285px;left:379.181305px"><nobr><img height="3.000000" width="118.000000" src ="bgimg/bg00009.jpg"/></nobr></div><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:305.829498px;left:379.847992px"><nobr>1 + exp (−(w · x + b)) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:335.052155px;left:304.803894px"><nobr>P(y = 0) = 1 − σ (w · x + b) </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:361.566803px;left:359.962494px"><nobr>= 1 − </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:353.529877px;left:455.330505px"><nobr>1 </nobr></span></span></p><div style="position:absolute;top:367.010651px;left:399.834686px"><nobr><img height="3.000000" width="118.000000" src ="bgimg/bg00010.jpg"/></nobr></div><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:370.678711px;left:400.501343px"><nobr>1 + exp (−(w · x + b)) </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:399.011932px;left:359.962555px"><nobr>= </nobr></span></span></p><div style="position:absolute;top:403.713318px;left:379.181305px"><nobr><img height="3.000000" width="118.000000" src ="bgimg/bg00011.jpg"/></nobr></div><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:389.276031px;left:390.174561px"><nobr>exp (−(w · x + b)) </nobr></span><span style="position:absolute;top:407.381409px;left:379.847992px"><nobr>1 + exp (−(w · x + b)) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:8.966399px;font-style:normal;font-weight:normal;color:#0000FF;"><span style="position:absolute;top:400.125061px;left:610.694580px"><nobr>(5.5) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:447.745819px;left:189.599930px"><nobr>The sigmoid function has the property </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:488.428070px;left:354.341248px"><nobr>1 − σ (x) = σ (−x) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:8.966399px;font-style:normal;font-weight:normal;color:#0000FF;"><span style="position:absolute;top:490.285034px;left:610.694580px"><nobr>(5.6) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:530.068115px;left:189.599930px"><nobr>so we could also have expressed P(y = 0) as σ (−(w · x + b)). </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:17.215401px;font-style:normal;font-weight:normal;color:#0000FF;"><span style="position:absolute;top:588.581299px;left:103.201134px"><nobr>5.2 Classiﬁcation with Logistic Regression </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:640.920471px;left:189.599792px"><nobr>The sigmoid function from the prior section thus gives us a way to take an instance </nobr></span><span style="position:absolute;top:655.904053px;left:189.599792px"><nobr>x and compute the probability P(y = 1|x). </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:672.800476px;left:209.525085px"><nobr>How do we make a decision about which class to apply to a test instance x? For </nobr></span><span style="position:absolute;top:687.785339px;left:189.599731px"><nobr>a given x, we say yes if the probability P(y = 1|x) is more than .5, and no otherwise. </nobr></span><span style="position:absolute;top:704.522278px;left:189.599716px"><nobr>We call .5 the decision boundary: </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.222899px;font-weight:bold;color:#0000FF;"><span style="position:absolute;top:701.686829px;left:137.223709px"><nobr>decision </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.222899px;font-weight:bold;color:#0000FF;"><span style="position:absolute;top:710.321533px;left:130.261047px"><nobr>boundary </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:753.461243px;left:298.765045px"><nobr>decision(x) = </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:744.614685px;left:399.493073px"><nobr>1 if P(y = 1|x) &gt; 0.5 </nobr></span><span style="position:absolute;top:761.511047px;left:399.493073px"><nobr>0 otherwise </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:802.380371px;left:189.599747px"><nobr>Let’s have some examples of applying logistic regression as a classiﬁer for language </nobr></span><span style="position:absolute;top:818.320435px;left:189.599747px"><nobr>tasks. </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:11.955200px;font-weight:bold;color:#0000FF;"><span style="position:absolute;top:866.825195px;left:189.599747px"><nobr>5.2.1 Sentiment Classiﬁcation </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:898.713684px;left:189.599747px"><nobr>Suppose we are doing binary sentiment classiﬁcation on movie review text, and </nobr></span><span style="position:absolute;top:913.697388px;left:189.599747px"><nobr>we would like to know whether to assign the sentiment class + or − to a review </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:930.593750px;left:189.599731px"><nobr>document doc. We’ll represent each input observation by the 6 features x1 </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:930.381165px;left:591.681030px"><nobr>. . . x6 </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:930.593750px;left:622.534424px"><nobr>of </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:946.535034px;left:189.599686px"><nobr>the input shown in the following table; Fig. <a href="#" onclick="gotoPage(4)">5.2 </a>shows the features in a sample mini </nobr></span></span></p><div style="position:absolute;top:743.417419px;left:387.286560px"><nobr><img height="32.000000" width="7.000000" src ="bgimg/bg00012.jpg"/></nobr></div>','<p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:113.700439px;left:320.747986px"><nobr>5.2 • </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:113.700439px;left:370.560974px"><nobr>C LASSIFICATION WITH L OGISTIC R EGRESSION </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-weight:bold;color:#000000;"><span style="position:absolute;top:113.541016px;left:650.958679px"><nobr>5 </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:149.817795px;left:213.599976px"><nobr>test document. </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:176.287109px;left:255.507980px"><nobr>Var Deﬁnition Value in Fig. <a href="#" onclick="gotoPage(4)">5.2 </a></nobr></span></span></p><div style="position:absolute;top:189.143967px;left:250.856018px"><nobr><img height="3.000000" width="370.000000" src ="bgimg/bg00013.jpg"/></nobr></div><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:192.878738px;left:255.508026px"><nobr>x1 </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:192.759201px;left:527.505310px"><nobr>3 </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:208.818771px;left:255.507980px"><nobr>x2 </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:191.802811px;left:289.885376px"><nobr>count(positive lexicon words ∈ doc) </nobr></span><span style="position:absolute;top:207.742844px;left:289.885315px"><nobr>count(negative lexicon words ∈ doc) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:208.699219px;left:527.505310px"><nobr>2 </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:232.861404px;left:255.507980px"><nobr>x3 </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:223.682785px;left:303.833313px"><nobr>1 if “no” ∈ doc </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:240.579178px;left:303.833313px"><nobr>0 otherwise </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:232.741867px;left:527.505310px"><nobr>1 </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:256.638763px;left:255.507980px"><nobr>x4 </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:256.519196px;left:527.505310px"><nobr>3 </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:280.682770px;left:255.507980px"><nobr>x5 </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:255.562820px;left:289.885315px"><nobr>count(1st and 2nd pronouns ∈ doc) </nobr></span><span style="position:absolute;top:271.504150px;left:303.833313px"><nobr>1 if “!” ∈ doc </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:288.400543px;left:303.833344px"><nobr>0 otherwise </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:280.563232px;left:527.505310px"><nobr>0 </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:304.460114px;left:255.508026px"><nobr>x6 </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:304.128021px;left:289.885376px"><nobr>ln(word count of doc) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:304.128021px;left:527.505371px"><nobr>ln(66) = 4.19 </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:349.280609px;left:217.940063px"><nobr>Let’s assume for the moment that we’ve already learned a real-valued weight for </nobr></span></span></p><div style="position:absolute;top:376.717346px;left:212.933319px"><nobr><img height="3.000000" width="446.000000" src ="bgimg/bg00014.jpg"/></nobr></div><div style="position:absolute;top:377.513336px;left:212.669327px"><nobr><img height="152.000000" width="3.000000" src ="bgimg/bg00015.jpg"/></nobr></div><div style="position:absolute;top:384.820648px;left:224.401337px"><nobr><img height="137.000000" width="423.000000" src ="bgimg/bg00016.jpg"/></nobr></div><p><span style="font-family:Times New Roman;font-size:9.861361px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:423.349487px;left:227.688477px"><nobr>It\'s hokey . There are virtually no surprises , and the writing is second-rate . </nobr></span></span></p><p><span style="font-family:Times New Roman;font-size:9.861361px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:438.152252px;left:224.401337px"><nobr>So why was it so enjoyable ? For one thing , the cast is </nobr></span></span></p><p><span style="font-family:Times New Roman;font-size:9.861361px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:452.954956px;left:227.688477px"><nobr>great . Another nice touch is the music . I was overcome with the urge to get off </nobr></span><span style="position:absolute;top:467.757690px;left:227.688477px"><nobr>the couch and start dancing . It sucked me in , and it\'ll do the same to you . </nobr></span></span></p><p><span style="font-family:Times New Roman;font-size:10.272250px;font-style:normal;font-weight:normal;color:#492E9F;"><span style="position:absolute;top:499.993042px;left:299.635529px"><nobr>x =3 </nobr></span></span></p><p><span style="font-family:Times New Roman;font-size:10.272250px;font-style:normal;font-weight:normal;color:#492E9F;"><span style="position:absolute;top:500.250061px;left:418.546875px"><nobr>x =4.19 </nobr></span></span></p><p><span style="font-family:Times New Roman;font-size:10.272250px;font-style:normal;font-weight:normal;color:#492E9F;"><span style="position:absolute;top:405.967773px;left:409.959473px"><nobr>x =1 </nobr></span></span></p><p><span style="font-family:Times New Roman;font-size:10.272250px;font-style:normal;font-weight:normal;color:#492E9F;"><span style="position:absolute;top:492.254669px;left:510.422089px"><nobr>x =3 </nobr></span></span></p><p><span style="font-family:Times New Roman;font-size:10.272250px;font-style:normal;font-weight:normal;color:#492E9F;"><span style="position:absolute;top:499.993042px;left:360.652710px"><nobr>x =0 </nobr></span></span></p><p><span style="font-family:Times New Roman;font-size:10.272250px;font-style:normal;font-weight:normal;color:#492E9F;"><span style="position:absolute;top:384.669983px;left:348.326019px"><nobr>x =2 </nobr></span></span></p><div style="position:absolute;top:377.513306px;left:656.137390px"><nobr><img height="152.000000" width="3.000000" src ="bgimg/bg00017.jpg"/></nobr></div><div style="position:absolute;top:527.494629px;left:212.933350px"><nobr><img height="3.000000" width="446.000000" src ="bgimg/bg00018.jpg"/></nobr></div><p><span style="background:#000000;font-family:Nimbus Roman;font-size:8.966399px;font-weight:bold;color:#FFFFFF;"><span style="position:absolute;top:531.601501px;left:214.929337px"><nobr>Figure 5.2 </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:8.966399px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:531.744995px;left:279.798676px"><nobr>A sample mini test document showing the extracted features in the vector x. </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:564.156494px;left:213.599976px"><nobr>each of these features, and that the 6 weights corresponding to the 6 features are </nobr></span><span style="position:absolute;top:579.141357px;left:213.599976px"><nobr>[2.5, −5.0, −1.2, 0.5, 2.0, 0.7], while b = 0.1. (We’ll discuss in the next section how </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:596.037781px;left:213.599976px"><nobr>the weights are learned.) The weight w1 </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:596.037781px;left:437.726624px"><nobr>, for example indicates how important a </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:611.977783px;left:213.599930px"><nobr>feature the number of positive lexicon words (great, nice, enjoyable, etc.) is to </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:627.917725px;left:213.599899px"><nobr>a positive sentiment decision, while w2 </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:627.917725px;left:428.954529px"><nobr>tells us the importance of negative lexicon </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:643.857666px;left:213.599899px"><nobr>words. Note that w1 </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:643.645203px;left:320.983917px"><nobr>= 2.5 is positive, while w2 </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:642.901245px;left:461.369263px"><nobr>= −5.0, meaning that negative words </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:659.797668px;left:213.599899px"><nobr>are negatively associated with a positive sentiment decision, and are about twice as </nobr></span><span style="position:absolute;top:675.739014px;left:213.599899px"><nobr>important as positive words. </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:690.722534px;left:233.525223px"><nobr>Given these 6 features and the input review x, P(+|x) and P(−|x) can be com-</nobr></span><span style="position:absolute;top:707.618958px;left:213.599899px"><nobr>puted using Eq. <a href="#" onclick="gotoPage(3)">5.5</a>: </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:736.431885px;left:216.209229px"><nobr>p(+|x) = P(y = 1|x) = σ (w · x + b) </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:756.357239px;left:334.097229px"><nobr>= σ ([2.5, −5.0, −1.2, 0.5, 2.0, 0.7] · [3, 2, 1, 3, 0, 4.19] + 0.1) </nobr></span><span style="position:absolute;top:777.026489px;left:334.097046px"><nobr>= σ (.833) </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:796.951843px;left:334.097046px"><nobr>= 0.70 </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:8.966399px;font-style:normal;font-weight:normal;color:#0000FF;"><span style="position:absolute;top:798.064880px;left:634.694336px"><nobr>(5.7) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:816.133240px;left:216.209030px"><nobr>p(−|x) = P(y = 0|x) = 1 − σ (w · x + b) </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:836.802429px;left:334.096954px"><nobr>= 0.30 </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:11.955200px;font-weight:bold;color:#0000FF;"><span style="position:absolute;top:872.166443px;left:213.600952px"><nobr>5.2.2 Other classiﬁcation tasks and features </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.222899px;font-weight:bold;color:#0000FF;"><span style="position:absolute;top:911.654724px;left:167.647629px"><nobr>period </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.222899px;font-weight:bold;color:#0000FF;"><span style="position:absolute;top:920.289490px;left:130.714294px"><nobr>disambiguation </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:898.713684px;left:213.600952px"><nobr>Logistic regression is commonly applied to all sorts of NLP tasks, and any property </nobr></span><span style="position:absolute;top:914.494385px;left:213.600952px"><nobr>of the input can be a feature. Consider the task of period disambiguation: deciding </nobr></span><span style="position:absolute;top:930.593628px;left:213.600952px"><nobr>if a period is the end of a sentence or part of a word, by classifying each period </nobr></span><span style="position:absolute;top:946.534912px;left:213.600952px"><nobr>into one of two classes EOS (end-of-sentence) and not-EOS. We might use features </nobr></span></span></p><div style="position:absolute;top:222.486893px;left:291.626831px"><nobr><img height="32.000000" width="7.000000" src ="bgimg/bg00019.jpg"/></nobr></div><div style="position:absolute;top:270.306885px;left:291.626831px"><nobr><img height="32.000000" width="7.000000" src ="bgimg/bg00020.jpg"/></nobr></div>','<p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-weight:bold;color:#000000;"><span style="position:absolute;top:112.375732px;left:103.201332px"><nobr>6 C HAPTER 5 </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#808080;"><span style="position:absolute;top:112.535156px;left:204.473267px"><nobr>• </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:112.535156px;left:222.738037px"><nobr>L OGISTIC R EGRESSION </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:149.817795px;left:189.599976px"><nobr>like x1 below expressing that the current word is lower case (perhaps with a positive </nobr></span><span style="position:absolute;top:165.757812px;left:189.599976px"><nobr>weight), or that the current word is in our abbreviations dictionary (“Prof. ”) (perhaps </nobr></span><span style="position:absolute;top:181.699142px;left:189.599976px"><nobr>with a negative weight). A feature can also express a quite complex combination of </nobr></span><span style="position:absolute;top:197.639160px;left:189.599976px"><nobr>properties. For example a period following an upper case word is likely to be an </nobr></span><span style="position:absolute;top:213.579178px;left:189.599976px"><nobr>EOS, but if the word itself is St. and the previous word is capitalized, then the </nobr></span><span style="position:absolute;top:229.519211px;left:189.599976px"><nobr>period is likely part of a shortening of the word street. </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:264.096100px;left:282.989319px"><nobr>x1 </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:263.764008px;left:302.435974px"><nobr>= </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:255.659988px;left:334.674683px"><nobr>1 if “ Case(wi ) = Lower” </nobr></span><span style="position:absolute;top:271.813873px;left:334.674652px"><nobr>0 otherwise </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:301.289459px;left:282.989319px"><nobr>x2 </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:300.957367px;left:302.435974px"><nobr>= </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:292.110840px;left:334.674652px"><nobr>1 if “wi ∈ AcronymDict” </nobr></span><span style="position:absolute;top:309.007233px;left:334.674591px"><nobr>0 otherwise </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:338.484131px;left:282.989258px"><nobr>x3 </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:338.152008px;left:302.435913px"><nobr>= </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:330.049408px;left:485.957184px"><nobr>) = Cap” </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:330.049408px;left:334.674591px"><nobr>1 if “wi = St. &amp; Case(wi−1 </nobr></span><span style="position:absolute;top:346.201996px;left:334.674530px"><nobr>0 otherwise </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-weight:bold;color:#0000FF;"><span style="position:absolute;top:372.297272px;left:189.599884px"><nobr>Designing features: Features are generally designed by examining the training </nobr></span><span style="position:absolute;top:388.396637px;left:189.599884px"><nobr>set with an eye to linguistic intuitions and the linguistic literature on the domain. A </nobr></span><span style="position:absolute;top:404.336639px;left:189.599884px"><nobr>careful error analysis on the training set or devset of an early version of a system </nobr></span><span style="position:absolute;top:420.276611px;left:189.599884px"><nobr>often provides insights into features. </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:436.216583px;left:209.525208px"><nobr>For some tasks it is especially helpful to build complex features that are combi-</nobr></span><span style="position:absolute;top:452.156586px;left:189.599884px"><nobr>nations of more primitive features. We saw such a feature for period disambiguation </nobr></span><span style="position:absolute;top:468.097870px;left:189.599884px"><nobr>above, where a period on the word St. was less likely to be the end of the sentence </nobr></span><span style="position:absolute;top:484.037872px;left:189.599899px"><nobr>if the previous word was capitalized. For logistic regression and naive Bayes these </nobr></span><span style="position:absolute;top:499.818481px;left:189.599899px"><nobr>combination features or feature interactions have to be designed by hand. </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.222899px;font-weight:bold;color:#0000FF;"><span style="position:absolute;top:497.893677px;left:141.162552px"><nobr>feature </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.222899px;font-weight:bold;color:#0000FF;"><span style="position:absolute;top:506.528320px;left:121.718559px"><nobr>interactions </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.222899px;font-weight:bold;color:#0000FF;"><span style="position:absolute;top:544.804260px;left:141.162552px"><nobr>feature </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.222899px;font-weight:bold;color:#0000FF;"><span style="position:absolute;top:553.438965px;left:130.819885px"><nobr>templates </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:515.917847px;left:209.525223px"><nobr>For many tasks (especially when feature values can reference speciﬁc words) </nobr></span><span style="position:absolute;top:531.698425px;left:189.599899px"><nobr>we’ll need large numbers of features. Often these are created automatically via fea-</nobr></span><span style="position:absolute;top:547.639771px;left:189.599899px"><nobr>ture templates, abstract speciﬁcations of features. For example a bigram template </nobr></span><span style="position:absolute;top:563.739136px;left:189.599899px"><nobr>for period disambiguation might create a feature for every pair of words that occurs </nobr></span><span style="position:absolute;top:579.679138px;left:189.599899px"><nobr>before a period in the training set. Thus the feature space is sparse, since we only </nobr></span><span style="position:absolute;top:595.619141px;left:189.599899px"><nobr>have to create a feature if that n-gram exists in that position in the training set. The </nobr></span><span style="position:absolute;top:611.559143px;left:189.599899px"><nobr>feature is generally created as a hash from the string descriptions. A user description </nobr></span><span style="position:absolute;top:627.499146px;left:189.599899px"><nobr>of a feature as, “bigram(American breakfast)” is hashed into a unique integer i that </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:643.440430px;left:189.599930px"><nobr>becomes the feature number f </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:647.862549px;left:349.067963px"><nobr>i </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:643.440430px;left:352.463959px"><nobr>. </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:659.380432px;left:209.525284px"><nobr>In order to avoid the extensive human effort of feature design, recent research in </nobr></span><span style="position:absolute;top:675.160950px;left:189.599960px"><nobr>NLP has focused on representation learning: ways to learn features automatically </nobr></span><span style="position:absolute;top:691.260437px;left:189.599976px"><nobr>in an unsupervised way from the input. We’ll introduce methods for representation </nobr></span><span style="position:absolute;top:707.200378px;left:189.599976px"><nobr>learning in Chapter 6 and Chapter 7. </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-weight:bold;color:#0000FF;"><span style="position:absolute;top:731.535645px;left:189.599976px"><nobr>Scaling input features: When different input features have extremely different </nobr></span><span style="position:absolute;top:747.635071px;left:189.599976px"><nobr>ranges of values, it’s common to rescale them so they have comparable ranges. We </nobr></span><span style="position:absolute;top:763.415710px;left:189.599976px"><nobr>standardize input values by centering them to result in a zero mean and a standard </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.222899px;font-weight:bold;color:#0000FF;"><span style="position:absolute;top:765.808167px;left:122.247978px"><nobr>standardize </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:779.355652px;left:189.599976px"><nobr>deviation of one (this transformation is sometimes called the z-score). That is, if µ </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:783.938477px;left:630.203979px"><nobr>i </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.222899px;font-weight:bold;color:#0000FF;"><span style="position:absolute;top:780.738831px;left:141.701340px"><nobr>z-score </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:795.296997px;left:189.600021px"><nobr>is the mean of the values of feature xi across the m observations in the input dataset, </nobr></span><span style="position:absolute;top:811.237000px;left:189.599976px"><nobr>and σi is the standard deviation of the values of features xi across the input dataset, </nobr></span><span style="position:absolute;top:827.177002px;left:189.599960px"><nobr>we can replace each feature xi by a new feature xi computed as follows: </nobr></span></span></p><p><span style="font-family:Standard Symbols PS;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:870.283875px;left:276.470581px"><nobr>µi = </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:861.504395px;left:306.811890px"><nobr>1 </nobr></span></span></p><div style="position:absolute;top:868.658020px;left:304.671997px"><nobr><img height="9.000000" width="138.000000" src ="bgimg/bg00021.jpg"/></nobr></div><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:879.728027px;left:305.338654px"><nobr>m </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:856.322571px;left:324.044006px"><nobr>m </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:888.776855px;left:320.564026px"><nobr>j =1 </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:870.616028px;left:338.661346px"><nobr>x </nobr></span></span></p><p><span style="font-family:Arial;font-size:7.372300px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:865.654114px;left:344.600006px"><nobr>( j ) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:876.861206px;left:344.559998px"><nobr>i </nobr></span></span></p><p><span style="font-family:Standard Symbols PS;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:870.283936px;left:404.268005px"><nobr>σi = </nobr></span></span></p><div style="position:absolute;top:852.052002px;left:445.254669px"><nobr><img height="3.000000" width="103.000000" src ="bgimg/bg00022.jpg"/></nobr></div><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:861.503174px;left:448.989380px"><nobr>1 </nobr></span></span></p><div style="position:absolute;top:874.984070px;left:446.848022px"><nobr><img height="3.000000" width="11.000000" src ="bgimg/bg00023.jpg"/></nobr></div><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:879.728027px;left:447.514709px"><nobr>m </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:856.322571px;left:466.221344px"><nobr>m </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:888.776855px;left:462.741364px"><nobr>j =1 </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-weight:bold;color:#000000;"><span style="position:absolute;top:870.337097px;left:488.772034px"><nobr>x </nobr></span></span></p><p><span style="font-family:Arial;font-size:7.372300px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:865.654114px;left:495.414703px"><nobr>( j ) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:876.861206px;left:495.414703px"><nobr>i </nobr></span></span></p><p><span style="font-family:Arial Unicode MS;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:869.540039px;left:510.004028px"><nobr>− µ </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:874.918640px;left:530.153381px"><nobr>i </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:861.355469px;left:541.482666px"><nobr>2 </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-weight:bold;color:#000000;"><span style="position:absolute;top:913.690430px;left:359.113373px"><nobr>x </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:913.637268px;left:377.121338px"><nobr>= </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-weight:bold;color:#000000;"><span style="position:absolute;top:904.697083px;left:397.006683px"><nobr>xi </nobr></span></span></p><p><span style="font-family:Arial Unicode MS;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:903.900085px;left:408.890686px"><nobr>− µ </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:909.279968px;left:429.039978px"><nobr>i </nobr></span></span></p><div style="position:absolute;top:918.337341px;left:396.339996px"><nobr><img height="3.000000" width="37.000000" src ="bgimg/bg00024.jpg"/></nobr></div><p><span style="font-family:Standard Symbols PS;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:925.273132px;left:409.018677px"><nobr>σi </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:8.966399px;font-style:normal;font-weight:normal;color:#0000FF;"><span style="position:absolute;top:914.750427px;left:610.694641px"><nobr>(5.8) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:946.375732px;left:189.600021px"><nobr>Alternatively, we can normalize the input features values to lie between 0 and 1: </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.222899px;font-weight:bold;color:#0000FF;"><span style="position:absolute;top:948.766907px;left:129.210663px"><nobr>normalize </nobr></span></span></p><div style="position:absolute;top:253.720291px;left:322.468170px"><nobr><img height="32.000000" width="7.000000" src ="bgimg/bg00025.jpg"/></nobr></div><div style="position:absolute;top:290.914948px;left:322.468140px"><nobr><img height="32.000000" width="7.000000" src ="bgimg/bg00026.jpg"/></nobr></div><div style="position:absolute;top:328.108307px;left:322.468140px"><nobr><img height="32.000000" width="7.000000" src ="bgimg/bg00027.jpg"/></nobr></div><div style="position:absolute;top:826.025635px;left:451.113495px"><nobr><img height="6.000000" width="3.000000" src ="bgimg/bg00028.jpg"/></nobr></div><div style="position:absolute;top:852.984131px;left:318.741333px"><nobr><img height="33.000000" width="128.000000" src ="bgimg/bg00029.jpg"/></nobr></div><div style="position:absolute;top:876.628723px;left:433.375580px"><nobr><img height="25.000000" width="9.000000" src ="bgimg/bg00030.jpg"/></nobr></div><div style="position:absolute;top:866.882629px;left:460.918671px"><nobr><img height="19.000000" width="18.000000" src ="bgimg/bg00031.jpg"/></nobr></div><div style="position:absolute;top:864.225464px;left:483.228790px"><nobr><img height="24.000000" width="6.000000" src ="bgimg/bg00032.jpg"/></nobr></div><div style="position:absolute;top:864.225464px;left:534.026123px"><nobr><img height="24.000000" width="6.000000" src ="bgimg/bg00033.jpg"/></nobr></div><div style="position:absolute;top:911.873718px;left:366.040253px"><nobr><img height="6.000000" width="3.000000" src ="bgimg/bg00034.jpg"/></nobr></div>','<p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:113.700439px;left:320.747986px"><nobr>5.2 • </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:113.700439px;left:370.560974px"><nobr>C LASSIFICATION WITH L OGISTIC R EGRESSION </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-weight:bold;color:#000000;"><span style="position:absolute;top:113.541016px;left:650.958679px"><nobr>7 </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-weight:bold;color:#000000;"><span style="position:absolute;top:155.277176px;left:366.733307px"><nobr>x = </nobr></span></span></p><div style="position:absolute;top:159.978683px;left:403.961334px"><nobr><img height="3.000000" width="100.000000" src ="bgimg/bg00035.jpg"/></nobr></div><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-weight:bold;color:#000000;"><span style="position:absolute;top:145.541336px;left:421.218628px"><nobr>xi − min(xi ) </nobr></span><span style="position:absolute;top:163.646729px;left:404.628021px"><nobr>max(xi ) − min(xi </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:164.390549px;left:497.719940px"><nobr>) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:8.966399px;font-style:normal;font-weight:normal;color:#0000FF;"><span style="position:absolute;top:156.390305px;left:634.694580px"><nobr>(5.9) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:186.551102px;left:213.599976px"><nobr>Having input data with comparable range is useful when comparing values across </nobr></span><span style="position:absolute;top:202.491135px;left:213.599976px"><nobr>features. Data scaling is especially important in large neural networks, since it helps </nobr></span><span style="position:absolute;top:218.431152px;left:213.599976px"><nobr>speed up gradient descent. </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:11.955200px;font-weight:bold;color:#0000FF;"><span style="position:absolute;top:252.059891px;left:213.599976px"><nobr>5.2.3 Processing many examples at once </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:278.361816px;left:213.599976px"><nobr>We’ve shown the equations for logistic regression for a single example. But in prac-</nobr></span><span style="position:absolute;top:294.301849px;left:213.599976px"><nobr>tice we’ll of course want to process an entire test set with many examples. Let’s </nobr></span><span style="position:absolute;top:310.241852px;left:213.599976px"><nobr>suppose we have a test set consisting of m test examples each of which we’d like to </nobr></span><span style="position:absolute;top:326.181885px;left:213.599976px"><nobr>classify. We’ll continue to use the notation from page <a href="#" onclick="gotoPage(1)">2</a>, in which a superscript value </nobr></span><span style="position:absolute;top:342.121918px;left:213.599976px"><nobr>in parentheses refers to the example index in some set of data (either for training or </nobr></span><span style="position:absolute;top:357.106842px;left:213.599976px"><nobr>for test). So in this case each test example x has a feature vector x , 1 ≤ i ≤ m. </nobr></span><span style="position:absolute;top:374.003204px;left:213.601273px"><nobr>(As usual, we’ll represent vectors and matrices in bold.) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:389.943207px;left:458.539917px"><nobr>is just to have a for-loop, and compute </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:389.943207px;left:233.525284px"><nobr>One way to compute each output value y ˆ </nobr></span><span style="position:absolute;top:405.883179px;left:213.599960px"><nobr>each test example one at a time: </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-weight:bold;color:#000000;"><span style="position:absolute;top:429.850677px;left:327.499939px"><nobr>foreach x in input [x , x , ..., x ] </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:450.598755px;left:381.025238px"><nobr>y = σ (w · x + b) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:8.966399px;font-style:normal;font-weight:normal;color:#0000FF;"><span style="position:absolute;top:452.455719px;left:628.717163px"><nobr>(5.10) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:475.735199px;left:233.526535px"><nobr>For the ﬁrst 3 test examples, then, we would be separately computing the pre-</nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:491.675171px;left:213.601196px"><nobr>dicted y ˆ </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:491.675171px;left:269.629211px"><nobr>as follows: </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:514.898743px;left:342.254547px"><nobr>P(y = 1|x ) = σ (w · x + b) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:536.390747px;left:342.254547px"><nobr>P(y = 1|x ) = σ (w · x + b) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:557.884033px;left:342.254547px"><nobr>P(y = 1|x ) = σ (w · x + b) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:583.020508px;left:213.601212px"><nobr>But it turns out that we can slightly modify our original equation Eq. <a href="#" onclick="gotoPage(3)">5.5 </a>to do </nobr></span><span style="position:absolute;top:598.960510px;left:213.601212px"><nobr>this much more ef ﬁciently. We’ll use matrix arithmetic to assign a class to all the </nobr></span><span style="position:absolute;top:614.900513px;left:213.601212px"><nobr>examples with one matrix operation! </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:630.840454px;left:233.526550px"><nobr>First, we’ll pack all the input feature vectors for each input x into a single input </nobr></span><span style="position:absolute;top:646.620972px;left:213.601242px"><nobr>matrix X, where each row i is a row vector consisting of the feature vector for in-</nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:662.721802px;left:213.601273px"><nobr>put example x </nobr></span></span></p><p><span style="font-family:Arial;font-size:7.372300px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:660.083435px;left:289.679962px"><nobr>(i) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:662.562317px;left:304.883942px"><nobr>(i.e., the vector x </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:662.721802px;left:408.509216px"><nobr>). Assuming each example has f features and </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:677.705322px;left:213.601273px"><nobr>weights, X will therefore be a matrix of shape [m × f ], as follows: </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-weight:bold;color:#000000;"><span style="position:absolute;top:735.385193px;left:356.110596px"><nobr>X = </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-weight:bold;color:#000000;"><span style="position:absolute;top:705.737122px;left:404.803925px"><nobr>x x . . . x </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-weight:bold;color:#000000;"><span style="position:absolute;top:727.574524px;left:404.803833px"><nobr>x x . . . x </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-weight:bold;color:#000000;"><span style="position:absolute;top:749.413208px;left:432.622406px"><nobr>x . . . x </nobr></span></span></p><p><span style="font-family:Arial;font-size:7.372300px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:744.783386px;left:411.445068px"><nobr>(3) (3) (3) </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:768.009216px;left:407.733002px"><nobr>. . . </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:689.344543px;left:391.962555px"><nobr> (1) (1) (1)  </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:712.723328px;left:391.962555px"><nobr> (2) (2) (2)  </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:8.966399px;font-style:normal;font-weight:normal;color:#0000FF;"><span style="position:absolute;top:736.498291px;left:628.717041px"><nobr>(5.11) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:791.471680px;left:233.526367px"><nobr>Now if we introduce b as a vector of length m which consists of the scalar bias </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:807.339661px;left:213.601013px"><nobr>term b repeated m times, b = [b, b, ..., b], and y ˆ = [y ˆ , y ˆ ..., y ˆ </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:807.358582px;left:567.103760px"><nobr>] as the vector of </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:823.512390px;left:213.601028px"><nobr>outputs (one scalar y ˆ </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:823.353027px;left:339.727692px"><nobr>for each input x and its feature vector x ), and represent </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:839.292969px;left:213.600952px"><nobr>the weight vector w as a column vector, we can compute all the outputs with a single </nobr></span><span style="position:absolute;top:855.392395px;left:213.600952px"><nobr>matrix multiplication and one addition: </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-weight:bold;color:#000000;"><span style="position:absolute;top:879.359863px;left:398.860931px"><nobr>y = Xw + b </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:8.966399px;font-style:normal;font-weight:normal;color:#0000FF;"><span style="position:absolute;top:880.472961px;left:628.716919px"><nobr>(5.12) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:903.751099px;left:213.600906px"><nobr>You should convince yourself that Eq. <a href="#" onclick="gotoPage(6)">5.12 </a>computes the same thing as our for-loop </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:919.692383px;left:213.600906px"><nobr>in Eq. <a href="#" onclick="gotoPage(6)">5.10</a>. For example y ˆ </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:919.532959px;left:367.299591px"><nobr>, the ﬁrst entry of the output vector y, will correctly be: </nobr></span></span></p><p><span style="font-family:Arial;font-size:7.372300px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:943.232727px;left:315.099579px"><nobr>(1) </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:946.322510px;left:331.256958px"><nobr>= [x </nobr></span></span></p><p><span style="font-family:Arial;font-size:7.372300px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:941.692810px;left:354.851624px"><nobr>(1) </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:946.322510px;left:368.058258px"><nobr>, x </nobr></span></span></p><p><span style="font-family:Arial;font-size:7.372300px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:941.692810px;left:379.855591px"><nobr>(1) </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:946.322571px;left:393.062256px"><nobr>, ..., x </nobr></span></span></p><p><span style="font-family:Arial;font-size:7.372300px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:941.692810px;left:421.052887px"><nobr>(1) </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:945.578674px;left:434.259552px"><nobr>] · [w </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:950.883362px;left:458.578247px"><nobr>1 </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:946.322571px;left:464.156891px"><nobr>, w </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:950.883362px;left:478.903534px"><nobr>2 </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:946.322571px;left:484.482208px"><nobr>, ..., w </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:951.001221px;left:516.896912px"><nobr>f </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:946.322571px;left:521.727539px"><nobr>] + b </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:8.966399px;font-style:normal;font-weight:normal;color:#0000FF;"><span style="position:absolute;top:947.435669px;left:628.716919px"><nobr>(5.13) </nobr></span></span></p><div style="position:absolute;top:153.514969px;left:373.660217px"><nobr><img height="6.000000" width="3.000000" src ="bgimg/bg00036.jpg"/></nobr></div>','<p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-weight:bold;color:#000000;"><span style="position:absolute;top:112.375732px;left:103.201332px"><nobr>8 C HAPTER 5 </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#808080;"><span style="position:absolute;top:112.535156px;left:204.473267px"><nobr>• </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:112.535156px;left:222.738037px"><nobr>L OGISTIC R EGRESSION </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:149.658371px;left:189.599976px"><nobr>Note that we had to reorder X and w from the order they appeared in in Eq. <a href="#" onclick="gotoPage(3)">5.5 </a>to </nobr></span><span style="position:absolute;top:165.757812px;left:189.599930px"><nobr>make the multiplications come out properly. Here is Eq. <a href="#" onclick="gotoPage(6)">5.12 </a>again with the shapes </nobr></span><span style="position:absolute;top:181.699142px;left:189.599930px"><nobr>shown: </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-weight:bold;color:#000000;"><span style="position:absolute;top:210.282547px;left:348.817261px"><nobr>y = X w </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:210.282547px;left:455.878571px"><nobr>+ </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-weight:bold;color:#000000;"><span style="position:absolute;top:210.335693px;left:479.851929px"><nobr>b </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:229.464035px;left:314.907928px"><nobr>(m × 1) (m × f )( f × 1) (m × 1) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:8.966399px;font-style:normal;font-weight:normal;color:#0000FF;"><span style="position:absolute;top:231.320969px;left:604.717224px"><nobr>(5.14) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:259.217773px;left:209.525223px"><nobr>Modern compilers and compute hardware can compute this matrix operation </nobr></span><span style="position:absolute;top:275.157806px;left:189.599899px"><nobr>very ef ﬁciently, making the computation much faster, which becomes important </nobr></span><span style="position:absolute;top:291.097809px;left:189.599899px"><nobr>when training or testing on very large datasets. </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:11.955200px;font-weight:bold;color:#0000FF;"><span style="position:absolute;top:325.557220px;left:189.599899px"><nobr>5.2.4 Choosing a classiﬁer </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:351.859131px;left:189.599899px"><nobr>Logistic regression has a number of advantages over naive Bayes. Naive Bayes has </nobr></span><span style="position:absolute;top:367.799072px;left:189.599899px"><nobr>overly strong conditional independence assumptions. Consider two features which </nobr></span><span style="position:absolute;top:383.739105px;left:189.599899px"><nobr>are strongly correlated; in fact, imagine that we just add the same feature f 1 twice. </nobr></span><span style="position:absolute;top:399.679077px;left:189.599899px"><nobr>Naive Bayes will treat both copies of f 1 as if they were separate, multiplying them </nobr></span><span style="position:absolute;top:415.620392px;left:189.599930px"><nobr>both in, overestimating the evidence. By contrast, logistic regression is much more </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:431.560394px;left:189.599930px"><nobr>robust to correlated features; if two features f </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:435.908691px;left:437.361267px"><nobr>1 </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:431.560394px;left:447.118591px"><nobr>and f </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:435.908691px;left:476.494629px"><nobr>2 </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:431.560394px;left:486.251953px"><nobr>are perfectly correlated, re-</nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:447.500366px;left:189.599960px"><nobr>gression will simply assign part of the weight to w1 </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:447.500366px;left:481.846619px"><nobr>and part to w2 </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:447.500366px;left:561.169250px"><nobr>. Thus when </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:463.440308px;left:189.599930px"><nobr>there are many correlated features, logistic regression will assign a more accurate </nobr></span><span style="position:absolute;top:479.380310px;left:189.599930px"><nobr>probability than naive Bayes. So logistic regression generally works better on larger </nobr></span><span style="position:absolute;top:495.321655px;left:189.599930px"><nobr>documents or datasets and is a common default. </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:511.261597px;left:209.525269px"><nobr>Despite the less accurate probabilities, naive Bayes still often makes the correct </nobr></span><span style="position:absolute;top:527.201599px;left:189.599930px"><nobr>classiﬁcation decision. Furthermore, naive Bayes can work extremely well (some-</nobr></span><span style="position:absolute;top:543.141602px;left:189.599930px"><nobr>times even better than logistic regression) on very small datasets (<a href="#" onclick="gotoPage(24)">Ng </a><a href="#" onclick="gotoPage(24)">and </a><a href="#" onclick="gotoPage(24)">Jordan</a>, </nobr></span><span style="position:absolute;top:559.081604px;left:189.599930px"><nobr><a href="#" onclick="gotoPage(24)">2002</a>) or short documents (<a href="#" onclick="gotoPage(24)">Wang </a><a href="#" onclick="gotoPage(24)">and </a><a href="#" onclick="gotoPage(24)">Manning</a>, <a href="#" onclick="gotoPage(24)">2012</a>). Furthermore, naive Bayes is </nobr></span><span style="position:absolute;top:575.021606px;left:189.599930px"><nobr>easy to implement and very fast to train (there’s no optimization step). So it’s still a </nobr></span><span style="position:absolute;top:590.962891px;left:189.599930px"><nobr>reasonable approach to use in some situations. </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:17.215401px;font-style:normal;font-weight:normal;color:#0000FF;"><span style="position:absolute;top:634.474365px;left:103.201271px"><nobr>5.3 Multinomial logistic regression </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:686.254883px;left:189.599930px"><nobr>Sometimes we need more than two classes. Perhaps we might want to do 3-way </nobr></span><span style="position:absolute;top:702.194824px;left:189.599930px"><nobr>sentiment classiﬁcation (positive, negative, or neutral). Or we could be assigning </nobr></span><span style="position:absolute;top:718.134888px;left:189.599930px"><nobr>some of the labels we will introduce in Chapter 8, like the part of speech of a word </nobr></span><span style="position:absolute;top:734.074890px;left:189.599930px"><nobr>(choosing from 10, 30, or even 50 different parts of speech), or the named entity </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:750.014893px;left:189.599930px"><nobr>type of a phrase (choosing from tags like person, location, organization). </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.222899px;font-weight:bold;color:#0000FF;"><span style="position:absolute;top:758.638672px;left:120.099976px"><nobr>multinomial </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.222899px;font-weight:bold;color:#0000FF;"><span style="position:absolute;top:767.273254px;left:141.509308px"><nobr>logistic </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.222899px;font-weight:bold;color:#0000FF;"><span style="position:absolute;top:775.907959px;left:128.487976px"><nobr>regression </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:765.796753px;left:209.525269px"><nobr>In such cases we use multinomial logistic regression, also called softmax re-</nobr></span><span style="position:absolute;top:781.736816px;left:189.599976px"><nobr>gression (in older NLP literature you will sometimes see the name maxent classi-</nobr></span><span style="position:absolute;top:797.676758px;left:189.599930px"><nobr>ﬁer). In multinomial logistic regression we want to label each observation with a </nobr></span><span style="position:absolute;top:813.776184px;left:189.599930px"><nobr>class k from a set of K classes, under the stipulation that only one of these classes is </nobr></span><span style="position:absolute;top:829.556824px;left:189.599930px"><nobr>the correct one (sometimes called hard classiﬁcation; an observation can not be in </nobr></span><span style="position:absolute;top:845.496765px;left:189.599915px"><nobr>multiple classes). Let’s use the following representation: the output y for each input </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-weight:bold;color:#000000;"><span style="position:absolute;top:861.438171px;left:189.599899px"><nobr>x will be a vector of length K . If class c is the correct class, we’ll set yc </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:861.384949px;left:586.453247px"><nobr>= 1, and </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:877.378113px;left:189.599899px"><nobr>set all the other elements of y to be 0, i.e., yc </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:877.324951px;left:431.061188px"><nobr>= 1 and y </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:881.959656px;left:485.217224px"><nobr>j </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:876.581055px;left:491.877167px"><nobr>= 0 ∀ j = c. A vector like </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:893.318115px;left:189.599854px"><nobr>this y, with one value=1 and the rest 0, is called a one-hot vector. The job of the </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:909.184814px;left:189.599854px"><nobr>classiﬁer is to produce an estimate vector y ˆ . For each class k , the value y ˆ k </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:909.417480px;left:596.330505px"><nobr>will be </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:925.144958px;left:189.599808px"><nobr>the classiﬁer’s estimate of the probability p(yk </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:924.401123px;left:439.291748px"><nobr>= 1|x). </nobr></span></span></p><div style="position:absolute;top:877.031921px;left:539.161194px"><nobr><img height="13.000000" width="7.000000" src ="bgimg/bg00037.jpg"/></nobr></div>','<p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:113.696449px;left:368.395996px"><nobr>5.3 • </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:113.696449px;left:418.209045px"><nobr>M ULTINOMIAL LOGISTIC REGRESSION </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-weight:bold;color:#000000;"><span style="position:absolute;top:113.537025px;left:650.958679px"><nobr>9 </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:11.955200px;font-weight:bold;color:#0000FF;"><span style="position:absolute;top:147.825195px;left:213.599976px"><nobr>5.3.1 Softmax </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:174.127121px;left:213.599976px"><nobr>The multinomial logistic classiﬁer uses a generalization of the sigmoid, called the </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-weight:bold;color:#000000;"><span style="position:absolute;top:189.854568px;left:213.599976px"><nobr>softmax function, to compute p(yk </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:189.110764px;left:410.783966px"><nobr>= 1|x). The softmax function takes a vector </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.222899px;font-weight:bold;color:#0000FF;"><span style="position:absolute;top:192.300217px;left:161.771942px"><nobr>softmax </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-weight:bold;color:#000000;"><span style="position:absolute;top:205.794601px;left:213.599960px"><nobr>z = [z1 </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:205.794601px;left:251.083954px"><nobr>, z2 </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:205.794601px;left:267.717316px"><nobr>, ..., z </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:210.430588px;left:294.963989px"><nobr>K </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:205.794601px;left:302.723999px"><nobr>] of K arbitrary values and maps them to a probability distribution, </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:221.947189px;left:213.599976px"><nobr>with each value in the range [0,1], and all the values summing to 1. Like the sigmoid, </nobr></span><span style="position:absolute;top:237.887207px;left:213.599976px"><nobr>it is an exponential function. </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:253.669113px;left:233.525314px"><nobr>For a vector z of dimensionality K , the softmax is deﬁned as: </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:288.642670px;left:323.062622px"><nobr>softmax(zi ) = </nobr></span></span></p><div style="position:absolute;top:293.342621px;left:412.651978px"><nobr><img height="3.000000" width="77.000000" src ="bgimg/bg00038.jpg"/></nobr></div><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:279.649323px;left:430.762604px"><nobr>exp (zi ) </nobr></span><span style="position:absolute;top:299.729248px;left:428.814606px"><nobr>j =1 exp (z j ) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:287.898773px;left:495.763885px"><nobr>1 ≤ i ≤ K </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:8.966399px;font-style:normal;font-weight:normal;color:#0000FF;"><span style="position:absolute;top:289.755707px;left:628.717224px"><nobr>(5.15) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:328.753265px;left:213.599899px"><nobr>The softmax of an input vector z = [z </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:333.314117px;left:412.591888px"><nobr>1 </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:328.753265px;left:418.170532px"><nobr>, z </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:333.314117px;left:429.223877px"><nobr>2 </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:328.753265px;left:434.802521px"><nobr>, ..., zK </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:328.753265px;left:469.810547px"><nobr>] is thus a vector itself: </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:368.515961px;left:259.923889px"><nobr>softmax(z) = </nobr></span></span></p><div style="position:absolute;top:373.215973px;left:353.866638px"><nobr><img height="3.000000" width="249.000000" src ="bgimg/bg00039.jpg"/></nobr></div><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:359.523926px;left:369.165192px"><nobr>exp (z1 ) </nobr></span><span style="position:absolute;top:379.602631px;left:368.554626px"><nobr>i=1 exp (zi ) </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:368.515961px;left:427.633301px"><nobr>, </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:359.522552px;left:449.015961px"><nobr>exp (z2 ) </nobr></span><span style="position:absolute;top:379.602631px;left:448.405334px"><nobr>i=1 exp (zi ) </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:368.515961px;left:507.483948px"><nobr>, ..., </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:359.522552px;left:543.968018px"><nobr>exp (zK ) </nobr></span><span style="position:absolute;top:379.602631px;left:544.447998px"><nobr>i=1 exp (zi ) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:8.966399px;font-style:normal;font-weight:normal;color:#0000FF;"><span style="position:absolute;top:369.629059px;left:628.717224px"><nobr>(5.16) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:411.076508px;left:213.599930px"><nobr>The denominator </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:407.338623px;left:321.697296px"><nobr>K </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:417.072876px;left:321.697296px"><nobr>i=1 </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:410.863983px;left:339.122620px"><nobr>exp (zi ) is used to normalize all the values into probabilities. </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:427.016479px;left:213.599930px"><nobr>Thus for example given a vector: </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-weight:bold;color:#000000;"><span style="position:absolute;top:453.868042px;left:346.761292px"><nobr>z = [0.6, 1.1, −1.5, 1.2, 3.2, −1.1] </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:482.471741px;left:213.599792px"><nobr>the resulting (rounded) softmax(z) is </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:510.225311px;left:331.614471px"><nobr>[0.055, 0.090, 0.006, 0.099, 0.74, 0.010] </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:538.245850px;left:233.525177px"><nobr>Like the sigmoid, the softmax has the property of squashing values toward 0 or 1. </nobr></span><span style="position:absolute;top:554.185791px;left:213.601181px"><nobr>Thus if one of the inputs is larger than the others, it will tend to push its probability </nobr></span><span style="position:absolute;top:570.125732px;left:213.601181px"><nobr>toward 1, and suppress the probabilities of the smaller inputs. </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:11.955200px;font-weight:bold;color:#0000FF;"><span style="position:absolute;top:604.407837px;left:213.601181px"><nobr>5.3.2 Applying softmax in logistic regression </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:630.709717px;left:213.601181px"><nobr>When we apply softmax for logistic regression, the input will (just as for the sig-</nobr></span><span style="position:absolute;top:646.490356px;left:213.601181px"><nobr>moid) be the dot product between a weight vector w and an input vector x (plus a </nobr></span><span style="position:absolute;top:662.430298px;left:213.601242px"><nobr>bias). But now we’ll need separate weight vectors wk and bias bk for each of the K </nobr></span><span style="position:absolute;top:678.529724px;left:213.601273px"><nobr>classes. The probability of each of our output classes y ˆ k can thus be computed as: </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:725.665283px;left:337.427948px"><nobr>p(yk = 1|x) = </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:717.415833px;left:437.510529px"><nobr>exp (w </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:722.257141px;left:472.713165px"><nobr>k </nobr></span></span></p><p><span style="font-family:Arial Unicode MS;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:716.671997px;left:479.754517px"><nobr>· x + b </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:722.257141px;left:512.573120px"><nobr>k </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:717.415833px;left:517.767822px"><nobr>) </nobr></span></span></p><div style="position:absolute;top:731.109375px;left:426.592010px"><nobr><img height="3.000000" width="108.000000" src ="bgimg/bg00040.jpg"/></nobr></div><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:735.297180px;left:433.302643px"><nobr>K </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:767.752808px;left:429.823975px"><nobr>j =1 </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:749.259949px;left:447.921295px"><nobr>exp (w </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:753.894531px;left:484.598633px"><nobr>j </nobr></span></span></p><p><span style="font-family:Arial Unicode MS;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:748.516052px;left:490.085297px"><nobr>· x + b </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:753.894531px;left:524.378662px"><nobr>j </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:749.259949px;left:528.021240px"><nobr>) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:8.966399px;font-style:normal;font-weight:normal;color:#0000FF;"><span style="position:absolute;top:727.522339px;left:628.717285px"><nobr>(5.17) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:791.205811px;left:233.525314px"><nobr>The form of Eq. <a href="#" onclick="gotoPage(8)">5.17 </a>makes it seem that we would compute each output sep-</nobr></span><span style="position:absolute;top:807.145813px;left:213.599976px"><nobr>arately. Instead, it’s more common to set up the equation for more ef ﬁcient com-</nobr></span><span style="position:absolute;top:823.085754px;left:213.599976px"><nobr>putation by modern vector processing hardware. We’ll do this by representing the </nobr></span><span style="position:absolute;top:838.866394px;left:213.599976px"><nobr>set of K weight vectors as a weight matrix W and a bias vector b. Each row k of </nobr></span><span style="position:absolute;top:854.010742px;left:213.599930px"><nobr>W corresponds to the vector of weights wk . W thus has shape [K × f ], for K the </nobr></span><span style="position:absolute;top:870.747742px;left:213.599976px"><nobr>number of output classes and f the number of input features. The bias vector b has </nobr></span><span style="position:absolute;top:886.847168px;left:213.599976px"><nobr>one value for each of the K output classes. If we represent the weights in this way, </nobr></span><span style="position:absolute;top:902.568542px;left:213.600021px"><nobr>we can compute y ˆ , the vector of output probabilities for each of the K classes, by a </nobr></span><span style="position:absolute;top:918.727112px;left:213.600021px"><nobr>single elegant equation: </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-weight:bold;color:#000000;"><span style="position:absolute;top:946.315125px;left:368.982697px"><nobr>y ˆ = softmax(Wx + b) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:8.966399px;font-style:normal;font-weight:normal;color:#0000FF;"><span style="position:absolute;top:947.435791px;left:628.717346px"><nobr>(5.18) </nobr></span></span></p><div style="position:absolute;top:298.983978px;left:414.061279px"><nobr><img height="14.000000" width="13.000000" src ="bgimg/bg00041.jpg"/></nobr></div><div style="position:absolute;top:354.488190px;left:348.840881px"><nobr><img height="40.000000" width="4.000000" src ="bgimg/bg00042.jpg"/></nobr></div><div style="position:absolute;top:378.858673px;left:355.275970px"><nobr><img height="14.000000" width="13.000000" src ="bgimg/bg00043.jpg"/></nobr></div><div style="position:absolute;top:378.858673px;left:435.125275px"><nobr><img height="14.000000" width="13.000000" src ="bgimg/bg00044.jpg"/></nobr></div><div style="position:absolute;top:378.858673px;left:531.169250px"><nobr><img height="14.000000" width="13.000000" src ="bgimg/bg00045.jpg"/></nobr></div><div style="position:absolute;top:354.488190px;left:603.673889px"><nobr><img height="40.000000" width="4.000000" src ="bgimg/bg00046.jpg"/></nobr></div><div style="position:absolute;top:410.120026px;left:308.418579px"><nobr><img height="14.000000" width="13.000000" src ="bgimg/bg00047.jpg"/></nobr></div><div style="position:absolute;top:745.858643px;left:427.999939px"><nobr><img height="19.000000" width="18.000000" src ="bgimg/bg00048.jpg"/></nobr></div>','<p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-weight:bold;color:#000000;"><span style="position:absolute;top:112.375732px;left:103.201332px"><nobr>10 C HAPTER 5 </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#808080;"><span style="position:absolute;top:112.535156px;left:211.115936px"><nobr>• </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:112.535156px;left:229.380722px"><nobr>L OGISTIC R EGRESSION </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:149.817795px;left:209.525345px"><nobr>If you work out the matrix arithmetic, you can see that the estimated score of </nobr></span><span style="position:absolute;top:165.757812px;left:189.600021px"><nobr>the ﬁrst output class y ˆ 1 (before we take the softmax) will correctly turn out to be </nobr></span><span style="position:absolute;top:180.742752px;left:189.600021px"><nobr>w1 · x + b1 . </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:197.639160px;left:209.525330px"><nobr>Fig. <a href="#" onclick="gotoPage(9)">5.3 </a>shows an intuition of the role of the weight vector versus weight matrix </nobr></span><span style="position:absolute;top:213.579178px;left:189.599991px"><nobr>in the computation of the output class probabilities for binary versus multinomial </nobr></span><span style="position:absolute;top:229.519211px;left:189.599991px"><nobr>logistic regression. </nobr></span></span></p><div style="position:absolute;top:264.357269px;left:188.933319px"><nobr><img height="3.000000" width="446.000000" src ="bgimg/bg00049.jpg"/></nobr></div><div style="position:absolute;top:265.154633px;left:188.669327px"><nobr><img height="507.000000" width="3.000000" src ="bgimg/bg00050.jpg"/></nobr></div><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:272.515228px;left:339.013306px"><nobr>Binary Logistic Regression </nobr></span></span></p><div style="position:absolute;top:285.726898px;left:196.773331px"><nobr><img height="230.000000" width="365.000000" src ="bgimg/bg00051.jpg"/></nobr></div><p><span style="font-family:Times New Roman;font-size:14.723042px;font-weight:bold;color:#000000;"><span style="position:absolute;top:368.843353px;left:318.716095px"><nobr>w </nobr></span></span></p><p><span style="font-family:Times New Roman;font-size:9.201900px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:438.793365px;left:281.338562px"><nobr>[f ⨉1] </nobr></span></span></p><p><span style="font-family:Times New Roman;font-size:9.815361px;font-weight:bold;color:#000000;"><span style="position:absolute;top:324.778961px;left:214.649139px"><nobr>Output </nobr></span></span></p><p><span style="font-family:Times New Roman;font-size:9.815361px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:340.057953px;left:213.927063px"><nobr>sigmoid </nobr></span></span></p><p><span style="font-family:Times New Roman;font-size:9.201900px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:394.624268px;left:311.843903px"><nobr>[1⨉f] </nobr></span></span></p><p><span style="font-family:Times New Roman;font-size:9.815361px;font-weight:bold;color:#000000;"><span style="position:absolute;top:499.410583px;left:205.975235px"><nobr>Input words </nobr></span></span></p><p><span style="font-family:Tahoma;font-size:9.508631px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:287.771820px;left:399.215179px"><nobr>p(+) = 1- p(-) </nobr></span></span></p><p><span style="font-family:Times New Roman;font-size:11.655741px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:425.653900px;left:509.637909px"><nobr>… </nobr></span></span></p><p><span style="font-family:Times New Roman;font-size:12.269201px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:327.924805px;left:435.472748px"><nobr>y </nobr></span></span></p><p><span style="font-family:Times New Roman;font-size:14.723042px;font-weight:bold;color:#000000;"><span style="position:absolute;top:418.329193px;left:288.792358px"><nobr>x </nobr></span></span></p><p><span style="font-family:Times New Roman;font-size:14.723042px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:320.993500px;left:331.018829px"><nobr>y </nobr></span></span></p><p><span style="font-family:Times New Roman;font-size:9.815361px;font-weight:bold;color:#000000;"><span style="position:absolute;top:425.795349px;left:196.773331px"><nobr>Input feature </nobr></span></span></p><p><span style="font-family:Times New Roman;font-size:9.815361px;font-weight:bold;color:#000000;"><span style="position:absolute;top:441.074371px;left:196.773331px"><nobr>vector </nobr></span></span></p><p><span style="font-family:Times New Roman;font-size:9.201900px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:343.921875px;left:320.093811px"><nobr>[scalar] </nobr></span></span></p><p><span style="font-family:Times New Roman;font-size:9.201900px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:463.852966px;left:336.312378px"><nobr>=3 words = 1 “no” = 0 </nobr></span></span></p><div style="position:absolute;top:484.120239px;left:439.956696px"><nobr><img height="12.000000" width="123.000000" src ="bgimg/bg00052.jpg"/></nobr></div><p><span style="font-family:Times New Roman;font-size:10.428822px;font-weight:bold;color:#000000;"><span style="position:absolute;top:425.475525px;left:337.051208px"><nobr>x x x </nobr></span><span style="position:absolute;top:449.835663px;left:316.602539px"><nobr>wordcount positive lexicon count of </nobr></span></span></p><p><span style="font-family:Times New Roman;font-size:10.428822px;font-weight:bold;color:#000000;"><span style="position:absolute;top:425.475525px;left:540.654114px"><nobr>x </nobr></span></span></p><p><span style="font-family:Times New Roman;font-size:10.428822px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:431.201172px;left:547.606689px"><nobr>f </nobr></span></span></p><p><span style="font-family:Courier New;font-size:10.735551px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:498.847015px;left:345.215332px"><nobr>dessert was great </nobr></span></span></p><p><span style="font-family:Times New Roman;font-size:9.815361px;font-weight:bold;color:#000000;"><span style="position:absolute;top:377.945465px;left:204.134842px"><nobr>Weight vector </nobr></span></span></p><div style="position:absolute;top:528.699951px;left:195.445343px"><nobr><img height="3.000000" width="433.000000" src ="bgimg/bg00053.jpg"/></nobr></div><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:537.625793px;left:323.877319px"><nobr>Multinomial Logistic Regression </nobr></span></span></p><div style="position:absolute;top:550.837830px;left:196.773331px"><nobr><img height="199.000000" width="430.000000" src ="bgimg/bg00054.jpg"/></nobr></div><p><span style="font-family:Times New Roman;font-size:11.934450px;font-weight:bold;color:#000000;"><span style="position:absolute;top:622.587769px;left:300.382050px"><nobr>W </nobr></span></span></p><p><span style="font-family:Times New Roman;font-size:6.630250px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:682.992004px;left:276.008301px"><nobr>[f⨉1] </nobr></span></span></p><p><span style="font-family:Times New Roman;font-size:8.486720px;font-weight:bold;color:#000000;"><span style="position:absolute;top:581.421143px;left:207.321228px"><nobr>Output </nobr></span></span></p><p><span style="font-family:Times New Roman;font-size:8.486720px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:594.631958px;left:206.702423px"><nobr>softmax </nobr></span></span></p><p><span style="font-family:Times New Roman;font-size:6.630250px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:644.801758px;left:297.199524px"><nobr>[K⨉f] </nobr></span></span></p><p><span style="font-family:Times New Roman;font-size:8.486720px;font-weight:bold;color:#000000;"><span style="position:absolute;top:735.596558px;left:204.729630px"><nobr>Input words </nobr></span></span></p><p><span style="font-family:Tahoma;font-size:8.221510px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:552.605957px;left:364.000824px"><nobr>p(+) </nobr></span></span></p><p><span style="font-family:Times New Roman;font-size:10.077980px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:671.823792px;left:467.287537px"><nobr>… </nobr></span></span></p><p><span style="font-family:Times New Roman;font-size:10.608400px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:583.933350px;left:368.507477px"><nobr>y </nobr></span></span></p><p><span style="font-family:Times New Roman;font-size:10.608400px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:583.933350px;left:420.252930px"><nobr>y </nobr></span></span></p><p><span style="font-family:Times New Roman;font-size:8.486720px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:591.404541px;left:427.325195px"><nobr>2 </nobr></span></span></p><p><span style="font-family:Times New Roman;font-size:10.608400px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:583.933350px;left:472.006409px"><nobr>y </nobr></span></span></p><p><span style="font-family:Times New Roman;font-size:11.934450px;font-weight:bold;color:#000000;"><span style="position:absolute;top:665.374939px;left:276.336334px"><nobr>x </nobr></span></span></p><p><span style="font-family:Times New Roman;font-size:11.934450px;font-weight:bold;color:#000000;"><span style="position:absolute;top:578.032471px;left:297.199524px"><nobr>y </nobr></span></span></p><p><span style="font-family:Times New Roman;font-size:8.486720px;font-weight:bold;color:#000000;"><span style="position:absolute;top:671.946167px;left:196.773331px"><nobr>Input feature </nobr></span></span></p><p><span style="font-family:Times New Roman;font-size:8.486720px;font-weight:bold;color:#000000;"><span style="position:absolute;top:685.156921px;left:196.773331px"><nobr>vector </nobr></span></span></p><p><span style="font-family:Times New Roman;font-size:6.630250px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:600.246460px;left:292.097778px"><nobr>[K⨉1] </nobr></span></span></p><p><span style="font-family:Times New Roman;font-size:7.956300px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:688.253174px;left:430.663696px"><nobr>count of </nobr></span></span></p><p><span style="font-family:Times New Roman;font-size:7.956300px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:700.373047px;left:429.884125px"><nobr>“no” = 0 </nobr></span></span></p><p><span style="font-family:Times New Roman;font-size:7.956300px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:689.549805px;left:297.357330px"><nobr>wordcount positive lexicon </nobr></span></span></p><p><span style="font-family:Times New Roman;font-size:7.956300px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:701.669678px;left:314.399139px"><nobr>=3 words = 1 </nobr></span></span></p><p><span style="font-family:Times New Roman;font-size:8.486720px;font-weight:bold;color:#000000;"><span style="position:absolute;top:671.946167px;left:318.416321px"><nobr>x x </nobr></span></span></p><p><span style="font-family:Times New Roman;font-size:8.486720px;font-weight:bold;color:#000000;"><span style="position:absolute;top:671.946167px;left:432.987061px"><nobr>x </nobr></span></span></p><p><span style="font-family:Times New Roman;font-size:8.486720px;font-weight:bold;color:#000000;"><span style="position:absolute;top:671.946167px;left:510.312347px"><nobr>x </nobr></span></span></p><p><span style="font-family:Times New Roman;font-size:8.486720px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:676.896729px;left:515.970154px"><nobr>f </nobr></span></span></p><p><span style="font-family:Courier New;font-size:9.547560px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:735.019531px;left:329.531189px"><nobr>dessert was great </nobr></span></span></p><p><span style="font-family:Tahoma;font-size:8.221510px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:552.605957px;left:416.249359px"><nobr>p(-) p(neut) </nobr></span></span></p><p><span style="font-family:Times New Roman;font-size:8.486720px;font-weight:bold;color:#000000;"><span style="position:absolute;top:627.390869px;left:203.138367px"><nobr>Weight </nobr></span></span></p><p><span style="font-family:Times New Roman;font-size:8.486720px;font-weight:bold;color:#000000;"><span style="position:absolute;top:640.601685px;left:203.138367px"><nobr>matrix </nobr></span></span></p><p><span style="font-family:Arial;font-size:7.956300px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:587.716187px;left:525.946716px"><nobr>These f red weights </nobr></span><span style="position:absolute;top:600.198730px;left:537.838684px"><nobr>are a row of W </nobr></span></span></p><p><span style="font-family:Arial;font-size:7.956300px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:613.480591px;left:538.024353px"><nobr>corresponding </nobr></span><span style="position:absolute;top:625.910034px;left:523.888672px"><nobr>to weight vector w , </nobr></span><span style="position:absolute;top:643.778076px;left:520.610596px"><nobr>(= weights for class 3) </nobr></span></span></p><div style="position:absolute;top:265.154633px;left:632.137268px"><nobr><img height="507.000000" width="3.000000" src ="bgimg/bg00055.jpg"/></nobr></div><div style="position:absolute;top:770.512024px;left:188.933319px"><nobr><img height="3.000000" width="446.000000" src ="bgimg/bg00056.jpg"/></nobr></div><p><span style="background:#000000;font-family:Nimbus Roman;font-size:8.966399px;font-weight:bold;color:#FFFFFF;"><span style="position:absolute;top:774.618896px;left:190.929337px"><nobr>Figure 5.3 Binary versus multinomial logistic regression. Binary logistic regression uses a </nobr></span><span style="position:absolute;top:789.230957px;left:189.599991px"><nobr>single weight vector w, and has a scalar output y ˆ . In multinomial logistic regression we have </nobr></span><span style="position:absolute;top:803.986328px;left:189.599960px"><nobr>K separate weight vectors corresponding to the K classes, all packed into a single weight </nobr></span><span style="position:absolute;top:818.401062px;left:189.599960px"><nobr>matrix W, and a vector output y ˆ . </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:11.955200px;font-weight:bold;color:#0000FF;"><span style="position:absolute;top:884.775879px;left:189.599991px"><nobr>5.3.3 Features in Multinomial Logistic Regression </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:914.653870px;left:189.599991px"><nobr>Features in multinomial logistic regression act like features in binary logistic regres-</nobr></span><span style="position:absolute;top:930.593872px;left:189.599991px"><nobr>sion, with the difference mentioned above that we’ll need separate weight vectors </nobr></span><span style="position:absolute;top:946.535156px;left:189.599991px"><nobr>and biases for each of the K classes. Recall our binary exclamation point feature x5 </nobr></span></span></p>','<p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:113.707115px;left:366.070648px"><nobr>5.4 • </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:113.707115px;left:415.883667px"><nobr>L EARNING IN L OGISTIC R EGRESSION </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-weight:bold;color:#000000;"><span style="position:absolute;top:113.547691px;left:644.317261px"><nobr>11 </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:149.817795px;left:213.599976px"><nobr>from page <a href="#" onclick="gotoPage(3)">4</a>: </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:186.925217px;left:367.207947px"><nobr>x5 = </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:178.078690px;left:418.893280px"><nobr>1 if “!” ∈ doc </nobr></span><span style="position:absolute;top:194.975098px;left:418.893280px"><nobr>0 otherwise </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:223.840988px;left:233.525284px"><nobr>In binary classiﬁcation a positive weight w5 on a feature inﬂuences the classiﬁer </nobr></span><span style="position:absolute;top:239.727859px;left:213.599930px"><nobr>toward y = 1 (positive sentiment) and a negative weight inﬂuences it toward y = 0 </nobr></span><span style="position:absolute;top:255.880447px;left:213.599899px"><nobr>(negative sentiment) with the absolute value indicating how important the feature </nobr></span><span style="position:absolute;top:271.821777px;left:213.599899px"><nobr>is. For multinomial logistic regression, by contrast, with separate weights for each </nobr></span><span style="position:absolute;top:287.761810px;left:213.599899px"><nobr>class, a feature can be evidence for or against each individual class. </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:303.701813px;left:233.525223px"><nobr>In 3-way multiclass sentiment classiﬁcation, for example, we must assign each </nobr></span><span style="position:absolute;top:318.685455px;left:213.599899px"><nobr>document one of the 3 classes +, −, or 0 (neutral). Now a feature related to excla-</nobr></span><span style="position:absolute;top:335.581879px;left:213.599823px"><nobr>mation marks might have a negative weight for 0 documents, and a positive weight </nobr></span><span style="position:absolute;top:350.565521px;left:213.599823px"><nobr>for + or − documents: </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:393.645142px;left:295.105133px"><nobr>Feature Deﬁnition w </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:398.132935px;left:475.461151px"><nobr>5,+ </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-weight:bold;color:#000000;"><span style="position:absolute;top:393.645142px;left:499.370483px"><nobr>w </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:397.582428px;left:508.961151px"><nobr>5,− </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-weight:bold;color:#000000;"><span style="position:absolute;top:393.645142px;left:532.870483px"><nobr>w </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:398.132935px;left:542.461182px"><nobr>5,0 </nobr></span></span></p><div style="position:absolute;top:406.660004px;left:290.454681px"><nobr><img height="3.000000" width="275.000000" src ="bgimg/bg00057.jpg"/></nobr></div><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:418.498688px;left:297.098694px"><nobr>f </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:422.864807px;left:301.123993px"><nobr>5 </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:418.166626px;left:306.702698px"><nobr>(x) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:409.318726px;left:364.830688px"><nobr>1 if “!” ∈ doc </nobr></span><span style="position:absolute;top:426.216461px;left:364.830688px"><nobr>0 otherwise </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:418.166626px;left:465.870667px"><nobr>3.5 3.1 </nobr></span></span></p><p><span style="font-family:Arial Unicode MS;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:417.422729px;left:532.870667px"><nobr>−5.3 </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:462.121857px;left:233.525330px"><nobr>Because these feature weights are dependent both on the input text and the output </nobr></span><span style="position:absolute;top:478.061798px;left:213.600021px"><nobr>class, we sometimes make this dependence explicit and represent the features them-</nobr></span><span style="position:absolute;top:493.789276px;left:213.600021px"><nobr>selves as f (x, y): a function of both the input and the class. Using such a notation </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:510.061310px;left:215.593384px"><nobr>f </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:514.428772px;left:219.617386px"><nobr>5 </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:509.729279px;left:225.197388px"><nobr>(x) above could be represented as three features f </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:514.428772px;left:495.208130px"><nobr>5 </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:509.729279px;left:500.788086px"><nobr>(x, +), f </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:514.428772px;left:546.249390px"><nobr>5 </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:508.985352px;left:551.829346px"><nobr>(x, −), and f </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:514.428772px;left:620.658752px"><nobr>5 </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:509.729279px;left:626.237427px"><nobr>(x, 0), </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:525.883057px;left:213.600021px"><nobr>each of which has a single weight. We’ll use this kind of notation in our description </nobr></span><span style="position:absolute;top:541.823059px;left:213.600021px"><nobr>of the CRF in Chapter 8. </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:17.215401px;font-style:normal;font-weight:normal;color:#0000FF;"><span style="position:absolute;top:586.054565px;left:127.201363px"><nobr>5.4 Learning in Logistic Regression </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:637.700989px;left:213.600021px"><nobr>How are the parameters of the model, the weights w and bias b, learned? Logistic </nobr></span><span style="position:absolute;top:653.800415px;left:213.600021px"><nobr>regression is an instance of supervised classiﬁcation in which we know the correct </nobr></span><span style="position:absolute;top:669.740417px;left:213.600021px"><nobr>label y (either 0 or 1) for each observation x. What the system produces via Eq. <a href="#" onclick="gotoPage(3)">5.5 </a></nobr></span><span style="position:absolute;top:685.522278px;left:213.599991px"><nobr>is y ˆ , the system’s estimate of the true y. We want to learn parameters (meaning w </nobr></span><span style="position:absolute;top:701.621765px;left:213.600021px"><nobr>and b) that make y ˆ for each training observation as close as possible to the true y. </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:717.561707px;left:233.525330px"><nobr>This requires two components that we foreshadowed in the introduction to the </nobr></span><span style="position:absolute;top:733.501709px;left:213.600021px"><nobr>chapter. The ﬁrst is a metric for how close the current label (y ˆ ) is to the true gold </nobr></span><span style="position:absolute;top:749.441711px;left:213.600021px"><nobr>label y. Rather than measure similarity, we usually talk about the opposite of this: </nobr></span><span style="position:absolute;top:765.381653px;left:213.600021px"><nobr>the distance between the system output and the gold output, and we call this distance </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.222899px;font-weight:bold;color:#0000FF;"><span style="position:absolute;top:783.554871px;left:179.415955px"><nobr>loss </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:781.163635px;left:213.599991px"><nobr>the loss function or the cost function. In the next section we’ll introduce the loss </nobr></span><span style="position:absolute;top:797.263000px;left:213.599960px"><nobr>function that is commonly used for logistic regression and also for neural networks, </nobr></span><span style="position:absolute;top:813.043640px;left:213.599960px"><nobr>the cross-entropy loss. </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:829.143066px;left:233.525284px"><nobr>The second thing we need is an optimization algorithm for iteratively updating </nobr></span><span style="position:absolute;top:845.083008px;left:213.599930px"><nobr>the weights so as to minimize this loss function. The standard algorithm for this is </nobr></span><span style="position:absolute;top:860.863586px;left:213.599930px"><nobr>gradient descent; we’ll introduce the stochastic gradient descent algorithm in the </nobr></span><span style="position:absolute;top:876.964355px;left:213.599930px"><nobr>following section. </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:892.904419px;left:233.525284px"><nobr>We’ll describe these algorithms for the simpler case of binary logistic regres-</nobr></span><span style="position:absolute;top:908.844421px;left:213.599930px"><nobr>sion in the next two sections, and then turn to multinomial logistic regression in </nobr></span><span style="position:absolute;top:924.784302px;left:213.599930px"><nobr>Section <a href="#" onclick="gotoPage(19)">5.8</a>. </nobr></span></span></p><div style="position:absolute;top:176.882812px;left:406.688110px"><nobr><img height="32.000000" width="7.000000" src ="bgimg/bg00058.jpg"/></nobr></div><div style="position:absolute;top:408.122803px;left:352.624176px"><nobr><img height="32.000000" width="7.000000" src ="bgimg/bg00059.jpg"/></nobr></div>','<p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-weight:bold;color:#000000;"><span style="position:absolute;top:112.375732px;left:103.201332px"><nobr>12 C HAPTER 5 </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#808080;"><span style="position:absolute;top:112.535156px;left:211.115936px"><nobr>• </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:112.535156px;left:229.380722px"><nobr>L OGISTIC R EGRESSION </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:17.215401px;font-style:normal;font-weight:normal;color:#0000FF;"><span style="position:absolute;top:145.631912px;left:103.201355px"><nobr>5.5 The cross-entropy loss function </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:197.412430px;left:189.600021px"><nobr>We need a loss function that expresses, for an observation x, how close the classiﬁer </nobr></span><span style="position:absolute;top:212.396072px;left:189.600037px"><nobr>output (y ˆ = σ (w · x + b)) is to the correct output (y, which is 0 or 1). We’ll call this: </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:241.681229px;left:287.580017px"><nobr>L(y ˆ , y) = How much y ˆ differs from the true y </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:8.966399px;font-style:normal;font-weight:normal;color:#0000FF;"><span style="position:absolute;top:242.794357px;left:604.717346px"><nobr>(5.19) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:270.435150px;left:209.525345px"><nobr>We do this via a loss function that prefers the correct class labels of the train-</nobr></span><span style="position:absolute;top:286.215729px;left:189.600021px"><nobr>ing examples to be more likely. This is called conditional maximum likelihood </nobr></span><span style="position:absolute;top:302.102631px;left:189.600037px"><nobr>estimation: we choose the parameters w , b that maximize the log probability of </nobr></span><span style="position:absolute;top:318.097076px;left:189.600037px"><nobr>the true y labels in the training data given the observations x. The resulting loss </nobr></span><span style="position:absolute;top:334.037109px;left:189.600052px"><nobr>function is the negative log likelihood loss, generally called the cross-entropy loss. </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.222899px;font-weight:bold;color:#0000FF;"><span style="position:absolute;top:331.852295px;left:114.582726px"><nobr>cross-entropy </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.222899px;font-weight:bold;color:#0000FF;"><span style="position:absolute;top:340.485596px;left:155.416061px"><nobr>loss </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:350.136475px;left:209.525391px"><nobr>Let’s derive this loss function, applied to a single observation x. We’d like to </nobr></span><span style="position:absolute;top:365.120026px;left:189.600052px"><nobr>learn weights that maximize the probability of the correct label p(y|x). Since there </nobr></span><span style="position:absolute;top:382.016449px;left:189.600052px"><nobr>are only two discrete outcomes (1 or 0), this is a Bernoulli distribution, and we can </nobr></span><span style="position:absolute;top:397.001343px;left:189.600052px"><nobr>express the probability p(y|x) that our classiﬁer produces for one observation as the </nobr></span><span style="position:absolute;top:413.897736px;left:189.600052px"><nobr>following (keeping in mind that if y=1, Eq. <a href="#" onclick="gotoPage(11)">5.20 </a>simpliﬁes to y ˆ ; if y=0, Eq. <a href="#" onclick="gotoPage(11)">5.20 </a></nobr></span><span style="position:absolute;top:428.881287px;left:189.600052px"><nobr>simpliﬁes to 1 − y ˆ ): </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:457.422638px;left:348.448090px"><nobr>p(y|x) = y ˆ (1 − y ˆ ) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:8.966399px;font-style:normal;font-weight:normal;color:#0000FF;"><span style="position:absolute;top:459.279633px;left:604.717468px"><nobr>(5.20) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:486.920410px;left:189.600143px"><nobr>Now we take the log of both sides. This will turn out to be handy mathematically, </nobr></span><span style="position:absolute;top:502.860413px;left:189.600143px"><nobr>and doesn’t hurt us; whatever values maximize a probability will also maximize the </nobr></span><span style="position:absolute;top:518.800354px;left:189.600143px"><nobr>log of the probability: </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:546.385315px;left:303.562805px"><nobr>log p(y|x) = log y ˆ (1 − y ˆ ) </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:566.310669px;left:363.444183px"><nobr>= y log y ˆ + (1 − y) log(1 − y ˆ ) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:8.966399px;font-style:normal;font-weight:normal;color:#0000FF;"><span style="position:absolute;top:568.167603px;left:604.717468px"><nobr>(5.21) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:595.808350px;left:189.600143px"><nobr>Eq. <a href="#" onclick="gotoPage(11)">5.21 </a>describes a log likelihood that should be maximized. In order to turn this </nobr></span><span style="position:absolute;top:611.748352px;left:189.600143px"><nobr>into a loss function (something that we need to minimize), we’ll just ﬂip the sign on </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:627.688354px;left:189.600143px"><nobr>Eq. <a href="#" onclick="gotoPage(11)">5.21</a>. The result is the cross-entropy loss LCE </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:627.688354px;left:450.322845px"><nobr>: </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:655.273254px;left:256.037506px"><nobr>LCE (y ˆ , y) = − log p(y|x) = − [y log y ˆ + (1 − y) log(1 − y ˆ )] </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:8.966399px;font-style:normal;font-weight:normal;color:#0000FF;"><span style="position:absolute;top:657.130249px;left:604.717529px"><nobr>(5.22) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:683.814636px;left:189.600220px"><nobr>Finally, we can plug in the deﬁnition of y ˆ = σ (w · x + b): </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:713.431946px;left:237.158859px"><nobr>LCE </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:712.355957px;left:257.770874px"><nobr>(y ˆ , y) = − [y log σ (w · x + b) + (1 − y) log (1 − σ (w · x + b))] </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:8.966399px;font-style:normal;font-weight:normal;color:#0000FF;"><span style="position:absolute;top:714.212891px;left:604.717468px"><nobr>(5.23) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:741.853699px;left:189.600143px"><nobr>Let’s see if this loss function does the right thing for our example from Fig. <a href="#" onclick="gotoPage(4)">5.2</a>. We </nobr></span><span style="position:absolute;top:757.793701px;left:189.600143px"><nobr>want the loss to be smaller if the model’s estimate is close to correct, and bigger if </nobr></span><span style="position:absolute;top:773.733704px;left:189.600143px"><nobr>the model is confused. So ﬁrst let’s suppose the correct gold label for the sentiment </nobr></span><span style="position:absolute;top:789.462463px;left:189.600143px"><nobr>example in Fig. <a href="#" onclick="gotoPage(4)">5.2 </a>is positive, i.e., y = 1. In this case our model is doing well, since </nobr></span><span style="position:absolute;top:805.615051px;left:189.600128px"><nobr>from Eq. <a href="#" onclick="gotoPage(3)">5.7 </a>it indeed gave the example a higher probability of being positive (.70) </nobr></span><span style="position:absolute;top:820.598633px;left:189.600128px"><nobr>than negative (.30). If we plug σ (w · x + b) = .70 and y = 1 into Eq. <a href="#" onclick="gotoPage(11)">5.23</a>, the right </nobr></span><span style="position:absolute;top:837.494995px;left:189.600128px"><nobr>side of the equation drops out, leading to the following loss (we’ll use log to mean </nobr></span><span style="position:absolute;top:853.435059px;left:189.600128px"><nobr>natural log when the base is not speciﬁed): </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:882.095947px;left:226.126785px"><nobr>L </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:886.452637px;left:233.513458px"><nobr>CE </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:881.763855px;left:246.740128px"><nobr>(y ˆ , y) = </nobr></span><span style="position:absolute;top:901.689209px;left:276.949341px"><nobr>= </nobr></span></span></p><p><span style="font-family:Arial Unicode MS;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:881.020020px;left:323.797455px"><nobr>−[y log σ (w · x + b) + (1 − y) log (1 − σ (w · x + b))] </nobr></span><span style="position:absolute;top:900.945312px;left:326.015991px"><nobr>− [log σ (w · x + b)] </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:921.614502px;left:276.949280px"><nobr>= </nobr></span></span></p><p><span style="font-family:Arial Unicode MS;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:920.870667px;left:371.586639px"><nobr>− log(.70) </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:941.539917px;left:276.949310px"><nobr>= .36 </nobr></span></span></p><div style="position:absolute;top:545.056030px;left:402.870178px"><nobr><img height="16.000000" width="3.000000" src ="bgimg/bg00060.jpg"/></nobr></div><div style="position:absolute;top:545.056030px;left:474.590698px"><nobr><img height="16.000000" width="3.000000" src ="bgimg/bg00061.jpg"/></nobr></div>','<p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:113.696449px;left:459.770630px"><nobr>5.6 • </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:113.696449px;left:509.583618px"><nobr>G RADIENT D ESCENT </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-weight:bold;color:#000000;"><span style="position:absolute;top:113.537025px;left:644.317261px"><nobr>13 </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:149.817795px;left:213.599976px"><nobr>By contrast, let’s pretend instead that the example in Fig. <a href="#" onclick="gotoPage(4)">5.2 </a>was actually negative, </nobr></span><span style="position:absolute;top:165.545242px;left:213.599976px"><nobr>i.e., y = 0 (perhaps the reviewer went on to say “But bottom line, the movie is </nobr></span><span style="position:absolute;top:181.699142px;left:213.599960px"><nobr>terrible! I beg you not to see it!”). In this case our model is confused and we’d want </nobr></span><span style="position:absolute;top:196.682785px;left:213.599960px"><nobr>the loss to be higher. Now if we plug y = 0 and 1 − σ (w · x + b) = .31 from Eq. <a href="#" onclick="gotoPage(3)">5.7 </a></nobr></span><span style="position:absolute;top:213.579178px;left:213.599976px"><nobr>into Eq. <a href="#" onclick="gotoPage(11)">5.23</a>, the left side of the equation drops out: </nobr></span></span></p><p><span style="font-family:Arial Unicode MS;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:239.966797px;left:350.257263px"><nobr>−[y log σ (w · x + b)+(1 − y) log (1 − σ (w · x + b))] </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:240.710617px;left:251.357315px"><nobr>LCE (y ˆ , y) = </nobr></span><span style="position:absolute;top:260.635895px;left:302.179810px"><nobr>= </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:280.561188px;left:302.179749px"><nobr>= </nobr></span></span></p><p><span style="font-family:Arial Unicode MS;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:259.892090px;left:466.255798px"><nobr>− [log (1 − σ (w · x + b))] </nobr></span><span style="position:absolute;top:279.817383px;left:466.255737px"><nobr>− log (.30) </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:300.486481px;left:302.179779px"><nobr>= </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:300.486481px;left:464.411774px"><nobr>1.2 </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:328.043060px;left:213.601120px"><nobr>Sure enough, the loss for the ﬁrst classiﬁer (.36) is less than the loss for the second </nobr></span><span style="position:absolute;top:343.983063px;left:213.601120px"><nobr>classiﬁer (1.2). </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:359.923096px;left:233.526443px"><nobr>Why does minimizing this negative log probability do what we want? A per-</nobr></span><span style="position:absolute;top:375.863068px;left:213.601120px"><nobr>fect classiﬁer would assign probability 1 to the correct outcome (y=1 or y=0) and </nobr></span><span style="position:absolute;top:391.803070px;left:213.601120px"><nobr>probability 0 to the incorrect outcome. That means if y equals 1, the higher y ˆ is </nobr></span><span style="position:absolute;top:407.744385px;left:213.601151px"><nobr>(the closer it is to 1), the better the classiﬁer; the lower y ˆ is (the closer it is to 0), </nobr></span><span style="position:absolute;top:422.727936px;left:213.601135px"><nobr>the worse the classiﬁer. If y equals 0, instead, the higher 1 − y ˆ is (closer to 1), the </nobr></span><span style="position:absolute;top:438.667908px;left:213.601151px"><nobr>better the classiﬁer. The negative log of y ˆ (if the true y equals 1) or 1 − y ˆ (if the true </nobr></span><span style="position:absolute;top:455.564331px;left:213.601196px"><nobr>y equals 0) is a convenient loss metric since it goes from 0 (negative log of 1, no </nobr></span><span style="position:absolute;top:471.504333px;left:213.601196px"><nobr>loss) to inﬁnity (negative log of 0, inﬁnite loss). This loss function also ensures that </nobr></span><span style="position:absolute;top:487.444275px;left:213.601196px"><nobr>as the probability of the correct answer is maximized, the probability of the incor-</nobr></span><span style="position:absolute;top:503.385620px;left:213.601196px"><nobr>rect answer is minimized; since the two sum to one, any increase in the probability </nobr></span><span style="position:absolute;top:519.325623px;left:213.601196px"><nobr>of the correct answer is coming at the expense of the incorrect answer. It’s called </nobr></span><span style="position:absolute;top:535.106201px;left:213.601196px"><nobr>the cross-entropy loss, because Eq. <a href="#" onclick="gotoPage(11)">5.21 </a>is also the formula for the cross-entropy </nobr></span><span style="position:absolute;top:551.205566px;left:213.601196px"><nobr>between the true probability distribution y and our estimated distribution y ˆ . </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:567.145569px;left:233.526535px"><nobr>Now we know what we want to minimize; in the next section, we’ll see how to </nobr></span><span style="position:absolute;top:583.085510px;left:213.601196px"><nobr>ﬁnd the minimum. </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:17.215401px;font-style:normal;font-weight:normal;color:#0000FF;"><span style="position:absolute;top:626.337036px;left:127.201202px"><nobr>5.6 Gradient Descent </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:678.116150px;left:213.601196px"><nobr>Our goal with gradient descent is to ﬁnd the optimal weights: minimize the loss </nobr></span><span style="position:absolute;top:694.057434px;left:213.601196px"><nobr>function we’ve deﬁned for the model. In Eq. <a href="#" onclick="gotoPage(12)">5.24 </a>below, we’ll explicitly represent </nobr></span><span style="position:absolute;top:709.997498px;left:213.601196px"><nobr>the fact that the loss function L is parameterized by the weights, which we’ll refer </nobr></span><span style="position:absolute;top:725.724915px;left:213.601196px"><nobr>to in machine learning in general as θ (in the case of logistic regression θ = w , b). </nobr></span><span style="position:absolute;top:741.877502px;left:213.601151px"><nobr>So the goal is to ﬁnd the set of weights which minimizes the loss function, averaged </nobr></span><span style="position:absolute;top:757.817566px;left:213.601151px"><nobr>over all examples: </nobr></span></span></p><p><span style="font-family:Standard Symbols PS;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:797.940857px;left:330.598511px"><nobr>θ </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:795.416992px;left:347.017181px"><nobr>= argmin </nobr></span></span></p><p><span style="font-family:Standard Symbols PS;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:811.538879px;left:380.878510px"><nobr>θ </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:786.636230px;left:407.245178px"><nobr>1 </nobr></span></span></p><div style="position:absolute;top:800.117371px;left:405.103943px"><nobr><img height="3.000000" width="11.000000" src ="bgimg/bg00062.jpg"/></nobr></div><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:804.861328px;left:405.770630px"><nobr>m </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:781.454590px;left:424.475952px"><nobr>m </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:813.910156px;left:420.382599px"><nobr>i=1 </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:795.417236px;left:439.094604px"><nobr>LCE ( f (x ; θ ), y ) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:8.966399px;font-style:normal;font-weight:normal;color:#0000FF;"><span style="position:absolute;top:796.530396px;left:628.717285px"><nobr>(5.24) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:834.952454px;left:213.599976px"><nobr>How shall we ﬁnd the minimum of this (or any) loss function? Gradient descent is a </nobr></span><span style="position:absolute;top:850.893799px;left:213.599976px"><nobr>method that ﬁnds a minimum of a function by ﬁguring out in which direction (in the </nobr></span><span style="position:absolute;top:866.833801px;left:213.599976px"><nobr>space of the parameters θ ) the function’s slope is rising the most steeply, and moving </nobr></span><span style="position:absolute;top:882.773743px;left:213.600021px"><nobr>in the opposite direction. The intuition is that if you are hiking in a canyon and trying </nobr></span><span style="position:absolute;top:898.713745px;left:213.600021px"><nobr>to descend most quickly down to the river at the bottom, you might look around </nobr></span><span style="position:absolute;top:914.653870px;left:213.600021px"><nobr>yourself 360 degrees, ﬁnd the direction where the ground is sloping the steepest, </nobr></span><span style="position:absolute;top:930.593872px;left:213.600021px"><nobr>and walk downhill in that direction. </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:946.375732px;left:233.525330px"><nobr>For logistic regression, this loss function is conveniently convex. A convex func-</nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.222899px;font-weight:bold;color:#0000FF;"><span style="position:absolute;top:947.758911px;left:166.530685px"><nobr>convex </nobr></span></span></p><div style="position:absolute;top:792.015930px;left:419.173248px"><nobr><img height="19.000000" width="18.000000" src ="bgimg/bg00063.jpg"/></nobr></div>','<p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-weight:bold;color:#000000;"><span style="position:absolute;top:112.375732px;left:103.201332px"><nobr>14 C HAPTER 5 </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#808080;"><span style="position:absolute;top:112.535156px;left:211.115936px"><nobr>• </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:112.535156px;left:229.380722px"><nobr>L OGISTIC R EGRESSION </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:149.817795px;left:189.600021px"><nobr>tion has at most one minimum; there are no local minima to get stuck in, so gradient </nobr></span><span style="position:absolute;top:165.757812px;left:189.600021px"><nobr>descent starting from any point is guaranteed to ﬁnd the minimum. (By contrast, </nobr></span><span style="position:absolute;top:181.699142px;left:189.600021px"><nobr>the loss for multi-layer neural networks is non-convex, and gradient descent may </nobr></span><span style="position:absolute;top:197.639160px;left:189.600021px"><nobr>get stuck in local minima for neural network training and never ﬁnd the global opti-</nobr></span><span style="position:absolute;top:213.579178px;left:189.600021px"><nobr>mum.) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:229.519211px;left:209.525345px"><nobr>Although the algorithm (and the concept of gradient) are designed for direction </nobr></span><span style="position:absolute;top:245.459229px;left:189.600021px"><nobr>vectors, let’s ﬁrst consider a visualization of the case where the parameter of our </nobr></span><span style="position:absolute;top:261.399261px;left:189.600021px"><nobr>system is just a single scalar w, shown in Fig. <a href="#" onclick="gotoPage(13)">5.4</a>. </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:277.340576px;left:209.525330px"><nobr>Given a random initialization of w at some value w , and assuming the loss </nobr></span><span style="position:absolute;top:293.280609px;left:189.599930px"><nobr>function L happened to have the shape in Fig. <a href="#" onclick="gotoPage(13)">5.4</a>, we need the algorithm to tell us </nobr></span><span style="position:absolute;top:309.220612px;left:189.599930px"><nobr>whether at the next iteration we should move left (making w smaller than w ) or </nobr></span><span style="position:absolute;top:325.160645px;left:189.599854px"><nobr>right (making w bigger than w ) to reach the minimum. </nobr></span></span></p><div style="position:absolute;top:353.334717px;left:188.933319px"><nobr><img height="3.000000" width="446.000000" src ="bgimg/bg00064.jpg"/></nobr></div><div style="position:absolute;top:354.131989px;left:188.669327px"><nobr><img height="188.000000" width="3.000000" src ="bgimg/bg00065.jpg"/></nobr></div><div style="position:absolute;top:362.302399px;left:274.725525px"><nobr><img height="173.000000" width="315.000000" src ="bgimg/bg00066.jpg"/></nobr></div><p><span style="font-family:Times New Roman;font-size:10.605251px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:502.385742px;left:578.444580px"><nobr>w </nobr></span></span></p><p><span style="font-family:Times New Roman;font-size:10.605251px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:364.941742px;left:305.486694px"><nobr>Loss </nobr></span></span></p><p><span style="font-family:Times New Roman;font-size:10.605251px;font-style:italic;color:#000000;"><span style="position:absolute;top:517.657349px;left:378.091370px"><nobr>0 </nobr></span></span></p><p><span style="font-family:Times New Roman;font-size:10.605251px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:498.992065px;left:375.672150px"><nobr>w </nobr></span></span></p><p><span style="font-family:Times New Roman;font-size:8.484200px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:496.421326px;left:385.883881px"><nobr>1 </nobr></span></span></p><p><span style="font-family:Times New Roman;font-size:8.484200px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:496.986969px;left:438.453857px"><nobr>min </nobr></span></span></p><p><span style="font-family:Times New Roman;font-size:8.484200px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:415.845490px;left:324.699829px"><nobr>1 </nobr></span></span></p><p><span style="font-family:Times New Roman;font-size:9.756831px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:419.424103px;left:234.366592px"><nobr>slope of loss at w </nobr></span><span style="position:absolute;top:434.117096px;left:254.364395px"><nobr>is negative </nobr></span></span></p><p><span style="font-family:Times New Roman;font-size:10.605251px;font-style:italic;color:#000000;"><span style="position:absolute;top:517.657349px;left:420.343750px"><nobr>(goal) </nobr></span></span></p><p><span style="font-family:Times New Roman;font-size:8.059991px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:400.669373px;left:415.057281px"><nobr>one step </nobr></span></span></p><p><span style="font-family:Times New Roman;font-size:8.059991px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:413.003815px;left:409.390076px"><nobr>of gradient </nobr></span></span></p><p><span style="font-family:Times New Roman;font-size:8.059991px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:425.338226px;left:416.702301px"><nobr>descent </nobr></span></span></p><div style="position:absolute;top:354.131989px;left:632.137329px"><nobr><img height="188.000000" width="3.000000" src ="bgimg/bg00067.jpg"/></nobr></div><div style="position:absolute;top:540.226685px;left:188.933350px"><nobr><img height="3.000000" width="446.000000" src ="bgimg/bg00068.jpg"/></nobr></div><p><span style="background:#000000;font-family:Nimbus Roman;font-size:8.966399px;font-weight:bold;color:#FFFFFF;"><span style="position:absolute;top:544.333557px;left:190.929337px"><nobr>Figure 5.4 The ﬁrst step in iteratively ﬁnding the minimum of this loss function, by moving </nobr></span><span style="position:absolute;top:559.089050px;left:189.599991px"><nobr>w in the reverse direction from the slope of the function. Since the slope is negative, we need </nobr></span><span style="position:absolute;top:573.700989px;left:189.599991px"><nobr>to move w in a positive direction, to the right. Here superscripts are used for learning steps, </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:8.966399px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:588.312988px;left:189.599991px"><nobr>so w </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:6.973800px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:585.774292px;left:211.192001px"><nobr>1 </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:8.966399px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:588.312988px;left:219.493332px"><nobr>means the initial value of w (which is 0), w </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:8.966399px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:588.312988px;left:433.374634px"><nobr>the value at the second step, and so on. </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.222899px;font-weight:bold;color:#0000FF;"><span style="position:absolute;top:621.606873px;left:135.625290px"><nobr>gradient </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:620.129028px;left:209.525284px"><nobr>The gradient descent algorithm answers this question by ﬁnding the gradient </nobr></span><span style="position:absolute;top:636.228394px;left:189.599960px"><nobr>of the loss function at the current point and moving in the opposite direction. The </nobr></span><span style="position:absolute;top:652.169739px;left:189.599960px"><nobr>gradient of a function of many variables is a vector pointing in the direction of the </nobr></span><span style="position:absolute;top:668.109741px;left:189.599960px"><nobr>greatest increase in a function. The gradient is a multi-variable generalization of the </nobr></span><span style="position:absolute;top:684.049683px;left:189.599960px"><nobr>slope, so for a function of one variable like the one in Fig. <a href="#" onclick="gotoPage(13)">5.4</a>, we can informally </nobr></span><span style="position:absolute;top:699.989746px;left:189.599960px"><nobr>think of the gradient as the slope. The dotted line in Fig. <a href="#" onclick="gotoPage(13)">5.4 </a>shows the slope of this </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:715.717102px;left:189.599960px"><nobr>hypothetical loss function at point w = w </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:715.929688px;left:413.870605px"><nobr>. You can see that the slope of this dotted </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:731.869690px;left:189.599930px"><nobr>line is negative. Thus to ﬁnd the minimum, gradient descent tells us to go in the </nobr></span><span style="position:absolute;top:747.811035px;left:189.599930px"><nobr>opposite direction: moving w in a positive direction. </nobr></span></span></p><div style="position:absolute;top:784.178711px;left:223.525330px"><nobr><img height="3.000000" width="14.000000" src ="bgimg/bg00069.jpg"/></nobr></div><p><span style="font-family:Nimbus Roman;font-size:7.222899px;font-weight:bold;color:#0000FF;"><span style="position:absolute;top:781.009583px;left:117.317345px"><nobr>learning rate </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:763.751099px;left:209.525284px"><nobr>The magnitude of the amount to move in gradient descent is the value of the </nobr></span><span style="position:absolute;top:779.478638px;left:189.599960px"><nobr>slope L( f (x; w), y) weighted by a learning rate η . A higher (faster) learning </nobr></span><span style="position:absolute;top:795.631165px;left:189.600021px"><nobr>rate means that we should move w more on each step. The change we make in our </nobr></span><span style="position:absolute;top:811.571106px;left:189.600037px"><nobr>parameter is the learning rate times the gradient (or the slope, in our single-variable </nobr></span><span style="position:absolute;top:827.511169px;left:189.600037px"><nobr>example): </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:863.465393px;left:321.954712px"><nobr>w = w − η </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:855.548035px;left:405.265381px"><nobr>d </nobr></span></span></p><div style="position:absolute;top:868.909363px;left:400.168030px"><nobr><img height="3.000000" width="18.000000" src ="bgimg/bg00070.jpg"/></nobr></div><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:873.653320px;left:400.834686px"><nobr>dw </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:864.209290px;left:418.621338px"><nobr>L( f (x; w), y) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:8.966399px;font-style:normal;font-weight:normal;color:#0000FF;"><span style="position:absolute;top:865.322388px;left:604.717285px"><nobr>(5.25) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:898.713745px;left:194.381271px"><nobr>Now let’s extend the intuition from a function of one scalar variable w to many </nobr></span><span style="position:absolute;top:914.653870px;left:189.599899px"><nobr>variables, because we don’t just want to move left or right, we want to know where </nobr></span><span style="position:absolute;top:930.593872px;left:189.599899px"><nobr>in the N-dimensional space (of the N parameters that make up θ ) we should move. </nobr></span><span style="position:absolute;top:946.375732px;left:189.599899px"><nobr>The gradient is just such a vector; it expresses the directional components of the </nobr></span></span></p>','<div style="position:absolute;top:0.000000px;left:0.000000px"><nobr><img height="1056.000000" width="816.000000" src ="bgimg/bg00071.jpg"/></nobr></div><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:113.707115px;left:459.770630px"><nobr>5.6 • </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:113.707115px;left:509.583618px"><nobr>G RADIENT D ESCENT </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-weight:bold;color:#000000;"><span style="position:absolute;top:113.547691px;left:644.317261px"><nobr>15 </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:149.817795px;left:213.599976px"><nobr>sharpest slope along each of those N dimensions. If we’re just imagining two weight </nobr></span><span style="position:absolute;top:165.757812px;left:213.599976px"><nobr>dimensions (say for one weight w and one bias b), the gradient might be a vector with </nobr></span><span style="position:absolute;top:181.699142px;left:213.599976px"><nobr>two orthogonal components, each of which tells us how much the ground slopes in </nobr></span><span style="position:absolute;top:197.639160px;left:213.599976px"><nobr>the w dimension and in the b dimension. Fig. <a href="#" onclick="gotoPage(14)">5.5 </a>shows a visualization of the value </nobr></span><span style="position:absolute;top:213.579178px;left:213.599991px"><nobr>of a 2-dimensional gradient vector taken at the red point. </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:5.977600px;font-style:italic;color:#000000;"><span style="position:absolute;top:303.539948px;left:479.130646px"><nobr>i </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:229.519211px;left:233.525330px"><nobr>In an actual logistic regression, the parameter vector w is much longer than 1 or </nobr></span><span style="position:absolute;top:245.459229px;left:213.599991px"><nobr>2, since the input feature vector x can be quite long, and we need a weight wi for </nobr></span><span style="position:absolute;top:261.399261px;left:213.600052px"><nobr>each xi . For each dimension/variable wi in w (plus the bias b), the gradient will have </nobr></span><span style="position:absolute;top:277.340576px;left:213.600067px"><nobr>a component that tells us the slope with respect to that variable. In each dimension </nobr></span><span style="position:absolute;top:293.280426px;left:213.600067px"><nobr>wi , we express the slope as a partial derivative of the loss function. Essentially </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:309.220459px;left:213.599991px"><nobr>we’re asking: “How much would a small change in that variable wi inﬂuence the </nobr></span><span style="position:absolute;top:325.160492px;left:213.599976px"><nobr>total loss function L?” </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:341.100494px;left:233.525314px"><nobr>Formally, then, the gradient of a multi-variable function f is a vector in which </nobr></span><span style="position:absolute;top:357.040527px;left:213.599976px"><nobr>each component expresses the partial derivative of f with respect to one of the vari-</nobr></span><span style="position:absolute;top:372.981842px;left:213.599976px"><nobr>ables. We’ll use the inverted Greek delta symbol ∇ to refer to the gradient, and </nobr></span><span style="position:absolute;top:388.709320px;left:213.599976px"><nobr>represent y ˆ as f (x; θ ) to make the dependence on θ more obvious: </nobr></span></span></p><p><span style="font-family:Standard Symbols PS;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:463.878632px;left:332.371979px"><nobr>∇L( f (x; θ ), y) = </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:457.156677px;left:434.377319px"><nobr> </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:409.867340px;left:434.377319px"><nobr> ∂ </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:433.246002px;left:434.377319px"><nobr> ∂ </nobr></span></span></p><p><span style="font-family:Standard Symbols PS;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:449.958588px;left:444.826660px"><nobr>∂ w2 </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:423.223969px;left:463.701324px"><nobr>L( f (x; θ ), y) </nobr></span><span style="position:absolute;top:442.521332px;left:463.701324px"><nobr>L( f (x; θ ), y) </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:473.097992px;left:434.377319px"><nobr> ∂ </nobr></span></span></p><p><span style="font-family:Standard Symbols PS;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:493.975922px;left:444.826660px"><nobr>∂ wn </nobr></span></span></p><p><span style="font-family:Standard Symbols PS;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:504.477814px;left:450.429352px"><nobr>∂ </nobr></span></span></p><p><span style="font-family:Standard Symbols PS;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:512.883911px;left:447.972015px"><nobr>∂ b </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:505.446625px;left:460.556000px"><nobr>L( f (x; θ ), y) </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:409.867340px;left:529.973389px"><nobr> </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:449.187347px;left:529.973389px"><nobr> </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:465.127319px;left:484.942719px"><nobr>.  </nobr></span><span style="position:absolute;top:481.599335px;left:463.701324px"><nobr>L( f (x; θ ), y) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:8.966399px;font-style:normal;font-weight:normal;color:#0000FF;"><span style="position:absolute;top:464.991638px;left:628.717346px"><nobr>(5.26) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:538.443115px;left:213.600052px"><nobr>The ﬁnal equation for updating θ based on the gradient is thus </nobr></span></span></p><p><span style="font-family:Standard Symbols PS;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:570.741394px;left:354.480042px"><nobr>θ = θ − η ∇L( f (x; θ ), y) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:8.966399px;font-style:normal;font-weight:normal;color:#0000FF;"><span style="position:absolute;top:572.598389px;left:628.717346px"><nobr>(5.27) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:11.955200px;font-weight:bold;color:#0000FF;"><span style="position:absolute;top:610.770569px;left:213.600021px"><nobr>5.6.1 The Gradient for Logistic Regression </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:638.673279px;left:213.600021px"><nobr>In order to update θ , we need a deﬁnition for the gradient ∇L( f (x; θ ), y). Recall that </nobr></span><span style="position:absolute;top:654.825806px;left:213.599976px"><nobr>for logistic regression, the cross-entropy loss function is: </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:687.124023px;left:261.158630px"><nobr>LCE (y ˆ , y) = − [y log σ (w · x + b) + (1 − y) log (1 − σ (w · x + b))] </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:8.966399px;font-style:normal;font-weight:normal;color:#0000FF;"><span style="position:absolute;top:688.981018px;left:628.717224px"><nobr>(5.28) </nobr></span></span></p><p><span style="font-family:Times New Roman;font-size:10.905300px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:728.848206px;left:413.150513px"><nobr>Cost(w,b) </nobr></span></span></p><p><span style="font-family:Times New Roman;font-size:10.905300px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:874.252258px;left:373.298126px"><nobr>w </nobr></span></span></p><p><span style="font-family:Times New Roman;font-size:10.905300px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:864.558594px;left:515.470520px"><nobr>b </nobr></span></span></p><p><span style="background:#000000;font-family:Nimbus Roman;font-size:8.966399px;font-weight:bold;color:#FFFFFF;"><span style="position:absolute;top:901.333557px;left:214.929337px"><nobr>Figure 5.5 Visualization of the gradient vector at the red point in two dimensions w and </nobr></span><span style="position:absolute;top:916.088989px;left:213.599976px"><nobr>b, showing a red arrow in the x-y plane pointing in the direction we will go to look for the </nobr></span><span style="position:absolute;top:930.700928px;left:213.599976px"><nobr>minimum: the opposite direction of the gradient (recall that the gradient points in the direction </nobr></span><span style="position:absolute;top:945.313049px;left:213.599976px"><nobr>of increase not decrease). </nobr></span></span></p>','<p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-weight:bold;color:#000000;"><span style="position:absolute;top:112.375732px;left:103.201332px"><nobr>16 C HAPTER 5 </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#808080;"><span style="position:absolute;top:112.535156px;left:211.115936px"><nobr>• </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:112.535156px;left:229.380722px"><nobr>L OGISTIC R EGRESSION </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:149.817795px;left:189.600021px"><nobr>It turns out that the derivative of this function for one observation vector x is Eq. <a href="#" onclick="gotoPage(15)">5.29 </a></nobr></span><span style="position:absolute;top:165.757812px;left:189.599976px"><nobr>(the interested reader can see Section <a href="#" onclick="gotoPage(21)">5.10 </a>for the derivation of this equation): </nobr></span></span></p><div style="position:absolute;top:207.355957px;left:318.246704px"><nobr><img height="3.000000" width="58.000000" src ="bgimg/bg00072.jpg"/></nobr></div><p><span style="font-family:Standard Symbols PS;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:193.662598px;left:318.913300px"><nobr>∂ LCE (y ˆ , y) </nobr></span><span style="position:absolute;top:211.821045px;left:335.494690px"><nobr>∂ w j </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:201.910812px;left:384.557373px"><nobr>= [σ (w · x + b) − y]x </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:207.290604px;left:502.238708px"><nobr>j </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:230.346848px;left:384.557373px"><nobr>= (y ˆ − y)x </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:235.725342px;left:447.078735px"><nobr>j </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:8.966399px;font-style:normal;font-weight:normal;color:#0000FF;"><span style="position:absolute;top:232.203781px;left:604.717407px"><nobr>(5.29) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:261.192535px;left:189.600098px"><nobr>You’ll also sometimes see this equation in the equivalent form: </nobr></span></span></p><div style="position:absolute;top:302.790680px;left:340.666656px"><nobr><img height="3.000000" width="58.000000" src ="bgimg/bg00073.jpg"/></nobr></div><p><span style="font-family:Standard Symbols PS;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:289.097321px;left:341.333405px"><nobr>∂ LCE (y ˆ , y) </nobr></span><span style="position:absolute;top:307.255707px;left:357.914673px"><nobr>∂ w j </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:297.345459px;left:406.977386px"><nobr>= −(y − y ˆ )x </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:302.725250px;left:479.820038px"><nobr>j </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:8.966399px;font-style:normal;font-weight:normal;color:#0000FF;"><span style="position:absolute;top:299.202393px;left:604.717346px"><nobr>(5.30) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:335.639740px;left:209.525391px"><nobr>Note in these equations that the gradient with respect to a single weight w j rep-</nobr></span><span style="position:absolute;top:351.739166px;left:189.600052px"><nobr>resents a very intuitive value: the difference between the true y and our estimated </nobr></span><span style="position:absolute;top:366.722748px;left:189.600098px"><nobr>y ˆ = σ (w · x + b) for that observation, multiplied by the corresponding input value x j . </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:11.955200px;font-weight:bold;color:#0000FF;"><span style="position:absolute;top:402.965240px;left:189.600098px"><nobr>5.6.2 The Stochastic Gradient Descent Algorithm </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:429.567169px;left:189.600098px"><nobr>Stochastic gradient descent is an online algorithm that minimizes the loss function </nobr></span><span style="position:absolute;top:445.507172px;left:189.600098px"><nobr>by computing its gradient after each training example, and nudging θ in the right </nobr></span><span style="position:absolute;top:461.447144px;left:189.600052px"><nobr>direction (the opposite direction of the gradient). (An “online algorithm” is one that </nobr></span><span style="position:absolute;top:477.387146px;left:189.600052px"><nobr>processes its input example by example, rather than waiting until it sees the entire </nobr></span><span style="position:absolute;top:493.327087px;left:189.600052px"><nobr>input.) Fig. <a href="#" onclick="gotoPage(15)">5.6 </a>shows the algorithm. </nobr></span></span></p><div style="position:absolute;top:530.182678px;left:188.933319px"><nobr><img height="3.000000" width="446.000000" src ="bgimg/bg00074.jpg"/></nobr></div><div style="position:absolute;top:530.978699px;left:188.669327px"><nobr><img height="244.000000" width="3.000000" src ="bgimg/bg00075.jpg"/></nobr></div><p><span style="font-family:Nimbus Roman;font-size:8.966399px;font-weight:bold;color:#000000;"><span style="position:absolute;top:544.603088px;left:214.174683px"><nobr>function S TOCHASTIC G RADIENT D ESCENT (L(), f (), x, y) returns θ </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:8.966399px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:559.406433px;left:235.095978px"><nobr># where: L is the loss function </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:8.966399px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:574.017090px;left:235.095978px"><nobr># f is a function parameterized by θ </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:8.966399px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:588.629089px;left:235.096008px"><nobr># x is the set of training inputs x </nobr></span></span></p><p><span style="font-family:Arial;font-size:6.973800px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:585.942932px;left:409.471954px"><nobr>(1) </nobr></span></span></p><p><span style="font-family:Arial;font-size:8.966399px;font-style:italic;color:#000000;"><span style="position:absolute;top:588.437744px;left:422.001312px"><nobr>, x </nobr></span></span></p><p><span style="font-family:Arial;font-size:6.973800px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:585.942932px;left:434.973267px"><nobr>(2) </nobr></span></span></p><p><span style="font-family:Arial;font-size:8.966399px;font-style:italic;color:#000000;"><span style="position:absolute;top:588.437744px;left:447.502594px"><nobr>, ..., x </nobr></span></span></p><p><span style="font-family:Arial;font-size:6.973800px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:585.942932px;left:475.049255px"><nobr>(m) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:8.966399px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:603.049805px;left:235.095947px"><nobr># y is the set of training outputs (labels) y , y , ..., y </nobr></span></span></p><p><span style="font-family:Standard Symbols PS;font-size:8.966399px;font-style:italic;color:#000000;"><span style="position:absolute;top:626.593628px;left:214.174606px"><nobr>θ ← 0 </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:8.966399px;font-weight:bold;color:#000000;"><span style="position:absolute;top:641.921631px;left:214.174606px"><nobr>repeat til done # see caption </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:8.966399px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:656.485779px;left:226.130585px"><nobr>For each training tuple (x , y ) (in random order) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:8.966399px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:671.289062px;left:238.085266px"><nobr>1. Optional (for reporting): # How are we doing on this tuple? </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:8.966399px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:685.901062px;left:250.039932px"><nobr>Compute y ˆ </nobr></span></span></p><p><span style="font-family:Arial;font-size:6.973800px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:683.213562px;left:303.467926px"><nobr>(i) </nobr></span></span></p><p><span style="font-family:Arial;font-size:8.966399px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:685.709778px;left:319.577271px"><nobr>= f (x ; θ ) # What is our estimated output y ˆ ? </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:8.966399px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:700.321777px;left:252.282578px"><nobr>Compute the loss L(y ˆ </nobr></span></span></p><p><span style="font-family:Arial;font-size:6.973800px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:697.825500px;left:356.086548px"><nobr>(i) </nobr></span></span></p><p><span style="font-family:Arial;font-size:8.966399px;font-style:italic;color:#000000;"><span style="position:absolute;top:700.321777px;left:366.551880px"><nobr>, y </nobr></span></span></p><p><span style="font-family:Arial;font-size:8.966399px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:700.321777px;left:386.963867px"><nobr>) # How far off is y ˆ </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:8.966399px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:700.513000px;left:497.562500px"><nobr>from the true output y </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:8.966399px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:700.513000px;left:612.111816px"><nobr>? </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:8.966399px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:714.264221px;left:238.085175px"><nobr>2. g ← ∇θ L( f (x ; θ ), y ) # How should we move θ to maximize loss? </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:8.966399px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:728.876282px;left:238.085175px"><nobr>3. θ ← θ − η g # Go the other way instead </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:8.966399px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:744.349060px;left:214.174500px"><nobr>return θ </nobr></span></span></p><div style="position:absolute;top:530.978699px;left:632.137329px"><nobr><img height="244.000000" width="3.000000" src ="bgimg/bg00076.jpg"/></nobr></div><div style="position:absolute;top:772.776062px;left:188.933319px"><nobr><img height="3.000000" width="446.000000" src ="bgimg/bg00077.jpg"/></nobr></div><p><span style="background:#000000;font-family:Nimbus Roman;font-size:8.966399px;font-weight:bold;color:#FFFFFF;"><span style="position:absolute;top:776.884277px;left:190.929337px"><nobr>Figure 5.6 The stochastic gradient descent algorithm. Step 1 (computing the loss) is used </nobr></span><span style="position:absolute;top:791.639648px;left:189.599991px"><nobr>mainly to report how well we are doing on the current tuple; we don’t need to compute the </nobr></span><span style="position:absolute;top:806.251709px;left:189.599991px"><nobr>loss in order to compute the gradient. The algorithm can terminate when it converges (or </nobr></span><span style="position:absolute;top:820.672363px;left:189.599991px"><nobr>when the gradient norm &lt; ), or when progress halts (for example when the loss starts going </nobr></span><span style="position:absolute;top:835.474365px;left:189.600021px"><nobr>up on a held-out set). </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.222899px;font-weight:bold;color:#0000FF;"><span style="position:absolute;top:869.204285px;left:103.246666px"><nobr>hyperparameter </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:866.674377px;left:209.525345px"><nobr>The learning rate η is a hyperparameter that must be adjusted. If it’s too high, </nobr></span><span style="position:absolute;top:882.773865px;left:189.599976px"><nobr>the learner will take steps that are too large, overshooting the minimum of the loss </nobr></span><span style="position:absolute;top:898.713867px;left:189.599976px"><nobr>function. If it’s too low, the learner will take steps that are too small, and take too </nobr></span><span style="position:absolute;top:914.653870px;left:189.599976px"><nobr>long to get to the minimum. It is common to start with a higher learning rate and then </nobr></span><span style="position:absolute;top:930.593872px;left:189.599976px"><nobr>slowly decrease it, so that it is a function of the iteration k of training; the notation </nobr></span></span></p><p><span style="font-family:Standard Symbols PS;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:948.846558px;left:189.599976px"><nobr>η </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:951.162537px;left:197.610641px"><nobr>k </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:946.535156px;left:206.126633px"><nobr>can be used to mean the value of the learning rate at iteration k . </nobr></span></span></p><div style="position:absolute;top:823.817688px;left:319.465363px"><nobr><img height="6.000000" width="5.000000" src ="bgimg/bg00078.jpg"/></nobr></div>','<p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:113.707115px;left:459.770630px"><nobr>5.6 • </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:113.707115px;left:509.583618px"><nobr>G RADIENT D ESCENT </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-weight:bold;color:#000000;"><span style="position:absolute;top:113.547691px;left:644.317261px"><nobr>17 </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:149.817795px;left:233.525314px"><nobr>We’ll discuss hyperparameters in more detail in Chapter 7, but in short, they are </nobr></span><span style="position:absolute;top:165.757812px;left:213.599976px"><nobr>a special kind of parameter for any machine learning model. Unlike regular param-</nobr></span><span style="position:absolute;top:181.699142px;left:213.599976px"><nobr>eters of a model (weights like w and b), which are learned by the algorithm from </nobr></span><span style="position:absolute;top:197.639160px;left:213.599991px"><nobr>the training set, hyperparameters are special parameters chosen by the algorithm </nobr></span><span style="position:absolute;top:213.579178px;left:213.599991px"><nobr>designer that affect how the algorithm works. </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:11.955200px;font-weight:bold;color:#0000FF;"><span style="position:absolute;top:247.770584px;left:213.599991px"><nobr>5.6.3 Working through an example </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:274.071198px;left:213.599991px"><nobr>Let’s walk through a single step of the gradient descent algorithm. We’ll use a </nobr></span><span style="position:absolute;top:290.012543px;left:213.599991px"><nobr>simpliﬁed version of the example in Fig. <a href="#" onclick="gotoPage(4)">5.2 </a>as it sees a single observation x, whose </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:305.739990px;left:213.599976px"><nobr>correct value is y = 1 (this is a positive review), and with a feature vector x = [x1 </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:305.739990px;left:636.545288px"><nobr>, x </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:310.300873px;left:648.342590px"><nobr>2 </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:305.739990px;left:653.921265px"><nobr>] </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:321.892578px;left:213.599899px"><nobr>consisting of these two features: </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-weight:bold;color:#000000;"><span style="position:absolute;top:349.035797px;left:315.657227px"><nobr>x </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:353.543549px;left:322.299896px"><nobr>1 </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:348.982666px;left:335.847870px"><nobr>= 3 (count of positive lexicon words) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-weight:bold;color:#000000;"><span style="position:absolute;top:368.908051px;left:315.657227px"><nobr>x2 = 2 (count of negative lexicon words) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:396.424561px;left:213.599899px"><nobr>Let’s assume the initial weights and bias in θ are all set to 0, and the initial learning </nobr></span><span style="position:absolute;top:412.364532px;left:213.599854px"><nobr>rate η is 0.1: </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-weight:bold;color:#000000;"><span style="position:absolute;top:439.507751px;left:379.273193px"><nobr>w </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:444.015503px;left:388.863861px"><nobr>1 </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:439.454681px;left:397.394531px"><nobr>= w </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:444.015503px;left:420.258514px"><nobr>2 </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:439.454681px;left:428.789185px"><nobr>= b = 0 </nobr></span></span></p><p><span style="font-family:Standard Symbols PS;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:459.380005px;left:439.565186px"><nobr>η = 0.1 </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:486.896545px;left:213.599823px"><nobr>The single update step requires that we compute the gradient, multiplied by the </nobr></span><span style="position:absolute;top:502.836487px;left:213.599823px"><nobr>learning rate </nobr></span></span></p><p><span style="font-family:Standard Symbols PS;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:529.182739px;left:337.045166px"><nobr>θ = θ − η ∇θ </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:529.926636px;left:429.894531px"><nobr>L( f (x ; θ ), y ) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:557.443115px;left:213.601151px"><nobr>In our mini example there are three parameters, so the gradient vector has 3 dimen-</nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:573.223755px;left:213.601151px"><nobr>sions, for w </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:577.731445px;left:275.953156px"><nobr>1 </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:573.223755px;left:281.531830px"><nobr>, w </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:577.731445px;left:297.763794px"><nobr>2 </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:573.383118px;left:303.343811px"><nobr>, and b. We can compute the ﬁrst gradient as follows: </nobr></span></span></p><p><span style="font-family:Standard Symbols PS;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:624.746460px;left:127.201172px"><nobr>∇w ,b </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:622.222595px;left:150.802505px"><nobr>L = </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:588.136658px;left:174.453171px"><nobr> </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:611.516724px;left:174.453171px"><nobr> </nobr></span></span></p><p><span style="font-family:Standard Symbols PS;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:598.507568px;left:188.887833px"><nobr>∂ LCE (y ˆ ,y) </nobr></span></span></p><div style="position:absolute;top:607.385315px;left:188.221329px"><nobr><img height="3.000000" width="43.000000" src ="bgimg/bg00079.jpg"/></nobr></div><p><span style="font-family:Standard Symbols PS;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:609.916138px;left:200.711990px"><nobr>∂ w1 </nobr></span><span style="position:absolute;top:619.199463px;left:188.887985px"><nobr>∂ LCE (y ˆ ,y) </nobr></span></span></p><div style="position:absolute;top:628.078674px;left:188.221329px"><nobr><img height="3.000000" width="43.000000" src ="bgimg/bg00080.jpg"/></nobr></div><p><span style="font-family:Standard Symbols PS;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:630.608154px;left:200.711990px"><nobr>∂ w2 </nobr></span><span style="position:absolute;top:639.891541px;left:188.887985px"><nobr>∂ LCE (y ˆ ,y) </nobr></span></span></p><div style="position:absolute;top:648.770691px;left:188.221329px"><nobr><img height="3.000000" width="43.000000" src ="bgimg/bg00081.jpg"/></nobr></div><p><span style="font-family:Standard Symbols PS;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:651.507935px;left:204.127991px"><nobr>∂ b </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:588.136719px;left:235.935974px"><nobr> </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:620.018066px;left:235.935974px"><nobr> = </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:592.122070px;left:261.015991px"><nobr> </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:616.032776px;left:261.015991px"><nobr> </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:605.405457px;left:273.857300px"><nobr>(σ (w · x + b) − y)x1 </nobr></span><span style="position:absolute;top:621.345459px;left:273.857300px"><nobr>(σ (w · x + b) − y)x2 </nobr></span><span style="position:absolute;top:637.286804px;left:273.857269px"><nobr>σ (w · x + b) − y </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:592.122070px;left:384.286560px"><nobr>  </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:616.032776px;left:384.286560px"><nobr> =  </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:605.405518px;left:422.206604px"><nobr>(σ (0) − 1)x1 </nobr></span><span style="position:absolute;top:621.345459px;left:422.206482px"><nobr>(σ (0) − 1)x2 </nobr></span><span style="position:absolute;top:637.286865px;left:422.206421px"><nobr>σ (0) − 1 </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:592.122131px;left:495.766388px"><nobr>  </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:616.032776px;left:495.766388px"><nobr> =  </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:610.710205px;left:567.614380px"><nobr>1 </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:626.650269px;left:567.614319px"><nobr>2 </nobr></span></span></p><p><span style="font-family:Arial Unicode MS;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:605.405518px;left:533.687744px"><nobr>−0.5x </nobr></span><span style="position:absolute;top:621.345520px;left:533.687683px"><nobr>−0.5x </nobr></span><span style="position:absolute;top:637.286865px;left:533.687622px"><nobr>−0.5 </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:592.122192px;left:577.178284px"><nobr>  </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:616.032837px;left:577.178284px"><nobr> =  </nobr></span></span></p><p><span style="font-family:Arial Unicode MS;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:605.405518px;left:615.099670px"><nobr>−1.5 </nobr></span></span></p><p><span style="font-family:Arial Unicode MS;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:621.345520px;left:615.099670px"><nobr>−1.0 </nobr></span></span></p><p><span style="font-family:Arial Unicode MS;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:637.286865px;left:615.099670px"><nobr>−0.5 </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:592.122192px;left:646.368958px"><nobr> </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:616.032837px;left:646.368958px"><nobr> </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:673.368591px;left:213.600952px"><nobr>Now that we have a gradient, we compute the new parameter vector θ </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:670.887573px;left:590.835632px"><nobr>1 </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:673.368591px;left:600.168945px"><nobr>by moving </nobr></span></span></p><p><span style="font-family:Standard Symbols PS;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:691.619934px;left:213.600952px"><nobr>θ </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:686.827576px;left:222.048950px"><nobr>0 </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:689.308594px;left:230.948944px"><nobr>in the opposite direction from the gradient: </nobr></span></span></p><p><span style="font-family:Standard Symbols PS;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:731.894714px;left:323.848969px"><nobr>θ = </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:701.794128px;left:354.100952px"><nobr> </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:725.704834px;left:354.100952px"><nobr> </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-weight:bold;color:#000000;"><span style="position:absolute;top:715.874512px;left:366.940979px"><nobr>w </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:720.382263px;left:376.531616px"><nobr>1 </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-weight:bold;color:#000000;"><span style="position:absolute;top:731.814575px;left:366.940979px"><nobr>w </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:736.322266px;left:376.531616px"><nobr>2 </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:748.033508px;left:366.940979px"><nobr>b </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:701.794128px;left:386.095612px"><nobr>  </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:725.704834px;left:386.095612px"><nobr> − η  </nobr></span></span></p><p><span style="font-family:Arial Unicode MS;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:715.077454px;left:432.418304px"><nobr>−1.5 </nobr></span></span></p><p><span style="font-family:Arial Unicode MS;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:731.017456px;left:432.418304px"><nobr>−1.0 </nobr></span></span></p><p><span style="font-family:Arial Unicode MS;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:746.957520px;left:432.418304px"><nobr>−0.5 </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:701.794128px;left:463.687622px"><nobr>  </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:725.704834px;left:463.687622px"><nobr> =  </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:715.821411px;left:501.608948px"><nobr>.15 </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:731.761414px;left:501.608948px"><nobr>.1 </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:747.701416px;left:501.608948px"><nobr>.05 </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:701.794128px;left:522.556946px"><nobr> </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:725.704834px;left:522.556946px"><nobr> </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:774.248047px;left:213.600967px"><nobr>So after one step of gradient descent, the weights have shifted to be: w1 = .15, </nobr></span><span style="position:absolute;top:790.188049px;left:213.600952px"><nobr>w2 = .1, and b = .05. </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:806.340576px;left:233.526260px"><nobr>Note that this observation x happened to be a positive example. We would expect </nobr></span><span style="position:absolute;top:822.280579px;left:213.600906px"><nobr>that after seeing more negative examples with high counts of negative words, that </nobr></span><span style="position:absolute;top:838.062500px;left:213.600906px"><nobr>the weight w2 would shift to have a negative value. </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:11.955200px;font-weight:bold;color:#0000FF;"><span style="position:absolute;top:872.413330px;left:213.600906px"><nobr>5.6.4 Mini-batch training </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:898.713989px;left:213.600906px"><nobr>Stochastic gradient descent is called stochastic because it chooses a single random </nobr></span><span style="position:absolute;top:914.653992px;left:213.600906px"><nobr>example at a time, moving the weights so as to improve performance on that single </nobr></span><span style="position:absolute;top:930.593994px;left:213.600906px"><nobr>example. That can result in very choppy movements, so it’s common to compute the </nobr></span><span style="position:absolute;top:946.535278px;left:213.600906px"><nobr>gradient over batches of training instances rather than a single instance. </nobr></span></span></p>','<p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-weight:bold;color:#000000;"><span style="position:absolute;top:112.375732px;left:103.201332px"><nobr>18 C HAPTER 5 </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#808080;"><span style="position:absolute;top:112.535156px;left:211.115936px"><nobr>• </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:112.535156px;left:229.380722px"><nobr>L OGISTIC R EGRESSION </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.222899px;font-weight:bold;color:#0000FF;"><span style="position:absolute;top:151.136230px;left:111.809326px"><nobr>batch training </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:149.658371px;left:209.525345px"><nobr>For example in batch training we compute the gradient over the entire dataset. </nobr></span><span style="position:absolute;top:165.757812px;left:189.599991px"><nobr>By seeing so many examples, batch training offers a superb estimate of which di-</nobr></span><span style="position:absolute;top:181.699142px;left:189.599991px"><nobr>rection to move the weights, at the cost of spending a lot of time processing every </nobr></span><span style="position:absolute;top:197.639160px;left:189.599991px"><nobr>single example in the training set to compute this perfect direction. </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.222899px;font-weight:bold;color:#0000FF;"><span style="position:absolute;top:215.812256px;left:125.454674px"><nobr>mini-batch </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:213.419754px;left:209.525330px"><nobr>A compromise is mini-batch training: we train on a group of m examples (per-</nobr></span><span style="position:absolute;top:229.519211px;left:189.599991px"><nobr>haps 512, or 1024) that is less than the whole dataset. (If m is the size of the dataset, </nobr></span><span style="position:absolute;top:245.246658px;left:189.599991px"><nobr>then we are doing batch gradient descent; if m = 1, we are back to doing stochas-</nobr></span><span style="position:absolute;top:261.399261px;left:189.599991px"><nobr>tic gradient descent.) Mini-batch training also has the advantage of computational </nobr></span><span style="position:absolute;top:277.340576px;left:189.599991px"><nobr>ef ﬁciency. The mini-batches can easily be vectorized, choosing the size of the mini-</nobr></span><span style="position:absolute;top:293.280609px;left:189.599991px"><nobr>batch based on the computational resources. This allows us to process all the exam-</nobr></span><span style="position:absolute;top:309.220612px;left:189.599991px"><nobr>ples in one mini-batch in parallel and then accumulate the loss, something that’s not </nobr></span><span style="position:absolute;top:325.160645px;left:189.599991px"><nobr>possible with individual or batch training. </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:341.100677px;left:209.525330px"><nobr>We just need to deﬁne mini-batch versions of the cross-entropy loss function </nobr></span><span style="position:absolute;top:357.041992px;left:189.599991px"><nobr>we deﬁned in Section <a href="#" onclick="gotoPage(10)">5.5 </a>and the gradient in Section <a href="#" onclick="gotoPage(14)">5.6.1</a>. Let’s extend the cross-</nobr></span><span style="position:absolute;top:372.981934px;left:189.599991px"><nobr>entropy loss for one example from Eq. <a href="#" onclick="gotoPage(11)">5.22 </a>to mini-batches of size m. We’ll continue </nobr></span><span style="position:absolute;top:388.921967px;left:189.599976px"><nobr>to use the notation that x and y mean the ith training features and training label, </nobr></span><span style="position:absolute;top:404.861938px;left:189.599991px"><nobr>respectively. We make the assumption that the training examples are independent: </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:446.537384px;left:291.264008px"><nobr>log p(training labels) = log </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:432.576019px;left:452.947998px"><nobr>m </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:465.030334px;left:448.854645px"><nobr>i=1 </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:445.793549px;left:467.455994px"><nobr>p(y |x ) </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:491.586792px;left:411.266602px"><nobr>= </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:477.625427px;left:435.602631px"><nobr>m </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:510.079712px;left:431.509277px"><nobr>i=1 </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:490.842926px;left:450.221283px"><nobr>log p(y |x ) </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:535.892334px;left:411.266602px"><nobr>= − </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:522.674805px;left:447.399902px"><nobr>m </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:555.129150px;left:443.306549px"><nobr>i=1 </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:536.968323px;left:462.018555px"><nobr>LCE </nobr></span></span></p><p><span style="font-family:Arial;font-size:7.372300px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:533.547729px;left:493.681274px"><nobr>(i) (i) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:8.966399px;font-style:normal;font-weight:normal;color:#0000FF;"><span style="position:absolute;top:537.749268px;left:604.717346px"><nobr>(5.31) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:580.250122px;left:189.600021px"><nobr>Now the cost function for the mini-batch of m examples is the average loss for each </nobr></span><span style="position:absolute;top:596.190063px;left:189.601318px"><nobr>example: </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:637.865479px;left:170.741318px"><nobr>Cost (y ˆ , y) = </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:629.084717px;left:252.554626px"><nobr>1 </nobr></span></span></p><div style="position:absolute;top:642.565308px;left:250.413330px"><nobr><img height="3.000000" width="11.000000" src ="bgimg/bg00082.jpg"/></nobr></div><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:647.309387px;left:251.080002px"><nobr>m </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:623.902588px;left:269.785339px"><nobr>m </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:656.358215px;left:265.690674px"><nobr>i=1 </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:638.197449px;left:284.404022px"><nobr>LCE </nobr></span></span></p><p><span style="font-family:Arial;font-size:7.372300px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:634.775635px;left:316.066681px"><nobr>(i) (i) </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:682.170776px;left:231.194641px"><nobr>= − </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:674.133911px;left:262.874634px"><nobr>1 </nobr></span></span></p><div style="position:absolute;top:687.614685px;left:260.734680px"><nobr><img height="3.000000" width="11.000000" src ="bgimg/bg00083.jpg"/></nobr></div><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:692.358704px;left:261.401337px"><nobr>m </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:668.951904px;left:280.106689px"><nobr>m </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:701.407532px;left:276.012024px"><nobr>i=1 </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:682.170715px;left:294.723999px"><nobr>y log σ (w · x + b) + (1 − y ) log 1 − σ (w · x + b) (5.32) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:726.527161px;left:189.601242px"><nobr>The mini-batch gradient is the average of the individual gradients from Eq. <a href="#" onclick="gotoPage(15)">5.29</a>: </nobr></span></span></p><div style="position:absolute;top:772.902649px;left:281.504028px"><nobr><img height="3.000000" width="101.000000" src ="bgimg/bg00084.jpg"/></nobr></div><p><span style="font-family:Standard Symbols PS;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:759.209290px;left:282.170563px"><nobr>∂ Cost (y ˆ , y) </nobr></span><span style="position:absolute;top:777.646667px;left:301.092010px"><nobr>∂ w j </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:768.202637px;left:351.764008px"><nobr>= </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:759.421753px;left:373.122681px"><nobr>1 </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:777.646667px;left:371.649323px"><nobr>m </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:754.241211px;left:390.354645px"><nobr>m </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:786.695496px;left:386.260010px"><nobr>i=1 </nobr></span></span></p><p><span style="font-family:Standard Symbols PS;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:767.458740px;left:411.245331px"><nobr>σ (w · x + b) − y x </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:8.966399px;font-style:normal;font-weight:normal;color:#0000FF;"><span style="position:absolute;top:769.315674px;left:604.717224px"><nobr>(5.33) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:811.815125px;left:209.525269px"><nobr>Instead of using the sum notation, we can more efﬁciently compute the gradient </nobr></span><span style="position:absolute;top:827.755127px;left:189.599930px"><nobr>in its matrix form, following the vectorization we saw on page <a href="#" onclick="gotoPage(6)">7</a>, where we have a </nobr></span><span style="position:absolute;top:842.738770px;left:189.599930px"><nobr>matrix X of size [m × f ] representing the m inputs in the batch, and a vector y of size </nobr></span><span style="position:absolute;top:858.679993px;left:189.599930px"><nobr>[m × 1] representing the correct outputs: </nobr></span></span></p><div style="position:absolute;top:909.942932px;left:306.421326px"><nobr><img height="9.000000" width="146.000000" src ="bgimg/bg00085.jpg"/></nobr></div><p><span style="font-family:Standard Symbols PS;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:902.134583px;left:307.087952px"><nobr>∂ Cost (y ˆ , y) 1 </nobr></span></span></p><p><span style="font-family:Standard Symbols PS;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:920.293091px;left:328.201355px"><nobr>∂ w m </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:910.384094px;left:376.680023px"><nobr>= (y ˆ − y) X </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:943.705322px;left:376.679932px"><nobr>= </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:934.924500px;left:398.039917px"><nobr>1 </nobr></span></span></p><div style="position:absolute;top:942.520264px;left:395.898651px"><nobr><img height="9.000000" width="110.000000" src ="bgimg/bg00086.jpg"/></nobr></div><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:953.149292px;left:396.565308px"><nobr>m </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:942.961365px;left:409.225311px"><nobr>(σ (Xw + b) − y) X </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:8.966399px;font-style:normal;font-weight:normal;color:#0000FF;"><span style="position:absolute;top:944.818359px;left:604.717224px"><nobr>(5.34) </nobr></span></span></p><div style="position:absolute;top:443.137451px;left:448.751953px"><nobr><img height="19.000000" width="16.000000" src ="bgimg/bg00087.jpg"/></nobr></div><div style="position:absolute;top:488.186859px;left:430.299927px"><nobr><img height="19.000000" width="18.000000" src ="bgimg/bg00088.jpg"/></nobr></div><div style="position:absolute;top:533.236206px;left:442.097229px"><nobr><img height="19.000000" width="18.000000" src ="bgimg/bg00089.jpg"/></nobr></div><div style="position:absolute;top:634.464111px;left:264.482666px"><nobr><img height="19.000000" width="18.000000" src ="bgimg/bg00090.jpg"/></nobr></div><div style="position:absolute;top:679.513367px;left:274.804016px"><nobr><img height="19.000000" width="18.000000" src ="bgimg/bg00091.jpg"/></nobr></div><div style="position:absolute;top:676.856201px;left:490.816681px"><nobr><img height="24.000000" width="6.000000" src ="bgimg/bg00092.jpg"/></nobr></div><div style="position:absolute;top:676.856201px;left:592.520630px"><nobr><img height="24.000000" width="6.000000" src ="bgimg/bg00093.jpg"/></nobr></div><div style="position:absolute;top:764.801331px;left:385.051971px"><nobr><img height="19.000000" width="18.000000" src ="bgimg/bg00094.jpg"/></nobr></div><div style="position:absolute;top:762.144104px;left:407.975037px"><nobr><img height="24.000000" width="4.000000" src ="bgimg/bg00095.jpg"/></nobr></div><div style="position:absolute;top:762.144104px;left:517.449280px"><nobr><img height="24.000000" width="4.000000" src ="bgimg/bg00096.jpg"/></nobr></div>','<p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:113.696449px;left:475.857300px"><nobr>5.7 • </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:113.696449px;left:525.670288px"><nobr>R EGULARIZATION </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-weight:bold;color:#000000;"><span style="position:absolute;top:113.537025px;left:644.317261px"><nobr>19 </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:17.215401px;font-style:normal;font-weight:normal;color:#0000FF;"><span style="position:absolute;top:145.631912px;left:127.201332px"><nobr>5.7 Regularization </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:213.472000px;left:394.722656px"><nobr>Numquam ponenda est pluralitas sine necessitate </nobr></span><span style="position:absolute;top:229.292480px;left:386.605316px"><nobr>‘Plurality should never be proposed unless needed’ </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:8.966399px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:246.134354px;left:569.097351px"><nobr>William of Occam </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.222899px;font-weight:bold;color:#0000FF;"><span style="position:absolute;top:377.659027px;left:151.796066px"><nobr>overﬁtting </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.222899px;font-weight:bold;color:#0000FF;"><span style="position:absolute;top:393.598999px;left:152.681396px"><nobr>generalize </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:296.640472px;left:213.600021px"><nobr>There is a problem with learning weights that make the model perfectly match the </nobr></span><span style="position:absolute;top:312.580475px;left:213.600021px"><nobr>training data. If a feature is perfectly predictive of the outcome because it happens </nobr></span><span style="position:absolute;top:328.520508px;left:213.600021px"><nobr>to only occur in one class, it will be assigned a very high weight. The weights for </nobr></span><span style="position:absolute;top:344.460541px;left:213.600021px"><nobr>features will attempt to perfectly ﬁt details of the training set, in fact too perfectly, </nobr></span><span style="position:absolute;top:360.400543px;left:213.600021px"><nobr>modeling noisy factors that just accidentally correlate with the class. This problem is </nobr></span><span style="position:absolute;top:376.182465px;left:213.600021px"><nobr>called overﬁtting. A good model should be able to generalize well from the training </nobr></span><span style="position:absolute;top:392.281860px;left:213.600052px"><nobr>data to the unseen test set, but a model that overﬁts will have poor generalization. </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.222899px;font-weight:bold;color:#0000FF;"><span style="position:absolute;top:409.538971px;left:136.261429px"><nobr>regularization </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:408.009338px;left:233.525391px"><nobr>To avoid overﬁtting, a new regularization term R(θ ) is added to the objective </nobr></span><span style="position:absolute;top:424.161865px;left:213.600098px"><nobr>function in Eq. <a href="#" onclick="gotoPage(12)">5.24</a>, resulting in the following objective for a batch of m exam-</nobr></span><span style="position:absolute;top:440.101837px;left:213.600098px"><nobr>ples (slightly rewritten from Eq. <a href="#" onclick="gotoPage(12)">5.24 </a>to be maximizing log probability rather than </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:456.041840px;left:213.600098px"><nobr>minimizing loss, and removing the </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:453.150146px;left:404.272095px"><nobr>1 </nobr></span></span></p><div style="position:absolute;top:460.530640px;left:402.514648px"><nobr><img height="3.000000" width="9.000000" src ="bgimg/bg00097.jpg"/></nobr></div><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:463.054565px;left:403.181305px"><nobr>m </nobr></span></span></p><p><span style="font-family:Standard Symbols PS;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:493.881073px;left:324.973297px"><nobr>θ </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:491.357269px;left:341.391968px"><nobr>= argmax </nobr></span></span></p><p><span style="font-family:Standard Symbols PS;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:507.479126px;left:376.355957px"><nobr>θ </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:477.395874px;left:406.802612px"><nobr>m </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:509.850159px;left:402.707947px"><nobr>i=1 </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:456.041779px;left:415.193298px"><nobr>term which doesn’t affect the argmax): </nobr></span><span style="position:absolute;top:490.613403px;left:421.419983px"><nobr>log P(y |x ) − α R(θ ) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:8.966399px;font-style:normal;font-weight:normal;color:#0000FF;"><span style="position:absolute;top:492.470367px;left:628.717346px"><nobr>(5.35) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.222899px;font-weight:bold;color:#0000FF;"><span style="position:absolute;top:588.460205px;left:183.162674px"><nobr>L2 </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.222899px;font-weight:bold;color:#0000FF;"><span style="position:absolute;top:597.094971px;left:136.261353px"><nobr>regularization </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:527.485291px;left:213.600021px"><nobr>The new regularization term R(θ ) is used to penalize large weights. Thus a setting </nobr></span><span style="position:absolute;top:543.639160px;left:213.601318px"><nobr>of the weights that matches the training data perfectly— but uses many weights with </nobr></span><span style="position:absolute;top:559.579163px;left:213.601318px"><nobr>high values to do so—will be penalized more than a setting that matches the data a </nobr></span><span style="position:absolute;top:575.519104px;left:213.601318px"><nobr>little less well, but does so using smaller weights. There are two common ways to </nobr></span><span style="position:absolute;top:591.246582px;left:213.601318px"><nobr>compute this regularization term R(θ ). L2 regularization is a quadratic function of </nobr></span><span style="position:absolute;top:607.399170px;left:213.601364px"><nobr>the weight values, named because it uses the (square of the) L2 norm of the weight </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:622.384033px;left:213.601364px"><nobr>values. The L2 norm, ||θ || </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:627.687378px;left:356.644043px"><nobr>2 </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:623.181030px;left:362.222717px"><nobr>, is the same as the Euclidean distance of the vector θ </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:639.280457px;left:213.601364px"><nobr>from the origin. If θ consists of n weights, then: </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:672.036011px;left:369.204041px"><nobr>R(θ ) = ||θ ||2 = </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:658.818542px;left:474.442810px"><nobr>n </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:691.272766px;left:469.872192px"><nobr>j =1 </nobr></span></span></p><p><span style="font-family:Standard Symbols PS;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:675.303833px;left:487.970825px"><nobr>θ </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:669.848755px;left:496.418823px"><nobr>2 </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:678.703857px;left:496.365448px"><nobr>j </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:8.966399px;font-style:normal;font-weight:normal;color:#0000FF;"><span style="position:absolute;top:673.893005px;left:628.717468px"><nobr>(5.36) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:710.164490px;left:213.600143px"><nobr>The L2 regularized objective function becomes: </nobr></span></span></p><p><span style="font-family:Standard Symbols PS;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:744.706604px;left:312.577484px"><nobr>θ = argmax </nobr></span></span></p><p><span style="font-family:Standard Symbols PS;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:760.828430px;left:363.960114px"><nobr>θ </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:730.745300px;left:402.154785px"><nobr>m </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:763.199463px;left:398.061462px"><nobr>i=1 </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:743.962708px;left:416.773529px"><nobr>log P(y |x ) − α </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:730.745239px;left:531.069458px"><nobr>n </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:763.199463px;left:526.498779px"><nobr>j =1 </nobr></span></span></p><p><span style="font-family:Standard Symbols PS;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:747.230469px;left:544.596130px"><nobr>θ </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:741.775391px;left:553.044189px"><nobr>2 </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:750.630554px;left:552.990784px"><nobr>j </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:8.966399px;font-style:normal;font-weight:normal;color:#0000FF;"><span style="position:absolute;top:745.819641px;left:628.717529px"><nobr>(5.37) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-weight:bold;color:#000000;"><span style="position:absolute;top:782.025085px;left:213.600174px"><nobr>L1 regularization is a linear function of the weight values, named after the L1 norm </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.222899px;font-weight:bold;color:#0000FF;"><span style="position:absolute;top:779.184265px;left:183.162842px"><nobr>L1 </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.222899px;font-weight:bold;color:#0000FF;"><span style="position:absolute;top:787.818909px;left:136.261520px"><nobr>regularization </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:797.168091px;left:213.600174px"><nobr>|| W || </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:802.472778px;left:240.353485px"><nobr>1 </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:797.965027px;left:245.932190px"><nobr>, the sum of the absolute values of the weights, or Manhattan distance (the </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:814.064453px;left:213.600174px"><nobr>Manhattan distance is the distance you’d have to walk between two points in a city </nobr></span><span style="position:absolute;top:830.004456px;left:213.600174px"><nobr>with a street grid like New York): </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:862.761414px;left:367.380157px"><nobr>R(θ ) = ||θ ||1 = </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:849.542542px;left:472.618927px"><nobr>n </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:881.998169px;left:467.433594px"><nobr>i=1 </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:862.761414px;left:486.145630px"><nobr>|θi | </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:8.966399px;font-style:normal;font-weight:normal;color:#0000FF;"><span style="position:absolute;top:864.618347px;left:628.717590px"><nobr>(5.38) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:898.941833px;left:213.601639px"><nobr>The L1 regularized objective function becomes: </nobr></span></span></p><p><span style="font-family:Standard Symbols PS;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:936.009094px;left:309.893646px"><nobr>θ </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:933.485229px;left:326.312286px"><nobr>= argmax </nobr></span></span></p><p><span style="font-family:Standard Symbols PS;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:949.607117px;left:361.276276px"><nobr>θ </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:919.523865px;left:399.470947px"><nobr>m </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:951.978088px;left:395.377594px"><nobr>1=i </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:932.741333px;left:414.089600px"><nobr>log P(y |x ) − α </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:919.523865px;left:528.384277px"><nobr>n </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:951.978088px;left:523.813660px"><nobr>j =1 </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:932.741333px;left:541.912354px"><nobr>|θ j | </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:8.966399px;font-style:normal;font-weight:normal;color:#0000FF;"><span style="position:absolute;top:934.598389px;left:628.717590px"><nobr>(5.39) </nobr></span></span></p><div style="position:absolute;top:487.957306px;left:401.499908px"><nobr><img height="19.000000" width="18.000000" src ="bgimg/bg00098.jpg"/></nobr></div><div style="position:absolute;top:669.380005px;left:468.049438px"><nobr><img height="19.000000" width="18.000000" src ="bgimg/bg00099.jpg"/></nobr></div><div style="position:absolute;top:730.678833px;left:392.013092px"><nobr><img height="40.000000" width="4.000000" src ="bgimg/bg00100.jpg"/></nobr></div><div style="position:absolute;top:741.306641px;left:396.853424px"><nobr><img height="19.000000" width="18.000000" src ="bgimg/bg00101.jpg"/></nobr></div><div style="position:absolute;top:730.678833px;left:491.359436px"><nobr><img height="40.000000" width="4.000000" src ="bgimg/bg00102.jpg"/></nobr></div><div style="position:absolute;top:741.306641px;left:524.676086px"><nobr><img height="19.000000" width="18.000000" src ="bgimg/bg00103.jpg"/></nobr></div><div style="position:absolute;top:860.104004px;left:466.225525px"><nobr><img height="19.000000" width="18.000000" src ="bgimg/bg00104.jpg"/></nobr></div><div style="position:absolute;top:919.457458px;left:389.329254px"><nobr><img height="40.000000" width="4.000000" src ="bgimg/bg00105.jpg"/></nobr></div><div style="position:absolute;top:930.083984px;left:394.168243px"><nobr><img height="19.000000" width="18.000000" src ="bgimg/bg00106.jpg"/></nobr></div><div style="position:absolute;top:919.457458px;left:488.674255px"><nobr><img height="40.000000" width="4.000000" src ="bgimg/bg00107.jpg"/></nobr></div><div style="position:absolute;top:930.083984px;left:521.990967px"><nobr><img height="19.000000" width="18.000000" src ="bgimg/bg00108.jpg"/></nobr></div>','<p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-weight:bold;color:#000000;"><span style="position:absolute;top:112.375732px;left:103.201332px"><nobr>20 C HAPTER 5 </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#808080;"><span style="position:absolute;top:112.535156px;left:211.115936px"><nobr>• </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:112.535156px;left:229.380722px"><nobr>L OGISTIC R EGRESSION </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.222899px;font-weight:bold;color:#0000FF;"><span style="position:absolute;top:167.990891px;left:150.599976px"><nobr>lasso </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.222899px;font-weight:bold;color:#0000FF;"><span style="position:absolute;top:183.016190px;left:149.001312px"><nobr>ridge </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:149.817795px;left:189.600021px"><nobr>These kinds of regularization come from statistics, where L1 regularization is called </nobr></span><span style="position:absolute;top:165.598389px;left:189.600021px"><nobr>lasso regression (<a href="#" onclick="gotoPage(24)">Tibshirani</a>, <a href="#" onclick="gotoPage(24)">1996</a>) and L2 regularization is called ridge regression, </nobr></span><span style="position:absolute;top:181.699142px;left:189.599976px"><nobr>and both are commonly used in language processing. L2 regularization is easier to </nobr></span><span style="position:absolute;top:197.639160px;left:189.599976px"><nobr>optimize because of its simple derivative (the derivative of θ is just 2θ ), while </nobr></span><span style="position:absolute;top:212.622803px;left:189.599976px"><nobr>L1 regularization is more complex (the derivative of |θ | is non-continuous at zero). </nobr></span><span style="position:absolute;top:229.519211px;left:189.599960px"><nobr>But while L2 prefers weight vectors with many small weights, L1 prefers sparse </nobr></span><span style="position:absolute;top:245.459229px;left:189.599960px"><nobr>solutions with some larger weights but many more weights set to zero. Thus L1 </nobr></span><span style="position:absolute;top:261.399261px;left:189.599960px"><nobr>regularization leads to much sparser weight vectors, that is, far fewer features. </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:277.340576px;left:209.525284px"><nobr>Both L1 and L2 regularization have Bayesian interpretations as constraints on </nobr></span><span style="position:absolute;top:293.280609px;left:189.599960px"><nobr>the prior of how weights should look. L1 regularization can be viewed as a Laplace </nobr></span><span style="position:absolute;top:309.220612px;left:189.599960px"><nobr>prior on the weights. L2 regularization corresponds to assuming that weights are </nobr></span><span style="position:absolute;top:324.948090px;left:189.599960px"><nobr>distributed according to a Gaussian distribution with mean µ = 0. In a Gaussian </nobr></span><span style="position:absolute;top:341.100677px;left:189.600021px"><nobr>or normal distribution, the further away a value is from the mean, the lower its </nobr></span><span style="position:absolute;top:357.040680px;left:189.600021px"><nobr>probability (scaled by the variance σ ). By using a Gaussian prior on the weights, we </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:372.982025px;left:189.600037px"><nobr>are saying that weights prefer to have the value 0. A Gaussian for a weight θ </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:377.404083px;left:597.878662px"><nobr>j </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:372.982025px;left:604.842651px"><nobr>is </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:403.834045px;left:344.553375px"><nobr>1 </nobr></span></span></p><div style="position:absolute;top:417.314606px;left:325.549347px"><nobr><img height="3.000000" width="146.000000" src ="bgimg/bg00109.jpg"/></nobr></div><div style="position:absolute;top:419.970642px;left:338.833374px"><nobr><img height="3.000000" width="32.000000" src ="bgimg/bg00110.jpg"/></nobr></div><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:427.027130px;left:339.500031px"><nobr>2π σ </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:424.799438px;left:363.954681px"><nobr>2 </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:433.393219px;left:363.994690px"><nobr>j </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:411.870728px;left:372.604004px"><nobr>exp − </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:403.386627px;left:415.493317px"><nobr>(θ </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:408.022583px;left:429.041290px"><nobr>j </nobr></span></span></p><p><span style="font-family:Arial Unicode MS;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:402.642731px;left:434.529297px"><nobr>− µ </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:408.022583px;left:456.153259px"><nobr>j </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:403.386627px;left:459.795959px"><nobr>) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:422.511139px;left:432.178650px"><nobr>2σ </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:8.966399px;font-style:normal;font-weight:normal;color:#0000FF;"><span style="position:absolute;top:413.727661px;left:604.717285px"><nobr>(5.40) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:457.419098px;left:189.599976px"><nobr>If we multiply each weight by a Gaussian prior on the weight, we are thus maximiz-</nobr></span><span style="position:absolute;top:473.359100px;left:189.599976px"><nobr>ing the following constraint: </nobr></span></span></p><p><span style="font-family:Standard Symbols PS;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:514.345093px;left:241.235977px"><nobr>θ </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:511.821228px;left:257.654663px"><nobr>= argmax </nobr></span></span></p><p><span style="font-family:Standard Symbols PS;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:527.943115px;left:292.618652px"><nobr>θ </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:497.858551px;left:321.957306px"><nobr>m </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:530.314209px;left:317.863983px"><nobr>i=1 </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:511.077362px;left:335.469299px"><nobr>P(y |x ) × </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:497.858551px;left:411.511932px"><nobr>n </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:530.314209px;left:406.941254px"><nobr>j =1 </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:503.040466px;left:443.879944px"><nobr>1 </nobr></span></span></p><div style="position:absolute;top:516.521240px;left:424.875977px"><nobr><img height="3.000000" width="146.000000" src ="bgimg/bg00111.jpg"/></nobr></div><div style="position:absolute;top:519.178589px;left:438.160004px"><nobr><img height="3.000000" width="32.000000" src ="bgimg/bg00112.jpg"/></nobr></div><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:526.233826px;left:438.826660px"><nobr>2π σ </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:524.006104px;left:463.281372px"><nobr>2 </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:532.599854px;left:463.321350px"><nobr>j </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:511.077362px;left:471.929321px"><nobr>exp − </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:500.326111px;left:564.275940px"><nobr>2 </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:501.849396px;left:514.818604px"><nobr>(θ j − µ j ) </nobr></span><span style="position:absolute;top:521.717896px;left:531.504028px"><nobr>2σ </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:8.966399px;font-style:normal;font-weight:normal;color:#0000FF;"><span style="position:absolute;top:512.934326px;left:604.717285px"><nobr>(5.41) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:558.751953px;left:189.599976px"><nobr>which in log space, with µ = 0, and assuming 2σ = 1, corresponds to </nobr></span></span></p><p><span style="font-family:Standard Symbols PS;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:596.502686px;left:296.326599px"><nobr>θ = argmax </nobr></span></span></p><p><span style="font-family:Standard Symbols PS;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:612.624512px;left:347.709259px"><nobr>θ </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:582.541260px;left:378.154633px"><nobr>m </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:614.995544px;left:374.061310px"><nobr>i=1 </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:595.758789px;left:392.773346px"><nobr>log P(y |x ) − α </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:582.541260px;left:499.319946px"><nobr>n </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:614.995605px;left:494.749298px"><nobr>j =1 </nobr></span></span></p><p><span style="font-family:Standard Symbols PS;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:599.026611px;left:512.846619px"><nobr>θ </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:593.571533px;left:521.296021px"><nobr>2 </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:602.426636px;left:521.242615px"><nobr>j </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:8.966399px;font-style:normal;font-weight:normal;color:#0000FF;"><span style="position:absolute;top:597.615784px;left:604.717285px"><nobr>(5.42) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:637.805908px;left:189.599976px"><nobr>which is in the same form as Eq. <a href="#" onclick="gotoPage(18)">5.37</a>. </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:17.215401px;font-style:normal;font-weight:normal;color:#0000FF;"><span style="position:absolute;top:681.024048px;left:103.201317px"><nobr>5.8 Learning in Multinomial Logistic Regression </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:732.804504px;left:189.599976px"><nobr>The loss function for multinomial logistic regression generalizes the loss function </nobr></span><span style="position:absolute;top:748.744568px;left:189.599976px"><nobr>for binary logistic regression from 2 to K classes. Recall that that the cross-entropy </nobr></span><span style="position:absolute;top:764.684570px;left:189.599976px"><nobr>loss for binary logistic regression (repeated from Eq. <a href="#" onclick="gotoPage(11)">5.22</a>) is: </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:791.968079px;left:256.037323px"><nobr>L </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:796.324890px;left:263.423950px"><nobr>CE </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:790.892151px;left:276.650635px"><nobr>(y ˆ , y) = − log p(y|x) = − [y log y ˆ + (1 − y) log(1 − y ˆ )] </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:8.966399px;font-style:normal;font-weight:normal;color:#0000FF;"><span style="position:absolute;top:792.749146px;left:604.717346px"><nobr>(5.43) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:819.012512px;left:189.600052px"><nobr>The loss function for multinomial logistic regression generalizes the two terms in </nobr></span><span style="position:absolute;top:834.740051px;left:189.600052px"><nobr>Eq. <a href="#" onclick="gotoPage(19)">5.43 </a>(one that is non-zero when y = 1 and one that is non-zero when y = 0) to </nobr></span><span style="position:absolute;top:850.734558px;left:189.600021px"><nobr>K terms. As we mentioned above, for multinomial regression we’ll represent both y </nobr></span><span style="position:absolute;top:866.613953px;left:189.600021px"><nobr>and y ˆ as vectors. The true label y is a vector with K elements, each corresponding </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:882.773926px;left:189.600037px"><nobr>to a class, with yc </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:882.561340px;left:287.644043px"><nobr>= 1 if the correct class is c, with all other elements of y being 0. </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:898.495300px;left:189.600021px"><nobr>And our classiﬁer will produce an estimate vector with K elements y ˆ , each element </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-weight:bold;color:#000000;"><span style="position:absolute;top:914.435242px;left:189.599976px"><nobr>y ˆ </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:919.282776px;left:196.242630px"><nobr>k </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:914.441406px;left:204.758621px"><nobr>of which represents the estimated probability p(yk </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:913.697510px;left:475.093262px"><nobr>= 1|x). </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:930.434448px;left:209.525223px"><nobr>The loss function for a single example x, generalizing from binary logistic re-</nobr></span><span style="position:absolute;top:946.535278px;left:189.599884px"><nobr>gression, is the sum of the logs of the K output classes, each weighted by their </nobr></span></span></p><div style="position:absolute;top:420.902832px;left:327.678589px"><nobr><img height="24.000000" width="13.000000" src ="bgimg/bg00113.jpg"/></nobr></div><div style="position:absolute;top:398.585510px;left:396.197357px"><nobr><img height="40.000000" width="7.000000" src ="bgimg/bg00114.jpg"/></nobr></div><div style="position:absolute;top:398.585449px;left:472.573395px"><nobr><img height="40.000000" width="7.000000" src ="bgimg/bg00115.jpg"/></nobr></div><div style="position:absolute;top:508.420044px;left:317.762634px"><nobr><img height="19.000000" width="16.000000" src ="bgimg/bg00116.jpg"/></nobr></div><div style="position:absolute;top:508.420044px;left:406.226562px"><nobr><img height="19.000000" width="16.000000" src ="bgimg/bg00117.jpg"/></nobr></div><div style="position:absolute;top:520.109497px;left:427.005249px"><nobr><img height="24.000000" width="13.000000" src ="bgimg/bg00118.jpg"/></nobr></div><div style="position:absolute;top:497.793457px;left:495.523987px"><nobr><img height="40.000000" width="7.000000" src ="bgimg/bg00119.jpg"/></nobr></div><div style="position:absolute;top:497.792145px;left:571.900085px"><nobr><img height="40.000000" width="7.000000" src ="bgimg/bg00120.jpg"/></nobr></div><div style="position:absolute;top:593.102722px;left:372.853271px"><nobr><img height="19.000000" width="18.000000" src ="bgimg/bg00121.jpg"/></nobr></div><div style="position:absolute;top:593.102783px;left:492.926605px"><nobr><img height="19.000000" width="18.000000" src ="bgimg/bg00122.jpg"/></nobr></div>','<p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:113.643150px;left:443.048004px"><nobr>5.9 • </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:113.643150px;left:492.861053px"><nobr>I NTERPRETING MODELS </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-weight:bold;color:#000000;"><span style="position:absolute;top:113.483727px;left:644.317322px"><nobr>21 </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:149.658371px;left:213.600021px"><nobr>probability yk (Eq. <a href="#" onclick="gotoPage(20)">5.44</a>). This turns out to be just the negative log probability of the </nobr></span><span style="position:absolute;top:165.757812px;left:213.600037px"><nobr>correct class c (Eq. <a href="#" onclick="gotoPage(20)">5.45</a>): </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:206.590652px;left:257.162720px"><nobr>L </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:210.946045px;left:264.549377px"><nobr>CE </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:205.514725px;left:277.774719px"><nobr>(y ˆ , y) = − </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:192.295898px;left:350.625397px"><nobr>K </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:224.942139px;left:345.632080px"><nobr>k =1 </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-weight:bold;color:#000000;"><span style="position:absolute;top:206.311691px;left:365.244080px"><nobr>yk </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:206.251144px;left:378.557404px"><nobr>log y ˆ k </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:8.966399px;font-style:normal;font-weight:normal;color:#0000FF;"><span style="position:absolute;top:207.371658px;left:628.717346px"><nobr>(5.44) </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:241.642746px;left:314.492035px"><nobr>= − log y ˆ </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:247.021240px;left:369.674683px"><nobr>c </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:242.386551px;left:374.702698px"><nobr>, </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:242.599121px;left:394.617340px"><nobr>(where c is the correct class) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:8.966399px;font-style:normal;font-weight:normal;color:#0000FF;"><span style="position:absolute;top:243.499680px;left:628.717346px"><nobr>(5.45) </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:264.224030px;left:314.492035px"><nobr>= − log p ˆ (yc </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:264.224030px;left:390.446655px"><nobr>= 1|x) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:265.180420px;left:440.594635px"><nobr>(where c is the correct class) </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:295.798676px;left:314.491974px"><nobr>= − log </nobr></span></span></p><div style="position:absolute;top:301.244049px;left:363.960022px"><nobr><img height="3.000000" width="119.000000" src ="bgimg/bg00123.jpg"/></nobr></div><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:286.806641px;left:380.708008px"><nobr>exp (wc · x + bc ) </nobr></span><span style="position:absolute;top:306.885498px;left:380.122650px"><nobr>j =1 exp (wj · x + b </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:312.265289px;left:473.071930px"><nobr>j </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:307.629303px;left:476.713257px"><nobr>) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:296.755219px;left:498.221252px"><nobr>(c is the correct class) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:8.966399px;font-style:normal;font-weight:normal;color:#0000FF;"><span style="position:absolute;top:297.655762px;left:628.717224px"><nobr>(5.46) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:336.292572px;left:213.601273px"><nobr>How did we get from Eq. <a href="#" onclick="gotoPage(20)">5.44 </a>to Eq. <a href="#" onclick="gotoPage(20)">5.45</a>? Because only one class (let’s call it c) is </nobr></span><span style="position:absolute;top:352.020020px;left:213.601273px"><nobr>the correct one, the vector y takes the value 1 only for this value of k , i.e., has yc = 1 </nobr></span><span style="position:absolute;top:367.216156px;left:213.601242px"><nobr>and y j = 0 ∀ j = c. That means the terms in the sum in Eq. <a href="#" onclick="gotoPage(20)">5.44 </a>will all be 0 except </nobr></span><span style="position:absolute;top:384.113892px;left:213.601242px"><nobr>for the term corresponding to the true class c. Hence the cross-entropy loss is simply </nobr></span><span style="position:absolute;top:400.053864px;left:213.601212px"><nobr>the log of the output probability corresponding to the correct class, and we therefore </nobr></span><span style="position:absolute;top:415.834412px;left:213.601212px"><nobr>also call Eq. <a href="#" onclick="gotoPage(20)">5.45 </a>the negative log likelihood loss. </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.222899px;font-weight:bold;color:#0000FF;"><span style="position:absolute;top:413.909607px;left:145.641235px"><nobr>negative log </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.222899px;font-weight:bold;color:#0000FF;"><span style="position:absolute;top:422.542969px;left:136.425232px"><nobr>likelihood loss </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:431.933838px;left:233.525223px"><nobr>Of course for gradient descent we don’t need the loss, we need its gradient. The </nobr></span><span style="position:absolute;top:447.873810px;left:213.601242px"><nobr>gradient for a single example turns out to be very similar to the gradient for binary </nobr></span><span style="position:absolute;top:462.857361px;left:213.601242px"><nobr>logistic regression, (y ˆ − y)x, that we saw in Eq. <a href="#" onclick="gotoPage(15)">5.29</a>. Let’s consider one piece of the </nobr></span><span style="position:absolute;top:479.755096px;left:213.601273px"><nobr>gradient, the derivative for a single weight. For each class k , the weight of the ith </nobr></span><span style="position:absolute;top:495.535706px;left:213.601273px"><nobr>element of input x is wk ,i . What is the partial derivative of the loss with respect to </nobr></span><span style="position:absolute;top:511.475708px;left:213.599930px"><nobr>wk ,i ? This derivative turns out to be just the difference between the true value for the </nobr></span><span style="position:absolute;top:527.575073px;left:213.599930px"><nobr>class k (which is either 1 or 0) and the probability the classiﬁer outputs for class k , </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:543.515076px;left:411.887909px"><nobr>corresponding to the ith element of the weight </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:543.355652px;left:213.599899px"><nobr>weighted by the value of the input xi </nobr></span><span style="position:absolute;top:559.455017px;left:213.599915px"><nobr>vector for class k : </nobr></span></span></p><div style="position:absolute;top:596.535950px;left:312.081329px"><nobr><img height="3.000000" width="31.000000" src ="bgimg/bg00124.jpg"/></nobr></div><p><span style="font-family:Standard Symbols PS;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:583.174561px;left:312.747925px"><nobr>∂ LCE </nobr></span><span style="position:absolute;top:601.001099px;left:312.933319px"><nobr>∂ wk ,i </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:591.092041px;left:351.133362px"><nobr>= −(yk − y ˆ k )xi </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:619.585388px;left:351.133240px"><nobr>= −(yk </nobr></span><span style="position:absolute;top:619.585388px;left:398.582581px"><nobr>− p(yk </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:619.585388px;left:438.329193px"><nobr>= 1|x))xi </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:651.465393px;left:351.133148px"><nobr>= − yk </nobr></span></span></p><p><span style="font-family:Arial Unicode MS;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:651.465393px;left:405.419769px"><nobr>− </nobr></span></span></p><div style="position:absolute;top:656.910645px;left:418.513306px"><nobr><img height="3.000000" width="119.000000" src ="bgimg/bg00125.jpg"/></nobr></div><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:642.473389px;left:434.629120px"><nobr>exp (wk · x + bk ) </nobr></span><span style="position:absolute;top:662.552063px;left:434.675995px"><nobr>j =1 exp (wj · x + b </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:667.931885px;left:527.625244px"><nobr>j </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:663.296021px;left:531.267944px"><nobr>) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-weight:bold;color:#000000;"><span style="position:absolute;top:652.262451px;left:550.007935px"><nobr>xi </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:8.966399px;font-style:normal;font-weight:normal;color:#0000FF;"><span style="position:absolute;top:653.322449px;left:628.717224px"><nobr>(5.47) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:691.959229px;left:233.525284px"><nobr>We’ll return to this case of the gradient for softmax regression when we introduce </nobr></span><span style="position:absolute;top:707.899231px;left:213.599930px"><nobr>neural networks in Chapter 7, and at that time we’ll also discuss the derivation of </nobr></span><span style="position:absolute;top:723.679871px;left:213.599930px"><nobr>this gradient in equations Eq. ??–Eq. ??. </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:17.215401px;font-style:normal;font-weight:normal;color:#0000FF;"><span style="position:absolute;top:767.231995px;left:127.201294px"><nobr>5.9 Interpreting models </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:819.012512px;left:213.599960px"><nobr>Often we want to know more than just the correct classiﬁcation of an observation. </nobr></span><span style="position:absolute;top:834.952576px;left:213.599960px"><nobr>We want to know why the classiﬁer made the decision it did. That is, we want our </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.222899px;font-weight:bold;color:#0000FF;"><span style="position:absolute;top:852.216309px;left:140.642609px"><nobr>interpretable </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:850.734558px;left:213.599960px"><nobr>decision to be interpretable. Interpretability can be hard to deﬁne strictly, but the </nobr></span><span style="position:absolute;top:866.833862px;left:213.599930px"><nobr>core idea is that as humans we should know why our algorithms reach the conclu-</nobr></span><span style="position:absolute;top:882.773926px;left:213.599930px"><nobr>sions they do. Because the features to logistic regression are often human-designed, </nobr></span><span style="position:absolute;top:898.713867px;left:213.599930px"><nobr>one way to understand a classiﬁer’s decision is to understand the role each feature </nobr></span><span style="position:absolute;top:914.653870px;left:213.599930px"><nobr>plays in the decision. Logistic regression can be combined with statistical tests (the </nobr></span><span style="position:absolute;top:930.593994px;left:213.599930px"><nobr>likelihood ratio test, or the Wald test); investigating whether a particular feature is </nobr></span><span style="position:absolute;top:946.535278px;left:213.599930px"><nobr>signiﬁcant by one of these tests, or inspecting its magnitude (how large is the weight </nobr></span></span></p><div style="position:absolute;top:202.857346px;left:345.322693px"><nobr><img height="19.000000" width="18.000000" src ="bgimg/bg00126.jpg"/></nobr></div><div style="position:absolute;top:306.885345px;left:365.369293px"><nobr><img height="14.000000" width="13.000000" src ="bgimg/bg00127.jpg"/></nobr></div><div style="position:absolute;top:367.666901px;left:296.217194px"><nobr><img height="13.000000" width="7.000000" src ="bgimg/bg00128.jpg"/></nobr></div><div style="position:absolute;top:638.181396px;left:384.358490px"><nobr><img height="40.000000" width="7.000000" src ="bgimg/bg00129.jpg"/></nobr></div><div style="position:absolute;top:662.552002px;left:419.922607px"><nobr><img height="14.000000" width="13.000000" src ="bgimg/bg00130.jpg"/></nobr></div><div style="position:absolute;top:638.181519px;left:538.466675px"><nobr><img height="40.000000" width="7.000000" src ="bgimg/bg00131.jpg"/></nobr></div>','<p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-weight:bold;color:#000000;"><span style="position:absolute;top:112.375732px;left:103.201332px"><nobr>22 C HAPTER 5 </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#808080;"><span style="position:absolute;top:112.535156px;left:211.115936px"><nobr>• </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:112.535156px;left:229.380722px"><nobr>L OGISTIC R EGRESSION </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:149.817795px;left:189.600021px"><nobr>w associated with the feature?) can help us interpret why the classiﬁer made the </nobr></span><span style="position:absolute;top:165.757812px;left:189.600021px"><nobr>decision it makes. This is enormously important for building transparent models. </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:181.699142px;left:209.525345px"><nobr>Furthermore, in addition to its use as a classiﬁer, logistic regression in NLP and </nobr></span><span style="position:absolute;top:197.639160px;left:189.600021px"><nobr>many other ﬁelds is widely used as an analytic tool for testing hypotheses about the </nobr></span><span style="position:absolute;top:213.579178px;left:189.600021px"><nobr>effect of various explanatory variables (features). In text classiﬁcation, perhaps we </nobr></span><span style="position:absolute;top:229.519211px;left:189.600021px"><nobr>want to know if logically negative words (no, not, never) are more likely to be asso-</nobr></span><span style="position:absolute;top:245.459229px;left:189.599991px"><nobr>ciated with negative sentiment, or if negative reviews of movies are more likely to </nobr></span><span style="position:absolute;top:261.399261px;left:189.599991px"><nobr>discuss the cinematography. However, in doing so it’s necessary to control for po-</nobr></span><span style="position:absolute;top:277.340576px;left:189.599991px"><nobr>tential confounds: other factors that might inﬂuence sentiment (the movie genre, the </nobr></span><span style="position:absolute;top:293.280609px;left:189.599991px"><nobr>year it was made, perhaps the length of the review in words). Or we might be study-</nobr></span><span style="position:absolute;top:309.220612px;left:189.599991px"><nobr>ing the relationship between NLP-extracted linguistic features and non-linguistic </nobr></span><span style="position:absolute;top:325.160645px;left:189.599991px"><nobr>outcomes (hospital readmissions, political outcomes, or product sales), but need to </nobr></span><span style="position:absolute;top:341.100677px;left:189.599991px"><nobr>control for confounds (the age of the patient, the county of voting, the brand of the </nobr></span><span style="position:absolute;top:357.040680px;left:189.599991px"><nobr>product). In such cases, logistic regression allows us to test whether some feature is </nobr></span><span style="position:absolute;top:372.982025px;left:189.599991px"><nobr>associated with some outcome above and beyond the effect of other features. </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:17.215401px;font-style:normal;font-weight:normal;color:#0000FF;"><span style="position:absolute;top:416.000153px;left:103.201332px"><nobr>5.10 Advanced: Deriving the Gradient Equation </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:467.780701px;left:189.599991px"><nobr>In this section we give the derivation of the gradient of the cross-entropy loss func-</nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:483.720642px;left:189.599991px"><nobr>tion L </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:488.196991px;left:222.763977px"><nobr>CE </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:483.720642px;left:241.098663px"><nobr>for logistic regression. Let’s start with some quick calculus refreshers. </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:499.448120px;left:189.599976px"><nobr>First, the derivative of ln(x): </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:523.692139px;left:372.909302px"><nobr>d </nobr></span></span></p><div style="position:absolute;top:537.053284px;left:369.273315px"><nobr><img height="3.000000" width="69.000000" src ="bgimg/bg00132.jpg"/></nobr></div><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:541.797424px;left:369.940002px"><nobr>dx </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:532.351990px;left:386.279999px"><nobr>ln(x) = </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:523.572510px;left:430.678650px"><nobr>1 </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:541.797424px;left:431.030670px"><nobr>x </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:8.966399px;font-style:normal;font-weight:normal;color:#0000FF;"><span style="position:absolute;top:533.465088px;left:604.717346px"><nobr>(5.48) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:562.327209px;left:209.525391px"><nobr>Second, the (very elegant) derivative of the sigmoid: </nobr></span></span></p><div style="position:absolute;top:613.156006px;left:338.326660px"><nobr><img height="3.000000" width="34.000000" src ="bgimg/bg00133.jpg"/></nobr></div><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:599.462646px;left:338.993378px"><nobr>d σ (z) </nobr></span><span style="position:absolute;top:617.900024px;left:348.869354px"><nobr>dz </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:607.712036px;left:375.790680px"><nobr>= σ (z)(1 − σ (z)) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:8.966399px;font-style:normal;font-weight:normal;color:#0000FF;"><span style="position:absolute;top:609.569031px;left:604.717346px"><nobr>(5.49) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.222899px;font-weight:bold;color:#0000FF;"><span style="position:absolute;top:641.686890px;left:128.932022px"><nobr>chain rule </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:639.294373px;left:209.525391px"><nobr>Finally, the chain rule of derivatives. Suppose we are computing the derivative </nobr></span><span style="position:absolute;top:655.181213px;left:189.600021px"><nobr>of a composite function f (x) = u(v(x)). The derivative of f (x) is the derivative of </nobr></span><span style="position:absolute;top:671.121216px;left:189.599976px"><nobr>u(x) with respect to v(x) times the derivative of v(x) with respect to x: </nobr></span></span></p><div style="position:absolute;top:708.726746px;left:369.830658px"><nobr><img height="3.000000" width="84.000000" src ="bgimg/bg00134.jpg"/></nobr></div><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:703.282715px;left:395.017334px"><nobr>= · </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:695.365234px;left:370.497314px"><nobr>d f du dv </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:713.470703px;left:371.339996px"><nobr>dx dv dx </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:8.966399px;font-style:normal;font-weight:normal;color:#0000FF;"><span style="position:absolute;top:705.139648px;left:604.717346px"><nobr>(5.50) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:734.080505px;left:209.525391px"><nobr>First, we want to know the derivative of the loss function with respect to a single </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:750.020508px;left:189.600052px"><nobr>weight w </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:754.443848px;left:239.413406px"><nobr>j </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:750.020508px;left:246.376083px"><nobr>(we’ll need to compute it for each weight, and for the bias): </nobr></span></span></p><p><span style="font-family:Standard Symbols PS;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:787.414734px;left:217.984070px"><nobr>∂ LCE </nobr></span></span></p><div style="position:absolute;top:800.776062px;left:217.317322px"><nobr><img height="3.000000" width="83.000000" src ="bgimg/bg00135.jpg"/></nobr></div><p><span style="font-family:Standard Symbols PS;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:805.241028px;left:220.935989px"><nobr>∂ w j </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:796.075928px;left:256.369324px"><nobr>= </nobr></span></span></p><p><span style="font-family:Standard Symbols PS;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:789.606445px;left:283.608002px"><nobr>∂ </nobr></span></span></p><p><span style="font-family:Standard Symbols PS;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:805.241028px;left:276.254669px"><nobr>∂ w </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:809.823914px;left:295.528015px"><nobr>j </nobr></span></span></p><p><span style="font-family:Arial Unicode MS;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:795.332031px;left:302.609344px"><nobr>− [y log σ (w · x + b) + (1 − y) log (1 − σ (w · x + b))] </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:833.066711px;left:256.369263px"><nobr>= − </nobr></span></span></p><p><span style="font-family:Standard Symbols PS;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:827.341125px;left:302.415924px"><nobr>∂ </nobr></span></span></p><div style="position:absolute;top:838.510620px;left:294.395996px"><nobr><img height="3.000000" width="155.000000" src ="bgimg/bg00136.jpg"/></nobr></div><p><span style="font-family:Standard Symbols PS;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:842.975769px;left:295.062653px"><nobr>∂ w </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:847.557312px;left:314.335999px"><nobr>j </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:833.066711px;left:319.573334px"><nobr>y log σ (w · x + b) + </nobr></span></span></p><p><span style="font-family:Standard Symbols PS;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:827.341125px;left:432.363922px"><nobr>∂ </nobr></span></span></p><p><span style="font-family:Standard Symbols PS;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:842.975769px;left:425.010651px"><nobr>∂ w </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:847.557312px;left:444.284027px"><nobr>j </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:833.066711px;left:449.521362px"><nobr>(1 − y) log [1 − σ (w · x + b)] </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:8.966399px;font-style:normal;font-weight:normal;color:#0000FF;"><span style="position:absolute;top:861.549011px;left:604.717285px"><nobr>(5.51) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:886.705811px;left:189.601318px"><nobr>Next, using the chain rule, and relying on the derivative of log: </nobr></span></span></p><p><span style="font-family:Standard Symbols PS;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:911.034729px;left:193.934647px"><nobr>∂ LCE </nobr></span></span></p><div style="position:absolute;top:924.396057px;left:193.268005px"><nobr><img height="3.000000" width="353.000000" src ="bgimg/bg00137.jpg"/></nobr></div><p><span style="font-family:Standard Symbols PS;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:928.861084px;left:196.888000px"><nobr>∂ w j </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:918.952026px;left:232.320007px"><nobr>= − </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:911.034729px;left:291.581329px"><nobr>y </nobr></span></span></p><p><span style="font-family:Standard Symbols PS;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:928.064026px;left:262.526672px"><nobr>σ (w · x + b) </nobr></span></span></p><p><span style="font-family:Standard Symbols PS;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:913.226440px;left:337.074646px"><nobr>∂ </nobr></span></span></p><p><span style="font-family:Standard Symbols PS;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:928.861084px;left:329.721313px"><nobr>∂ w </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:933.443909px;left:348.996002px"><nobr>j </nobr></span></span></p><p><span style="font-family:Standard Symbols PS;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:918.952026px;left:354.232025px"><nobr>σ (w · x + b) − </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:909.958740px;left:462.897308px"><nobr>1 − y </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:928.064026px;left:433.842651px"><nobr>1 − σ (w · x + b) </nobr></span></span></p><p><span style="font-family:Standard Symbols PS;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:913.226440px;left:529.043945px"><nobr>∂ </nobr></span></span></p><p><span style="font-family:Standard Symbols PS;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:928.861084px;left:521.690674px"><nobr>∂ w </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:933.443909px;left:540.964050px"><nobr>j </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:918.952026px;left:546.201416px"><nobr>1 − σ (w · x + b) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:8.966399px;font-style:normal;font-weight:normal;color:#0000FF;"><span style="position:absolute;top:947.435791px;left:604.717346px"><nobr>(5.52) </nobr></span></span></p><div style="position:absolute;top:823.766785px;left:289.778137px"><nobr><img height="32.000000" width="4.000000" src ="bgimg/bg00138.jpg"/></nobr></div><div style="position:absolute;top:823.766785px;left:598.510132px"><nobr><img height="32.000000" width="4.000000" src ="bgimg/bg00139.jpg"/></nobr></div>','<p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:113.696449px;left:511.282654px"><nobr>5.11 • </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:113.696449px;left:568.401611px"><nobr>S UMMARY </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-weight:bold;color:#000000;"><span style="position:absolute;top:113.537025px;left:644.317261px"><nobr>23 </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:149.817795px;left:213.599976px"><nobr>Rearranging terms: </nobr></span></span></p><p><span style="font-family:Standard Symbols PS;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:178.163986px;left:264.830658px"><nobr>∂ LCE </nobr></span></span></p><div style="position:absolute;top:191.525314px;left:264.164001px"><nobr><img height="3.000000" width="279.000000" src ="bgimg/bg00140.jpg"/></nobr></div><p><span style="font-family:Standard Symbols PS;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:195.990402px;left:267.782654px"><nobr>∂ w j </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:186.081467px;left:303.216003px"><nobr>= − </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:178.164062px;left:370.962677px"><nobr>y </nobr></span></span></p><p><span style="font-family:Standard Symbols PS;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:195.193436px;left:341.909332px"><nobr>σ (w · x + b) </nobr></span></span></p><p><span style="font-family:Arial Unicode MS;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:186.081467px;left:409.354645px"><nobr>− </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:177.088135px;left:452.167969px"><nobr>1 − y ∂ </nobr></span><span style="position:absolute;top:195.193436px;left:423.114655px"><nobr>1 − σ (w · x + b) ∂ w </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:200.573242px;left:538.722717px"><nobr>j </nobr></span></span></p><p><span style="font-family:Standard Symbols PS;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:186.081467px;left:543.958740px"><nobr>σ (w · x + b) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:8.966399px;font-style:normal;font-weight:normal;color:#0000FF;"><span style="position:absolute;top:214.565018px;left:628.717346px"><nobr>(5.53) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:243.357834px;left:213.600021px"><nobr>And now plugging in the derivative of the sigmoid, and using the chain rule one </nobr></span><span style="position:absolute;top:259.299164px;left:213.600021px"><nobr>more time, we end up with Eq. <a href="#" onclick="gotoPage(22)">5.54</a>: </nobr></span></span></p><p><span style="font-family:Standard Symbols PS;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:287.645355px;left:187.809357px"><nobr>∂ LCE </nobr></span></span></p><div style="position:absolute;top:301.006683px;left:187.142685px"><nobr><img height="3.000000" width="465.000000" src ="bgimg/bg00141.jpg"/></nobr></div><p><span style="font-family:Standard Symbols PS;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:305.471680px;left:190.761353px"><nobr>∂ w j </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:295.561432px;left:226.194687px"><nobr>= − </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:286.569427px;left:300.942657px"><nobr>y − σ (w · x + b) </nobr></span></span></p><p><span style="font-family:Standard Symbols PS;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:304.674713px;left:264.888000px"><nobr>σ (w · x + b)[1 − σ (w · x + b)] </nobr></span></span></p><p><span style="font-family:Standard Symbols PS;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:295.561371px;left:430.993317px"><nobr>σ (w · x + b)[1 − σ (w · x + b)] </nobr></span></span></p><p><span style="font-family:Standard Symbols PS;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:286.569336px;left:588.610596px"><nobr>∂ (w · x + b) </nobr></span><span style="position:absolute;top:305.471680px;left:608.538635px"><nobr>∂ w j </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:333.296051px;left:226.194656px"><nobr>= − </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:324.302734px;left:300.942657px"><nobr>y − σ (w · x + b) </nobr></span></span></p><div style="position:absolute;top:338.739990px;left:264.221313px"><nobr><img height="3.000000" width="158.000000" src ="bgimg/bg00142.jpg"/></nobr></div><p><span style="font-family:Standard Symbols PS;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:333.295990px;left:430.993317px"><nobr>σ (w · x + b)[1 − σ (w · x + b)]x </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:338.674469px;left:595.133240px"><nobr>j </nobr></span></span></p><p><span style="font-family:Standard Symbols PS;font-size:9.962600px;font-style:italic;color:#000000;"><span style="position:absolute;top:342.408051px;left:264.888000px"><nobr>σ (w · x + b)[1 − σ (w · x + b)] </nobr></span><span style="position:absolute;top:361.190674px;left:226.194580px"><nobr>= −[y − σ (w · x + b)]x </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:366.570465px;left:354.197266px"><nobr>j </nobr></span></span></p><p><span style="font-family:Arial;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:381.115967px;left:226.194611px"><nobr>= [σ (w · x + b) − y]x </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.372300px;font-style:italic;color:#000000;"><span style="position:absolute;top:386.495819px;left:343.875885px"><nobr>j </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:8.966399px;font-style:normal;font-weight:normal;color:#0000FF;"><span style="position:absolute;top:382.972931px;left:628.717224px"><nobr>(5.54) </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:17.215401px;font-style:normal;font-weight:normal;color:#0000FF;"><span style="position:absolute;top:426.190552px;left:127.201256px"><nobr>5.11 Summary </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:477.832947px;left:213.599915px"><nobr>This chapter introduced the logistic regression model of classiﬁcation. </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:500.380371px;left:235.518555px"><nobr>• Logistic regression is a supervised machine learning classiﬁer that extracts </nobr></span><span style="position:absolute;top:516.321716px;left:246.809219px"><nobr>real-valued features from the input, multiplies each by a weight, sums them, </nobr></span><span style="position:absolute;top:532.102295px;left:246.809219px"><nobr>and passes the sum through a sigmoid function to generate a probability. A </nobr></span><span style="position:absolute;top:548.201660px;left:246.809189px"><nobr>threshold is used to make a decision. </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:567.033630px;left:235.518524px"><nobr>• Logistic regression can be used with two classes (e.g., positive and negative </nobr></span><span style="position:absolute;top:582.814209px;left:246.809189px"><nobr>sentiment) or with multiple classes (multinomial logistic regression, for ex-</nobr></span><span style="position:absolute;top:598.914917px;left:246.809189px"><nobr>ample for n-ary text classiﬁcation, part-of-speech labeling, etc.). </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:617.587524px;left:235.518524px"><nobr>• Multinomial logistic regression uses the softmax function to compute proba-</nobr></span><span style="position:absolute;top:633.686951px;left:246.809189px"><nobr>bilities. </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:652.518921px;left:235.518524px"><nobr>• The weights (vector w and bias b) are learned from a labeled training set via a </nobr></span><span style="position:absolute;top:668.299500px;left:246.809189px"><nobr>loss function, such as the cross-entropy loss, that must be minimized. </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:687.132812px;left:235.518494px"><nobr>• Minimizing this loss function is a convex optimization problem, and iterative </nobr></span><span style="position:absolute;top:703.072754px;left:246.809189px"><nobr>algorithms like gradient descent are used to ﬁnd the optimal weights. </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:721.904785px;left:235.518524px"><nobr>• Regularization is used to avoid overﬁtting. </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:740.896179px;left:235.518555px"><nobr>• Logistic regression is also one of the most useful analytic tools, because of its </nobr></span><span style="position:absolute;top:756.836182px;left:246.809219px"><nobr>ability to transparently study the importance of individual features. </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:17.215401px;font-style:normal;font-weight:normal;color:#0000FF;"><span style="position:absolute;top:798.495667px;left:127.201225px"><nobr>Bibliographical and Historical Notes </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:834.952271px;left:213.599899px"><nobr>Logistic regression was developed in the ﬁeld of statistics, where it was used for </nobr></span><span style="position:absolute;top:850.893555px;left:213.599899px"><nobr>the analysis of binary data by the 1960s, and was particularly common in medicine </nobr></span><span style="position:absolute;top:866.833496px;left:213.599899px"><nobr>(<a href="#" onclick="gotoPage(24)">Cox</a>, <a href="#" onclick="gotoPage(24)">1969</a>). Starting in the late 1970s it became widely used in linguistics as one </nobr></span><span style="position:absolute;top:882.773560px;left:213.599899px"><nobr>of the formal foundations of the study of linguistic variation (<a href="#" onclick="gotoPage(24)">Sankoff </a><a href="#" onclick="gotoPage(24)">and </a><a href="#" onclick="gotoPage(24)">Labov</a>, </nobr></span><span style="position:absolute;top:898.713562px;left:213.599899px"><nobr><a href="#" onclick="gotoPage(24)">1979</a>). </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:914.653564px;left:233.525223px"><nobr>Nonetheless, logistic regression didn’t become common in natural language pro-</nobr></span><span style="position:absolute;top:930.593506px;left:213.599899px"><nobr>cessing until the 1990s, when it seems to have appeared simultaneously from two </nobr></span><span style="position:absolute;top:946.534851px;left:213.599899px"><nobr>directions. The ﬁrst source was the neighboring ﬁelds of information retrieval and </nobr></span></span></p><div style="position:absolute;top:176.782883px;left:336.624878px"><nobr><img height="32.000000" width="4.000000" src ="bgimg/bg00143.jpg"/></nobr></div><div style="position:absolute;top:176.781494px;left:509.551483px"><nobr><img height="32.000000" width="4.000000" src ="bgimg/bg00144.jpg"/></nobr></div><div style="position:absolute;top:286.262848px;left:259.603546px"><nobr><img height="32.000000" width="4.000000" src ="bgimg/bg00145.jpg"/></nobr></div><div style="position:absolute;top:286.262787px;left:422.691528px"><nobr><img height="32.000000" width="4.000000" src ="bgimg/bg00146.jpg"/></nobr></div><div style="position:absolute;top:323.996185px;left:259.604858px"><nobr><img height="32.000000" width="4.000000" src ="bgimg/bg00147.jpg"/></nobr></div><div style="position:absolute;top:323.996094px;left:422.691528px"><nobr><img height="32.000000" width="4.000000" src ="bgimg/bg00148.jpg"/></nobr></div>','<p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-weight:bold;color:#000000;"><span style="position:absolute;top:112.375732px;left:103.201332px"><nobr>24 C HAPTER 5 </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#808080;"><span style="position:absolute;top:112.535156px;left:211.115936px"><nobr>• </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:112.535156px;left:229.380722px"><nobr>L OGISTIC R EGRESSION </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:149.817795px;left:189.600021px"><nobr>speech processing, both of which had made use of regression, and both of which </nobr></span><span style="position:absolute;top:165.757812px;left:189.600021px"><nobr>lent many other statistical techniques to NLP. Indeed a very early use of logistic </nobr></span><span style="position:absolute;top:181.699142px;left:189.600021px"><nobr>regression for document routing was one of the ﬁrst NLP applications to use (LSI) </nobr></span><span style="position:absolute;top:197.639160px;left:189.600021px"><nobr>embeddings as word representations (<a href="#" onclick="gotoPage(24)">Sch¨ </a><a href="#" onclick="gotoPage(24)">u</a><a href="#" onclick="gotoPage(24)">tze </a><a href="#" onclick="gotoPage(24)">et </a><a href="#" onclick="gotoPage(24)">al.</a>, <a href="#" onclick="gotoPage(24)">1995</a>). </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.222899px;font-weight:bold;color:#0000FF;"><span style="position:absolute;top:225.516281px;left:128.671997px"><nobr>maximum </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.222899px;font-weight:bold;color:#0000FF;"><span style="position:absolute;top:234.150955px;left:138.475998px"><nobr>entropy </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:213.579178px;left:209.525360px"><nobr>At the same time in the early 1990s logistic regression was developed and ap-</nobr></span><span style="position:absolute;top:229.359787px;left:189.600037px"><nobr>plied to NLP at IBM Research under the name maximum entropy modeling or </nobr></span><span style="position:absolute;top:245.299805px;left:189.599991px"><nobr>maxent (<a href="#" onclick="gotoPage(24)">Berger </a><a href="#" onclick="gotoPage(24)">et </a><a href="#" onclick="gotoPage(24)">al.</a>, <a href="#" onclick="gotoPage(24)">1996</a>), seemingly independent of the statistical literature. Un-</nobr></span><span style="position:absolute;top:261.399261px;left:189.599991px"><nobr>der that name it was applied to language modeling (<a href="#" onclick="gotoPage(24)">Rosenfeld</a>, <a href="#" onclick="gotoPage(24)">1996</a>), part-of-speech </nobr></span><span style="position:absolute;top:277.340576px;left:189.599991px"><nobr>tagging (<a href="#" onclick="gotoPage(24)">Ratnaparkhi</a>, <a href="#" onclick="gotoPage(24)">1996</a>), parsing (<a href="#" onclick="gotoPage(24)">Ratnaparkhi</a>, <a href="#" onclick="gotoPage(24)">1997</a>), coreference resolution </nobr></span><span style="position:absolute;top:293.280609px;left:189.599991px"><nobr>(<a href="#" onclick="gotoPage(24)">Kehler</a>, <a href="#" onclick="gotoPage(24)">1997</a>), and text classiﬁcation (<a href="#" onclick="gotoPage(24)">Nigam </a><a href="#" onclick="gotoPage(24)">et </a><a href="#" onclick="gotoPage(24)">al.</a>, <a href="#" onclick="gotoPage(24)">1999</a>). </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:309.220612px;left:209.525330px"><nobr>More on classiﬁcation can be found in machine learning textbooks (<a href="#" onclick="gotoPage(24)">Hastie </a><a href="#" onclick="gotoPage(24)">et </a><a href="#" onclick="gotoPage(24)">al. </a></nobr></span><span style="position:absolute;top:325.160645px;left:189.599991px"><nobr><a href="#" onclick="gotoPage(24)">2001</a>, <a href="#" onclick="gotoPage(24)">Witten </a><a href="#" onclick="gotoPage(24)">and </a><a href="#" onclick="gotoPage(24)">Frank </a><a href="#" onclick="gotoPage(24)">2005</a>, <a href="#" onclick="gotoPage(24)">Bishop </a><a href="#" onclick="gotoPage(24)">2006</a>, <a href="#" onclick="gotoPage(24)">Murphy </a><a href="#" onclick="gotoPage(24)">2012</a>). </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:17.215401px;font-style:normal;font-weight:normal;color:#0000FF;"><span style="position:absolute;top:365.760101px;left:103.201332px"><nobr>Exercises </nobr></span></span></p>','<p><span style="font-family:Nimbus Roman;font-size:9.962600px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:113.649818px;left:577.674683px"><nobr>Exercises 25 </nobr></span></span></p><div style="position:absolute;top:127.637291px;left:126.534668px"><nobr><img height="3.000000" width="532.000000" src ="bgimg/bg00149.jpg"/></nobr></div><p><span style="font-family:Nimbus Roman;font-size:7.970100px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:151.618973px;left:127.201332px"><nobr>Berger, A., S. A. Della Pietra, and V. J. Della Pietra. 1996. <a href="http://www.aclweb.org/anthology/J96-1002.pdf" target="_blank">A </a></nobr></span><span style="position:absolute;top:164.239014px;left:140.485321px"><nobr><a href="http://www.aclweb.org/anthology/J96-1002.pdf" target="_blank">maximum </a><a href="http://www.aclweb.org/anthology/J96-1002.pdf" target="_blank">entropy </a><a href="http://www.aclweb.org/anthology/J96-1002.pdf" target="_blank">approach </a><a href="http://www.aclweb.org/anthology/J96-1002.pdf" target="_blank">to </a><a href="http://www.aclweb.org/anthology/J96-1002.pdf" target="_blank">natural </a><a href="http://www.aclweb.org/anthology/J96-1002.pdf" target="_blank">language </a><a href="http://www.aclweb.org/anthology/J96-1002.pdf" target="_blank">process-</a></nobr></span><span style="position:absolute;top:176.857666px;left:140.485321px"><nobr><a href="http://www.aclweb.org/anthology/J96-1002.pdf" target="_blank">ing</a>. Computational Linguistics, 22(1):39–71. </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.970100px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:193.462967px;left:127.201324px"><nobr>Bishop, C. M. 2006. Pattern recognition and machine learn-</nobr></span><span style="position:absolute;top:206.081619px;left:140.485321px"><nobr>ing. Springer. </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.970100px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:222.685623px;left:127.201324px"><nobr>Cox, D. 1969. Analysis of Binary Data. Chapman and Hall, </nobr></span><span style="position:absolute;top:235.305664px;left:140.485321px"><nobr>London. </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.970100px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:251.909668px;left:127.201332px"><nobr>Hastie, T., R. J. Tibshirani, and J. H. Friedman. 2001. The </nobr></span><span style="position:absolute;top:264.529694px;left:140.485352px"><nobr>Elements of Statistical Learning. Springer. </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.970100px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:281.133698px;left:127.201363px"><nobr>Kehler, A. 1997. <a href="https://www.aclweb.org/anthology/W97-0319" target="_blank">Probabilistic </a><a href="https://www.aclweb.org/anthology/W97-0319" target="_blank">coreference </a><a href="https://www.aclweb.org/anthology/W97-0319" target="_blank">in </a><a href="https://www.aclweb.org/anthology/W97-0319" target="_blank">information </a></nobr></span><span style="position:absolute;top:293.752350px;left:140.485367px"><nobr><a href="https://www.aclweb.org/anthology/W97-0319" target="_blank">extraction</a>. EMNLP. </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.970100px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:310.357666px;left:127.201385px"><nobr>Murphy, K. P. 2012. Machine learning: A probabilistic per-</nobr></span><span style="position:absolute;top:322.976318px;left:140.485382px"><nobr>spective. MIT Press. </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.970100px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:339.581635px;left:127.201378px"><nobr>Ng, A. Y. and M. I. Jordan. 2002. On discriminative vs. </nobr></span><span style="position:absolute;top:352.200287px;left:140.485382px"><nobr>generative classiﬁers: A comparison of logistic regres-</nobr></span><span style="position:absolute;top:364.820312px;left:140.485382px"><nobr>sion and naive bayes. NeurIPS. </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.970100px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:381.424377px;left:127.201378px"><nobr>Nigam, K., J. D. Lafferty, and A. McCallum. 1999. Using </nobr></span><span style="position:absolute;top:394.043030px;left:140.485382px"><nobr>maximum entropy for text classiﬁcation. IJCAI-99 work-</nobr></span><span style="position:absolute;top:406.662994px;left:140.485382px"><nobr>shop on machine learning for information ﬁltering. </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.970100px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:423.266998px;left:127.201363px"><nobr>Ratnaparkhi, A. 1996. <a href="https://www.aclweb.org/anthology/W96-0213" target="_blank">A </a><a href="https://www.aclweb.org/anthology/W96-0213" target="_blank">maximum </a><a href="https://www.aclweb.org/anthology/W96-0213" target="_blank">entropy </a><a href="https://www.aclweb.org/anthology/W96-0213" target="_blank">part-of-speech </a></nobr></span><span style="position:absolute;top:435.886993px;left:140.485352px"><nobr><a href="https://www.aclweb.org/anthology/W96-0213" target="_blank">tagger</a>. EMNLP. </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.970100px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:452.490997px;left:127.201363px"><nobr>Ratnaparkhi, A. 1997. <a href="https://www.aclweb.org/anthology/W97-0301" target="_blank">A </a><a href="https://www.aclweb.org/anthology/W97-0301" target="_blank">linear </a><a href="https://www.aclweb.org/anthology/W97-0301" target="_blank">observed </a><a href="https://www.aclweb.org/anthology/W97-0301" target="_blank">time </a><a href="https://www.aclweb.org/anthology/W97-0301" target="_blank">statistical </a></nobr></span><span style="position:absolute;top:465.110992px;left:140.485352px"><nobr><a href="https://www.aclweb.org/anthology/W97-0301" target="_blank">parser </a><a href="https://www.aclweb.org/anthology/W97-0301" target="_blank">based </a><a href="https://www.aclweb.org/anthology/W97-0301" target="_blank">on </a><a href="https://www.aclweb.org/anthology/W97-0301" target="_blank">maximum </a><a href="https://www.aclweb.org/anthology/W97-0301" target="_blank">entropy </a><a href="https://www.aclweb.org/anthology/W97-0301" target="_blank">models</a>. EMNLP. </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.970100px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:481.714996px;left:127.201332px"><nobr>Rosenfeld, R. 1996. A maximum entropy approach to adap-</nobr></span><span style="position:absolute;top:494.333649px;left:140.485321px"><nobr>tive statistical language modeling. Computer Speech and </nobr></span><span style="position:absolute;top:506.953674px;left:140.485336px"><nobr>Language, 10:187–228. </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.970100px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:523.557678px;left:127.201332px"><nobr>Sankoff, D. and W. Labov. 1979. On the uses of variable </nobr></span><span style="position:absolute;top:536.177612px;left:140.485321px"><nobr>rules. Language in society, 8(2-3):189–222. </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.970100px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:552.781677px;left:127.201309px"><nobr>Sch¨ utze, H., D. A. Hull, and J. Pedersen. 1995. <a href="https://doi.org/10.1145/215206.215365" target="_blank">A </a><a href="https://doi.org/10.1145/215206.215365" target="_blank">compar-</a></nobr></span><span style="position:absolute;top:565.400269px;left:140.485321px"><nobr><a href="https://doi.org/10.1145/215206.215365" target="_blank">ison </a><a href="https://doi.org/10.1145/215206.215365" target="_blank">of </a><a href="https://doi.org/10.1145/215206.215365" target="_blank">classi</a><a href="https://doi.org/10.1145/215206.215365" target="_blank">ﬁers </a><a href="https://doi.org/10.1145/215206.215365" target="_blank">and </a><a href="https://doi.org/10.1145/215206.215365" target="_blank">document </a><a href="https://doi.org/10.1145/215206.215365" target="_blank">representations </a><a href="https://doi.org/10.1145/215206.215365" target="_blank">for </a><a href="https://doi.org/10.1145/215206.215365" target="_blank">the </a></nobr></span><span style="position:absolute;top:578.020264px;left:140.485321px"><nobr><a href="https://doi.org/10.1145/215206.215365" target="_blank">routing </a><a href="https://doi.org/10.1145/215206.215365" target="_blank">problem</a>. SIGIR-95. </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.970100px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:594.624268px;left:127.201324px"><nobr>Tibshirani, R. J. 1996. Regression shrinkage and selection </nobr></span><span style="position:absolute;top:607.244324px;left:140.485321px"><nobr>via the lasso. Journal of the Royal Statistical Society. Se-</nobr></span><span style="position:absolute;top:619.862976px;left:140.485306px"><nobr>ries B (Methodological), 58(1):267–288. </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.970100px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:636.468323px;left:127.201309px"><nobr>Wang, S. and C. D. Manning. 2012. <a href="https://www.aclweb.org/anthology/P12-2018" target="_blank">Baselines </a><a href="https://www.aclweb.org/anthology/P12-2018" target="_blank">and </a><a href="https://www.aclweb.org/anthology/P12-2018" target="_blank">bigrams: </a></nobr></span><span style="position:absolute;top:649.086975px;left:140.485291px"><nobr><a href="https://www.aclweb.org/anthology/P12-2018" target="_blank">Simple, </a><a href="https://www.aclweb.org/anthology/P12-2018" target="_blank">good </a><a href="https://www.aclweb.org/anthology/P12-2018" target="_blank">sentiment </a><a href="https://www.aclweb.org/anthology/P12-2018" target="_blank">and </a><a href="https://www.aclweb.org/anthology/P12-2018" target="_blank">topic </a><a href="https://www.aclweb.org/anthology/P12-2018" target="_blank">classiﬁcation</a>. ACL. </nobr></span></span></p><p><span style="font-family:Nimbus Roman;font-size:7.970100px;font-style:normal;font-weight:normal;color:#000000;"><span style="position:absolute;top:665.690979px;left:127.201294px"><nobr>Witten, I. H. and E. Frank. 2005. Data Mining: Practi-</nobr></span><span style="position:absolute;top:678.310974px;left:140.485291px"><nobr>cal Machine Learning Tools and Techniques, 2nd edition. </nobr></span><span style="position:absolute;top:690.929565px;left:140.485291px"><nobr>Morgan Kaufmann. </nobr></span></span></p>']
            }
        },
        mounted: function() {
            this.$nextTick(function() {
                this.pageNum = this.pageDatas.length;
                this.pageContent = this.pageDatas[0];

                this.setLeftMenuHeight();
            })
        },
        watch: {
            'currentPage': function(newVal, oldValue) {
                // console.log('newVal ' + newVal, 'oldValue ' + oldValue);
                if (newVal) {
                    this.pageContent = this.pageDatas[this.currentPage - 1];
                }
            },
            'zoomNum': function(newVal, oldValue) {
                if (newVal) {
                    this.zoomStyle = {
                        'transform': 'scale(' + newVal + ')',
                        '-webkit-transform': 'scale(' + newVal + ')',
                        '-ms-transform': 'scale(' + newVal + ')',
                        '-moz-transform': 'scale(' + newVal + ')',
                        '-o-transform': 'scale(' + newVal + ')'
                    }
                }
            }
        },
        methods: {
            
            changeCurrentPage: function(methods) {
                switch (methods) {
                    case 'first':
                        this.currentPage = 1;
                        break;
                    case 'prev':
                        if (this.currentPage > 1) {
                            this.currentPage -= 1;
                        }
                        break;
                    case 'next':
                        if (this.currentPage < this.pageNum) {
                            this.currentPage += 1;
                        }
                        break;
                    case 'last':
                        this.currentPage = this.pageNum;
                        break;
                }
            },

            gotoPage: function(page) {
                console.log(page);
                this.currentPage = page;
            },
            modifyZoom: function(type) {
                switch (type) {
                    case 'in':
                        if (this.zoomNum < 2) {
                            // this.zoomNum = (this.zoomNum + 0.1).toFixed(1);
                            this.zoomNum = (parseFloat(this.zoomNum) + 0.1).toFixed(1);
                        }
                        break;
                    case 'out':
                        if (this.zoomNum > 0.5) {
                            this.zoomNum = (parseFloat(this.zoomNum) - 0.1).toFixed(1);
                        }
                        break;
                    default:
                        break;
                }
                console.log(this.zoomNum);
            },
            setLeftMenuHeight: function() {
                // this.asideHeight = document.body.scrollHeight - 60;
                this.mainHeight = document.documentElement.clientHeight - 60 - 20;
                // 60为头部导航高度， 46为menu高度， 40为上下padding
                this.asideHeight = this.mainHeight - 20 - 46;
            },
            changePage: function(page) {
                this.currentPage = page;
                // this.pageContent = this.pageDatas[page - 1];
            },
            changeLeftMenu: function() {
                this.ifMenuShow = !this.ifMenuShow;
            }
        }
    });

function gotoPage(page) {
    console.log(page);
    app.gotoPage(page);
}

</script>

</html>